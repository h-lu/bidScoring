#!/usr/bin/env python3
"""æ‰¹é‡ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk Nodes).

This script is intentionally thin; most heavy logic is in `_hichunk_nodes_lib.py`
to keep files under the 500 LOC limit.
"""

from __future__ import annotations

import sys
import time
from typing import Any

import psycopg
from pgvector.psycopg import register_vector

from bid_scoring.config import load_settings
from bid_scoring.hichunk import HiChunkBuilder

from _hichunk_nodes_lib import (
    fetch_pending_versions,
    format_duration,
    get_stats,
    insert_hierarchical_nodes,
    reset_hierarchical_nodes,
)


DEFAULT_BATCH_SIZE = 10  # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
DEFAULT_LIMIT = 100  # æ¯æ¬¡è¿è¡Œæœ€å¤§å¤„ç†æ–‡æ¡£æ•°
DEFAULT_MAX_NODES_PER_DOC = 10000  # å•ä¸ªæ–‡æ¡£æœ€å¤§èŠ‚ç‚¹æ•°


def get_chunk_mapping(conn, version_id: str) -> dict[int, str]:
    """chunk_index -> chunk_id mapping for a version."""
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT chunk_id, chunk_index 
            FROM chunks 
            WHERE version_id = %s 
            ORDER BY chunk_index
            """,
            (version_id,),
        )
        rows = cur.fetchall()
        return {row[1]: str(row[0]) for row in rows}


def process_version(
    conn,
    version_data: dict[str, Any],
    builder: HiChunkBuilder,
    show_detail: bool = False,
) -> tuple[int, int]:
    """å¤„ç†å•ä¸ªæ–‡æ¡£ç‰ˆæœ¬."""
    version_id = version_data["version_id"]
    document_title = version_data["document_title"]
    content_list = version_data["content_list"]

    if show_detail:
        print(f"\nğŸ“„ å¤„ç†ç‰ˆæœ¬: {version_id[:8]}...")
        print(f"   æ–‡æ¡£æ ‡é¢˜: {document_title}")
        print(f"   content_list é•¿åº¦: {len(content_list)}")

    try:
        if show_detail:
            print("  ğŸ—ï¸  æ„å»ºå±‚æ¬¡ç»“æ„...", end=" ", flush=True)

        nodes = builder.build_hierarchy(content_list, document_title)

        if show_detail:
            print(f"âœ… ({len(nodes)} ä¸ªèŠ‚ç‚¹)")

        if len(nodes) > DEFAULT_MAX_NODES_PER_DOC:
            print(
                f"  âš ï¸  èŠ‚ç‚¹æ•°é‡ ({len(nodes)}) è¶…è¿‡é™åˆ¶ ({DEFAULT_MAX_NODES_PER_DOC})ï¼Œè·³è¿‡"
            )
            return 0, len(nodes)

        if show_detail:
            print("  ğŸ”— è·å– chunk æ˜ å°„...", end=" ", flush=True)

        chunk_mapping = get_chunk_mapping(conn, version_id)

        if show_detail:
            print(f"âœ… ({len(chunk_mapping)} ä¸ª chunks)")

        if show_detail:
            print("  ğŸ’¾ æ’å…¥èŠ‚ç‚¹...", end=" ", flush=True)

        return insert_hierarchical_nodes(
            conn, version_id, nodes, chunk_mapping, show_detail
        )

    except Exception as e:
        conn.rollback()
        print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
        return 0, 0


def main() -> None:
    start_time = time.time()

    settings = load_settings()
    dsn = settings["DATABASE_URL"]

    import argparse

    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk)")
    parser.add_argument("--version-id", help="æŒ‡å®šç‰ˆæœ¬ ID")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ {DEFAULT_BATCH_SIZE}ï¼‰",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=DEFAULT_LIMIT,
        help=f"æœ€å¤§å¤„ç†æ•°é‡ï¼ˆé»˜è®¤ {DEFAULT_LIMIT}ï¼‰",
    )
    parser.add_argument(
        "--max-nodes",
        type=int,
        default=DEFAULT_MAX_NODES_PER_DOC,
        help=f"å•ä¸ªæ–‡æ¡£æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤ {DEFAULT_MAX_NODES_PER_DOC}ï¼‰",
    )
    parser.add_argument("--show-detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¿›åº¦")
    parser.add_argument(
        "--dry-run", action="store_true", help="å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰"
    )
    parser.add_argument(
        "--reset",
        "-r",
        action="store_true",
        help="é‡ç½®/æ¸…ç©º hierarchical_nodes è¡¨åé‡æ–°ç”Ÿæˆ",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡ç½®ï¼Œè·³è¿‡ç¡®è®¤æç¤ºï¼ˆé…åˆ --reset ä½¿ç”¨ï¼‰",
    )
    # Kept for CLI backward compatibility; not implemented yet.
    parser.add_argument(
        "--with-embeddings",
        action="store_true",
        help="(æš‚æœªå®ç°) ä¸ºéå¶å­èŠ‚ç‚¹ç”Ÿæˆ embeddings",
    )
    parser.add_argument(
        "--embedding-model", default=None, help="(æš‚æœªå®ç°) embedding æ¨¡å‹åç§°"
    )
    args = parser.parse_args()

    if args.with_embeddings:
        print("âš ï¸  --with-embeddings å½“å‰è„šæœ¬æœªå®ç°ï¼Œå°†å¿½ç•¥è¯¥é€‰é¡¹ã€‚")

    with psycopg.connect(dsn) as conn:
        register_vector(conn)

        if args.reset:
            if not reset_hierarchical_nodes(conn, args.version_id, args.force):
                sys.exit(0)
            print()

        stats = get_stats(conn, args.version_id)

        print("=" * 80)
        print("ğŸš€ å¼€å§‹ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk)")
        print("=" * 80)
        print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        print(f"æœ€å¤§èŠ‚ç‚¹æ•°/æ–‡æ¡£: {args.max_nodes:,}")
        if args.version_id:
            print(f"ç‰ˆæœ¬è¿‡æ»¤: {args.version_id}")
        if args.dry_run:
            print("âš ï¸ å¹²è¿è¡Œæ¨¡å¼: ä¸ä¼šå†™å…¥æ•°æ®åº“")
        print()

        print("=" * 80)
        print("ğŸ“Š åˆå§‹ç»Ÿè®¡")
        print("=" * 80)
        print(f"  æ€»ç‰ˆæœ¬æ•°:     {stats['total_versions']:,}")
        print(f"  å·²å¤„ç†:       {stats['processed']:,}")
        print(f"  å¾…å¤„ç†:       {stats['to_process']:,}")
        print(f"  ç°æœ‰èŠ‚ç‚¹æ•°:   {stats['total_nodes']:,}")
        print()

        if stats["to_process"] == 0:
            print("âœ… æ‰€æœ‰ç‰ˆæœ¬éƒ½å·²å¤„ç†ï¼Œæ— éœ€å¤„ç†")
            return

        to_process = min(stats["to_process"], args.limit)
        print(f"å°†å¤„ç† {to_process} ä¸ªç‰ˆæœ¬ï¼ˆé™åˆ¶: {args.limit}ï¼‰")
        print()

        builder = HiChunkBuilder()

        total_success = 0
        total_fail = 0
        batch_num = 0
        processed = 0

        print("=" * 80)
        print("ğŸ”„ å¼€å§‹å¤„ç†")
        print("=" * 80)

        while processed < to_process:
            remaining = to_process - processed
            batch_size = min(args.batch_size, remaining)

            versions = fetch_pending_versions(
                conn,
                batch_size=batch_size,
                version_id=args.version_id,
            )

            if not versions:
                print("æ²¡æœ‰æ›´å¤šæ•°æ®éœ€è¦å¤„ç†")
                break

            batch_num += 1
            batch_len = len(versions)

            print(f"\næ‰¹æ¬¡ {batch_num:>3}: {batch_len:>3} ä¸ªç‰ˆæœ¬...")

            if args.dry_run:
                for v in versions:
                    print(
                        f"  â­ï¸  {v['version_id'][:8]}... ({len(v['content_list'])} items)"
                    )
                processed += batch_len
                continue

            batch_success = 0
            batch_fail = 0

            for version_data in versions:
                success, fail = process_version(
                    conn,
                    version_data,
                    builder,
                    show_detail=args.show_detail,
                )
                batch_success += success
                batch_fail += fail

            total_success += batch_success
            total_fail += batch_fail
            processed += batch_len

            print(f"  âœ… å®Œæˆ: {batch_success} ä¸ªèŠ‚ç‚¹")
            if batch_fail > 0:
                print(f"  âŒ å¤±è´¥: {batch_fail} ä¸ªèŠ‚ç‚¹")

            if batch_num % 10 == 0:
                progress = 100 * processed / to_process
                elapsed = time.time() - start_time
                eta = (
                    (elapsed / processed) * (to_process - processed)
                    if processed > 0
                    else 0
                )
                print(
                    f"\n  ğŸ“ˆ è¿›åº¦: {processed}/{to_process} ({progress:.1f}%) | å·²ç”¨: {format_duration(elapsed)} | é¢„è®¡å‰©ä½™: {format_duration(eta)}"
                )

        elapsed = time.time() - start_time

        print()
        print("=" * 80)
        print("âœ… å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"æˆåŠŸèŠ‚ç‚¹:     {total_success:,}")
        print(f"å¤±è´¥èŠ‚ç‚¹:     {total_fail:,}")
        print(f"å¤„ç†ç‰ˆæœ¬æ•°:   {processed:,}")
        print(f"ç”¨æ—¶:         {format_duration(elapsed)}")
        if processed > 0:
            print(f"å¹³å‡é€Ÿåº¦:     {processed / elapsed:.2f} ç‰ˆæœ¬/ç§’")

        final_stats = get_stats(conn, args.version_id)
        print()
        print("ğŸ“Š æœ€ç»ˆçŠ¶æ€")
        if final_stats["total_versions"] > 0:
            pct = 100 * final_stats["processed"] / final_stats["total_versions"]
        else:
            pct = 0.0
        print(
            f"  å·²å¤„ç†ç‰ˆæœ¬: {final_stats['processed']:,} / {final_stats['total_versions']:,} ({pct:.1f}%)"
        )
        print(f"  æ€»èŠ‚ç‚¹æ•°:   {final_stats['total_nodes']:,}")
        print(f"  å¾…å¤„ç†:     {final_stats['to_process']:,}")


if __name__ == "__main__":
    main()
