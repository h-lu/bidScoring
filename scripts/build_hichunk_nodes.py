#!/usr/bin/env python3
"""æ‰¹é‡ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk Nodes)

Features:
- ä» document_versions è¯»å– content_list
- ä½¿ç”¨ HiChunkBuilder æ„å»º4å±‚æ–‡æ¡£æ ‘ç»“æ„
- å°†èŠ‚ç‚¹æ’å…¥ hierarchical_nodes è¡¨
- è‡ªåŠ¨å…³è” chunks è¡¨ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰
- æ”¯æŒ embeddings ç”Ÿæˆï¼ˆéå¶å­èŠ‚ç‚¹ï¼Œå¯é€‰ï¼‰
- è¿›åº¦æ˜¾ç¤ºå’Œæ¢å¤æœºåˆ¶
- é”™è¯¯å¤„ç†å’Œäº‹åŠ¡å›æ»š

Tree Structure:
- Level 0 (sentence): å¶å­èŠ‚ç‚¹ï¼Œå¯¹åº” content_list å…ƒç´ 
- Level 1 (paragraph): æ®µè½èŠ‚ç‚¹ï¼Œåˆå¹¶ç›¸é‚»å¥å­
- Level 2 (section): ç« èŠ‚èŠ‚ç‚¹ï¼ŒæŒ‰æ ‡é¢˜åˆ†ç»„
- Level 3 (document): æ–‡æ¡£æ ¹èŠ‚ç‚¹
"""

import sys
import time
from datetime import datetime
from typing import Any

import psycopg
from pgvector.psycopg import register_vector

from bid_scoring.config import load_settings
from bid_scoring.hichunk import HiChunkBuilder
from bid_scoring.embeddings import embed_texts, get_embedding_client, get_embedding_config


# é…ç½®å‚æ•°
DEFAULT_BATCH_SIZE = 10       # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
DEFAULT_LIMIT = 100           # æ¯æ¬¡è¿è¡Œæœ€å¤§å¤„ç†æ–‡æ¡£æ•°
DEFAULT_MAX_NODES_PER_DOC = 10000  # å•ä¸ªæ–‡æ¡£æœ€å¤§èŠ‚ç‚¹æ•°


def reset_hierarchical_nodes(conn, version_id: str | None = None, force: bool = False) -> bool:
    """é‡ç½®/æ¸…ç©º hierarchical_nodes è¡¨
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_id: ç‰ˆæœ¬ IDï¼Œå¦‚æœæŒ‡å®šåˆ™åªåˆ é™¤è¯¥ç‰ˆæœ¬çš„è®°å½•ï¼Œå¦åˆ™åˆ é™¤æ‰€æœ‰
        force: æ˜¯å¦è·³è¿‡ç¡®è®¤æç¤º
    
    Returns:
        True è¡¨ç¤ºå·²é‡ç½®ï¼ŒFalse è¡¨ç¤ºç”¨æˆ·å–æ¶ˆ
    """
    with conn.cursor() as cur:
        # å…ˆæŸ¥è¯¢å°†è¦åˆ é™¤çš„è®°å½•æ•°
        if version_id:
            cur.execute(
                "SELECT COUNT(*) FROM hierarchical_nodes WHERE version_id = %s",
                (version_id,)
            )
        else:
            cur.execute("SELECT COUNT(*) FROM hierarchical_nodes")
        
        count = cur.fetchone()[0]
        
        if count == 0:
            print("â„¹ï¸  hierarchical_nodes è¡¨ä¸ºç©ºï¼Œæ— éœ€é‡ç½®")
            return True
        
        # ç¡®è®¤æç¤º
        if not force:
            scope = f"ç‰ˆæœ¬ '{version_id}'" if version_id else "æ‰€æœ‰ç‰ˆæœ¬"
            print(f"\nâš ï¸  è­¦å‘Š: è¿™å°†åˆ é™¤ {scope} çš„ {count} æ¡ hierarchical_nodes è®°å½•ï¼")
            response = input("ç¡®è®¤é‡ç½®? è¾“å…¥ 'yes' ç»§ç»­: ")
            if response.lower() != 'yes':
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
                return False
        
        # æ‰§è¡Œåˆ é™¤
        if version_id:
            cur.execute(
                "DELETE FROM hierarchical_nodes WHERE version_id = %s",
                (version_id,)
            )
        else:
            cur.execute("DELETE FROM hierarchical_nodes")
        
        conn.commit()
        
        scope = f"ç‰ˆæœ¬ '{version_id}'" if version_id else "æ‰€æœ‰ç‰ˆæœ¬"
        print(f"âœ… å·²é‡ç½® {scope} çš„ {count} æ¡è®°å½•")
        return True


def get_stats(conn, version_id: str | None = None) -> dict[str, Any]:
    """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        {
            'total_versions': æ€»ç‰ˆæœ¬æ•°,
            'processed_versions': å·²å¤„ç†ç‰ˆæœ¬æ•°,
            'to_process': å¾…å¤„ç†ç‰ˆæœ¬æ•°,
            'total_nodes': æ€»èŠ‚ç‚¹æ•°,
        }
    """
    with conn.cursor() as cur:
        # åŸºç¡€æŸ¥è¯¢ï¼šç»Ÿè®¡ document_versions ä¸­ content_list éç©ºçš„ç‰ˆæœ¬
        base_query = """
            SELECT 
                COUNT(*) FILTER (WHERE dv.content_list IS NOT NULL AND jsonb_array_length(dv.content_list) > 0) as total_versions,
                COUNT(*) FILTER (WHERE hn.version_id IS NOT NULL) as processed_versions,
                COUNT(*) FILTER (WHERE dv.content_list IS NOT NULL AND jsonb_array_length(dv.content_list) > 0 AND hn.version_id IS NULL) as to_process
            FROM document_versions dv
            LEFT JOIN (
                SELECT DISTINCT version_id FROM hierarchical_nodes
            ) hn ON dv.version_id = hn.version_id
        """
        
        params = []
        if version_id:
            base_query += " WHERE dv.version_id = %s"
            params.append(version_id)
        
        cur.execute(base_query, params)
        row = cur.fetchone()
        
        # ç»Ÿè®¡æ€»èŠ‚ç‚¹æ•°
        nodes_query = "SELECT COUNT(*) FROM hierarchical_nodes"
        nodes_params = []
        if version_id:
            nodes_query += " WHERE version_id = %s"
            nodes_params.append(version_id)
        
        cur.execute(nodes_query, nodes_params)
        total_nodes = cur.fetchone()[0]
        
        return {
            "total_versions": row[0],
            "processed": row[1],
            "to_process": row[2],
            "total_nodes": total_nodes,
        }


def fetch_pending_versions(
    conn,
    batch_size: int = DEFAULT_BATCH_SIZE,
    version_id: str | None = None,
) -> list[dict[str, Any]]:
    """è·å–å¾…å¤„ç†çš„æ–‡æ¡£ç‰ˆæœ¬
    
    ç­–ç•¥:
    1. é€‰æ‹© content_list éç©ºä¸”æœªå¤„ç†çš„ç‰ˆæœ¬
    2. è·å–ç‰ˆæœ¬åŸºæœ¬ä¿¡æ¯å’Œå…³è”çš„æ–‡æ¡£æ ‡é¢˜
    
    Returns:
        [{
            'version_id': ç‰ˆæœ¬ID,
            'doc_id': æ–‡æ¡£ID,
            'document_title': æ–‡æ¡£æ ‡é¢˜,
            'content_list': content_list JSON,
        }, ...]
    """
    with conn.cursor() as cur:
        query = """
            SELECT 
                dv.version_id,
                dv.doc_id,
                d.title as document_title,
                dv.content_list
            FROM document_versions dv
            JOIN documents d ON dv.doc_id = d.doc_id
            LEFT JOIN hierarchical_nodes hn ON dv.version_id = hn.version_id
            WHERE dv.content_list IS NOT NULL 
              AND jsonb_array_length(dv.content_list) > 0
              AND hn.version_id IS NULL
        """
        
        params = []
        if version_id:
            query += " AND dv.version_id = %s"
            params.append(version_id)
        
        query += " ORDER BY dv.created_at LIMIT %s"
        params.append(batch_size)
        
        cur.execute(query, params)
        rows = cur.fetchall()
        
        result = []
        for row in rows:
            result.append({
                "version_id": str(row[0]),
                "doc_id": str(row[1]),
                "document_title": row[2] or "untitled",
                "content_list": row[3],
            })
        
        return result


def get_chunk_mapping(conn, version_id: str) -> dict[int, str]:
    """è·å–ç‰ˆæœ¬ä¸‹çš„ chunk æ˜ å°„
    
    é€šè¿‡ chunk_index æ˜ å°„åˆ° chunk_idï¼Œç”¨äºå…³è”å¶å­èŠ‚ç‚¹
    
    Returns:
        {chunk_index: chunk_id, ...}
    """
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT chunk_id, chunk_index 
            FROM chunks 
            WHERE version_id = %s 
            ORDER BY chunk_index
            """,
            (version_id,)
        )
        rows = cur.fetchall()
        
        return {row[1]: str(row[0]) for row in rows}


def insert_hierarchical_nodes(
    conn,
    version_id: str,
    nodes: list,
    chunk_mapping: dict[int, str],
    show_detail: bool = False,
) -> tuple[int, int]:
    """æ’å…¥å±‚æ¬¡åŒ–èŠ‚ç‚¹åˆ°æ•°æ®åº“
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_id: ç‰ˆæœ¬ ID
        nodes: HiChunkNode åˆ—è¡¨
        chunk_mapping: chunk_index åˆ° chunk_id çš„æ˜ å°„
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    if not nodes:
        return 0, 0
    
    success_count = 0
    fail_count = 0
    
    # å‡†å¤‡æ’å…¥æ•°æ®
    insert_data = []
    leaf_nodes = [n for n in nodes if n.level == 0]
    
    # ä¸ºå¶å­èŠ‚ç‚¹å»ºç«‹ source_index åˆ° node çš„æ˜ å°„
    leaf_by_source_idx = {}
    for node in leaf_nodes:
        source_idx = node.metadata.get("source_index")
        if source_idx is not None:
            leaf_by_source_idx[source_idx] = node
    
    for node in nodes:
        # å¯¹äºå¶å­èŠ‚ç‚¹ï¼Œå°è¯•å…³è” chunks
        start_chunk_id = None
        end_chunk_id = None
        
        if node.level == 0:
            source_idx = node.metadata.get("source_index")
            if source_idx is not None and source_idx in chunk_mapping:
                start_chunk_id = chunk_mapping[source_idx]
                end_chunk_id = chunk_mapping[source_idx]
        elif node.level == 1:  # paragraph
            # å¯¹äºæ®µè½ï¼Œå…³è”å…¶åŒ…å«çš„å¶å­èŠ‚ç‚¹çš„ chunks
            child_source_indices = []
            for child_id in node.children_ids:
                child_node = next((n for n in leaf_nodes if n.node_id == child_id), None)
                if child_node:
                    source_idx = child_node.metadata.get("source_index")
                    if source_idx is not None:
                        child_source_indices.append(source_idx)
            
            if child_source_indices:
                min_idx = min(child_source_indices)
                max_idx = max(child_source_indices)
                if min_idx in chunk_mapping:
                    start_chunk_id = chunk_mapping[min_idx]
                if max_idx in chunk_mapping:
                    end_chunk_id = chunk_mapping[max_idx]
        
        insert_data.append((
            node.node_id,
            version_id,
            node.parent_id,
            node.level,
            node.node_type,
            node.content,
            node.children_ids,
            start_chunk_id,
            end_chunk_id,
            node.metadata,
        ))
    
    # æ‰¹é‡æ’å…¥
    try:
        with conn.cursor() as cur:
            cur.executemany(
                """
                INSERT INTO hierarchical_nodes (
                    node_id, version_id, parent_id, level, node_type,
                    content, children_ids, start_chunk_id, end_chunk_id, metadata
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (node_id) DO NOTHING
                """,
                insert_data
            )
        
        conn.commit()
        success_count = len(nodes)
        
        if show_detail:
            print(f"  âœ… å·²æ’å…¥ {success_count} ä¸ªèŠ‚ç‚¹")
            # æ˜¾ç¤ºå±‚çº§ç»Ÿè®¡
            for level in range(4):
                count = len([n for n in nodes if n.level == level])
                level_name = ["sentence", "paragraph", "section", "document"][level]
                print(f"    - Level {level} ({level_name}): {count}")
        
    except Exception as e:
        conn.rollback()
        fail_count = len(nodes)
        print(f"  âŒ æ’å…¥å¤±è´¥: {e}")
    
    return success_count, fail_count


def process_version(
    conn,
    version_data: dict[str, Any],
    builder: HiChunkBuilder,
    show_detail: bool = False,
) -> tuple[int, int]:
    """å¤„ç†å•ä¸ªæ–‡æ¡£ç‰ˆæœ¬
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_data: ç‰ˆæœ¬æ•°æ®
        builder: HiChunkBuilder å®ä¾‹
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    version_id = version_data["version_id"]
    document_title = version_data["document_title"]
    content_list = version_data["content_list"]
    
    if show_detail:
        print(f"\nğŸ“„ å¤„ç†ç‰ˆæœ¬: {version_id[:8]}...")
        print(f"   æ–‡æ¡£æ ‡é¢˜: {document_title}")
        print(f"   content_list é•¿åº¦: {len(content_list)}")
    
    try:
        # æ­¥éª¤ 1: æ„å»ºå±‚æ¬¡ç»“æ„
        if show_detail:
            print("  ğŸ—ï¸  æ„å»ºå±‚æ¬¡ç»“æ„...", end=" ", flush=True)
        
        nodes = builder.build_hierarchy(content_list, document_title)
        
        if show_detail:
            print(f"âœ… ({len(nodes)} ä¸ªèŠ‚ç‚¹)")
        
        # æ£€æŸ¥èŠ‚ç‚¹æ•°é‡é™åˆ¶
        if len(nodes) > DEFAULT_MAX_NODES_PER_DOC:
            print(f"  âš ï¸  èŠ‚ç‚¹æ•°é‡ ({len(nodes)}) è¶…è¿‡é™åˆ¶ ({DEFAULT_MAX_NODES_PER_DOC})ï¼Œè·³è¿‡")
            return 0, len(nodes)
        
        # æ­¥éª¤ 2: è·å– chunk æ˜ å°„
        if show_detail:
            print("  ğŸ”— è·å– chunk æ˜ å°„...", end=" ", flush=True)
        
        chunk_mapping = get_chunk_mapping(conn, version_id)
        
        if show_detail:
            print(f"âœ… ({len(chunk_mapping)} ä¸ª chunks)")
        
        # æ­¥éª¤ 3: æ’å…¥èŠ‚ç‚¹
        if show_detail:
            print("  ğŸ’¾ æ’å…¥èŠ‚ç‚¹...", end=" ", flush=True)
        
        success, fail = insert_hierarchical_nodes(
            conn, version_id, nodes, chunk_mapping, show_detail
        )
        
        return success, fail
        
    except Exception as e:
        conn.rollback()
        print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
        return 0, 0


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        return f"{seconds/60:.1f}åˆ†é’Ÿ"
    else:
        return f"{seconds/3600:.1f}å°æ—¶"


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # åŠ è½½é…ç½®
    settings = load_settings()
    dsn = settings["DATABASE_URL"]
    
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk)")
    parser.add_argument("--version-id", help="æŒ‡å®šç‰ˆæœ¬ ID")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help=f"æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ {DEFAULT_BATCH_SIZE}ï¼‰")
    parser.add_argument("--limit", type=int, default=DEFAULT_LIMIT, help=f"æœ€å¤§å¤„ç†æ•°é‡ï¼ˆé»˜è®¤ {DEFAULT_LIMIT}ï¼‰")
    parser.add_argument("--max-nodes", type=int, default=DEFAULT_MAX_NODES_PER_DOC, help=f"å•ä¸ªæ–‡æ¡£æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤ {DEFAULT_MAX_NODES_PER_DOC}ï¼‰")
    parser.add_argument("--show-detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¿›åº¦")
    parser.add_argument("--dry-run", action="store_true", help="å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ•°æ®åº“ï¼‰")
    parser.add_argument("--reset", "-r", action="store_true", help="é‡ç½®/æ¸…ç©º hierarchical_nodes è¡¨åé‡æ–°ç”Ÿæˆ")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡ç½®ï¼Œè·³è¿‡ç¡®è®¤æç¤ºï¼ˆé…åˆ --reset ä½¿ç”¨ï¼‰")
    parser.add_argument("--with-embeddings", action="store_true", help="ä¸ºéå¶å­èŠ‚ç‚¹ç”Ÿæˆ embeddingsï¼ˆå¯é€‰ï¼Œè¾ƒæ…¢ï¼‰")
    parser.add_argument("--embedding-model", default=None, help="embedding æ¨¡å‹åç§°")
    args = parser.parse_args()
    
    # è¿æ¥æ•°æ®åº“
    with psycopg.connect(dsn) as conn:
        register_vector(conn)
        
        # å¤„ç†é‡ç½®è¯·æ±‚
        if args.reset:
            if not reset_hierarchical_nodes(conn, args.version_id, args.force):
                sys.exit(0)  # ç”¨æˆ·å–æ¶ˆï¼Œæ­£å¸¸é€€å‡º
            print()
        
        # è·å–åˆå§‹ç»Ÿè®¡
        stats = get_stats(conn, args.version_id)
        
        print("=" * 80)
        print("ğŸš€ å¼€å§‹ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk)")
        print("=" * 80)
        print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        print(f"æœ€å¤§èŠ‚ç‚¹æ•°/æ–‡æ¡£: {args.max_nodes:,}")
        if args.with_embeddings:
            config = get_embedding_config()
            print(f"Embedding æ¨¡å‹: {args.embedding_model or config['model']}")
            print(f"Embedding ç»´åº¦: {config['dim']}")
        if args.version_id:
            print(f"ç‰ˆæœ¬è¿‡æ»¤: {args.version_id}")
        if args.dry_run:
            print("âš ï¸ å¹²è¿è¡Œæ¨¡å¼: ä¸ä¼šå†™å…¥æ•°æ®åº“")
        print()
        
        print("=" * 80)
        print("ğŸ“Š åˆå§‹ç»Ÿè®¡")
        print("=" * 80)
        print(f"  æ€»ç‰ˆæœ¬æ•°:     {stats['total_versions']:,}")
        print(f"  å·²å¤„ç†:       {stats['processed']:,}")
        print(f"  å¾…å¤„ç†:       {stats['to_process']:,}")
        print(f"  ç°æœ‰èŠ‚ç‚¹æ•°:   {stats['total_nodes']:,}")
        print()
        
        if stats['to_process'] == 0:
            print("âœ… æ‰€æœ‰ç‰ˆæœ¬éƒ½å·²å¤„ç†ï¼Œæ— éœ€å¤„ç†")
            return
        
        # ç¡®è®¤å¤„ç†
        to_process = min(stats['to_process'], args.limit)
        print(f"å°†å¤„ç† {to_process} ä¸ªç‰ˆæœ¬ï¼ˆé™åˆ¶: {args.limit}ï¼‰")
        print()
        
        # åˆå§‹åŒ– builder
        builder = HiChunkBuilder()
        
        # åˆå§‹åŒ– embedding å®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        embedding_client = None
        if args.with_embeddings:
            embedding_client = get_embedding_client()
        
        # ä¸»å¾ªç¯
        total_success = 0
        total_fail = 0
        batch_num = 0
        processed = 0
        
        print("=" * 80)
        print("ğŸ”„ å¼€å§‹å¤„ç†")
        print("=" * 80)
        
        while processed < to_process:
            # è®¡ç®—å‰©ä½™éœ€è¦å¤„ç†çš„æ•°é‡
            remaining = to_process - processed
            batch_size = min(args.batch_size, remaining)
            
            # è·å–ä¸€æ‰¹å¾…å¤„ç†ç‰ˆæœ¬
            versions = fetch_pending_versions(
                conn,
                batch_size=batch_size,
                version_id=args.version_id,
            )
            
            if not versions:
                print("æ²¡æœ‰æ›´å¤šæ•°æ®éœ€è¦å¤„ç†")
                break
            
            batch_num += 1
            batch_len = len(versions)
            
            print(f"\næ‰¹æ¬¡ {batch_num:>3}: {batch_len:>3} ä¸ªç‰ˆæœ¬...")
            
            if args.dry_run:
                # å¹²è¿è¡Œæ¨¡å¼ï¼šåªæ‰“å°ï¼Œä¸å®é™…å¤„ç†
                for v in versions:
                    print(f"  â­ï¸  {v['version_id'][:8]}... ({len(v['content_list'])} items)")
                processed += batch_len
                continue
            
            # å¤„ç†æ¯ä¸ªç‰ˆæœ¬
            batch_success = 0
            batch_fail = 0
            
            for version_data in versions:
                success, fail = process_version(
                    conn, version_data, builder,
                    show_detail=args.show_detail,
                )
                batch_success += success
                batch_fail += fail
            
            total_success += batch_success
            total_fail += batch_fail
            processed += batch_len
            
            print(f"  âœ… å®Œæˆ: {batch_success} ä¸ªèŠ‚ç‚¹")
            if batch_fail > 0:
                print(f"  âŒ å¤±è´¥: {batch_fail} ä¸ªèŠ‚ç‚¹")
            
            # æ¯ 10 æ‰¹æ¬¡æ˜¾ç¤ºè¿›åº¦
            if batch_num % 10 == 0:
                progress = 100 * processed / to_process
                elapsed = time.time() - start_time
                eta = (elapsed / processed) * (to_process - processed) if processed > 0 else 0
                print(f"\n  ğŸ“ˆ è¿›åº¦: {processed}/{to_process} ({progress:.1f}%) | å·²ç”¨: {format_duration(elapsed)} | é¢„è®¡å‰©ä½™: {format_duration(eta)}")
        
        # æœ€ç»ˆç»Ÿè®¡
        elapsed = time.time() - start_time
        
        print()
        print("=" * 80)
        print("âœ… å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"æˆåŠŸèŠ‚ç‚¹:     {total_success:,}")
        print(f"å¤±è´¥èŠ‚ç‚¹:     {total_fail:,}")
        print(f"å¤„ç†ç‰ˆæœ¬æ•°:   {processed:,}")
        print(f"ç”¨æ—¶:         {format_duration(elapsed)}")
        if processed > 0:
            print(f"å¹³å‡é€Ÿåº¦:     {processed/elapsed:.2f} ç‰ˆæœ¬/ç§’")
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = get_stats(conn, args.version_id)
        print()
        print("ğŸ“Š æœ€ç»ˆçŠ¶æ€")
        print(f"  å·²å¤„ç†ç‰ˆæœ¬: {final_stats['processed']:,} / {final_stats['total_versions']:,} ({100*final_stats['processed']/final_stats['total_versions']:.1f}%)")
        print(f"  æ€»èŠ‚ç‚¹æ•°:   {final_stats['total_nodes']:,}")
        print(f"  å¾…å¤„ç†:     {final_stats['to_process']:,}")


if __name__ == "__main__":
    main()
