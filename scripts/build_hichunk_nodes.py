#!/usr/bin/env python3
"""æ‰¹é‡ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk Nodes)

Features:
- ä» document_versions è¯»å– content_list
- ä½¿ç”¨ HiChunkBuilder æ„å»º4å±‚æ–‡æ¡£æ ‘ç»“æ„
- å°†èŠ‚ç‚¹æ’å…¥ hierarchical_nodes è¡¨
- è‡ªåŠ¨å…³è” chunks è¡¨ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰
- æ”¯æŒ embeddings ç”Ÿæˆï¼ˆéå¶å­èŠ‚ç‚¹ï¼Œå¯é€‰ï¼‰
- è¿›åº¦æ˜¾ç¤ºå’Œæ¢å¤æœºåˆ¶
- é”™è¯¯å¤„ç†å’Œäº‹åŠ¡å›æ»š

Tree Structure:
- Level 0 (sentence): å¶å­èŠ‚ç‚¹ï¼Œå¯¹åº” content_list å…ƒç´ 
- Level 1 (paragraph): æ®µè½èŠ‚ç‚¹ï¼Œåˆå¹¶ç›¸é‚»å¥å­
- Level 2 (section): ç« èŠ‚èŠ‚ç‚¹ï¼ŒæŒ‰æ ‡é¢˜åˆ†ç»„
- Level 3 (document): æ–‡æ¡£æ ¹èŠ‚ç‚¹
"""

import sys
import time
from functools import lru_cache
from typing import Any

import psycopg
from psycopg.types.json import Jsonb
from pgvector.psycopg import register_vector

from bid_scoring.config import load_settings
from bid_scoring.hichunk import HiChunkBuilder
from bid_scoring.embeddings import get_embedding_client, get_embedding_config


# é…ç½®å‚æ•°
DEFAULT_BATCH_SIZE = 10  # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
DEFAULT_LIMIT = 100  # æ¯æ¬¡è¿è¡Œæœ€å¤§å¤„ç†æ–‡æ¡£æ•°
DEFAULT_MAX_NODES_PER_DOC = 10000  # å•ä¸ªæ–‡æ¡£æœ€å¤§èŠ‚ç‚¹æ•°


def reset_hierarchical_nodes(
    conn, version_id: str | None = None, force: bool = False
) -> bool:
    """é‡ç½®/æ¸…ç©º hierarchical_nodes è¡¨

    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_id: ç‰ˆæœ¬ IDï¼Œå¦‚æœæŒ‡å®šåˆ™åªåˆ é™¤è¯¥ç‰ˆæœ¬çš„è®°å½•ï¼Œå¦åˆ™åˆ é™¤æ‰€æœ‰
        force: æ˜¯å¦è·³è¿‡ç¡®è®¤æç¤º

    Returns:
        True è¡¨ç¤ºå·²é‡ç½®ï¼ŒFalse è¡¨ç¤ºç”¨æˆ·å–æ¶ˆ
    """
    with conn.cursor() as cur:
        # å…ˆæŸ¥è¯¢å°†è¦åˆ é™¤çš„è®°å½•æ•°
        if version_id:
            cur.execute(
                "SELECT COUNT(*) FROM hierarchical_nodes WHERE version_id = %s",
                (version_id,),
            )
        else:
            cur.execute("SELECT COUNT(*) FROM hierarchical_nodes")

        count = cur.fetchone()[0]

        if count == 0:
            print("â„¹ï¸  hierarchical_nodes è¡¨ä¸ºç©ºï¼Œæ— éœ€é‡ç½®")
            return True

        # ç¡®è®¤æç¤º
        if not force:
            scope = f"ç‰ˆæœ¬ '{version_id}'" if version_id else "æ‰€æœ‰ç‰ˆæœ¬"
            print(
                f"\nâš ï¸  è­¦å‘Š: è¿™å°†åˆ é™¤ {scope} çš„ {count} æ¡ hierarchical_nodes è®°å½•ï¼"
            )
            response = input("ç¡®è®¤é‡ç½®? è¾“å…¥ 'yes' ç»§ç»­: ")
            if response.lower() != "yes":
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
                return False

        # æ‰§è¡Œåˆ é™¤
        if version_id:
            cur.execute(
                "DELETE FROM hierarchical_nodes WHERE version_id = %s", (version_id,)
            )
        else:
            cur.execute("DELETE FROM hierarchical_nodes")

        conn.commit()

        scope = f"ç‰ˆæœ¬ '{version_id}'" if version_id else "æ‰€æœ‰ç‰ˆæœ¬"
        print(f"âœ… å·²é‡ç½® {scope} çš„ {count} æ¡è®°å½•")
        return True


def get_stats(conn, version_id: str | None = None) -> dict[str, Any]:
    """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯

    Returns:
        {
            'total_versions': æ€»ç‰ˆæœ¬æ•°,
            'processed_versions': å·²å¤„ç†ç‰ˆæœ¬æ•°,
            'to_process': å¾…å¤„ç†ç‰ˆæœ¬æ•°,
            'total_nodes': æ€»èŠ‚ç‚¹æ•°,
        }
    """
    with conn.cursor() as cur:
        # åŸºç¡€æŸ¥è¯¢ï¼šç»Ÿè®¡æœ‰ chunks çš„ç‰ˆæœ¬
        base_query = """
            SELECT 
                COUNT(DISTINCT c.version_id) as total_versions,
                COUNT(DISTINCT hn.version_id) as processed_versions,
                COUNT(DISTINCT c.version_id) FILTER (WHERE hn.version_id IS NULL) as to_process
            FROM chunks c
            LEFT JOIN hierarchical_nodes hn ON c.version_id = hn.version_id
        """

        params = []
        if version_id:
            base_query += " WHERE c.version_id = %s"
            params.append(version_id)

        cur.execute(base_query, params)
        row = cur.fetchone()

        # ç»Ÿè®¡æ€»èŠ‚ç‚¹æ•°
        nodes_query = "SELECT COUNT(*) FROM hierarchical_nodes"
        nodes_params = []
        if version_id:
            nodes_query += " WHERE version_id = %s"
            nodes_params.append(version_id)

        cur.execute(nodes_query, nodes_params)
        total_nodes = cur.fetchone()[0]

        return {
            "total_versions": row[0] or 0,
            "processed": row[1] or 0,
            "to_process": row[2] or 0,
            "total_nodes": total_nodes,
        }


def fetch_pending_versions(
    conn,
    batch_size: int = DEFAULT_BATCH_SIZE,
    version_id: str | None = None,
) -> list[dict[str, Any]]:
    """è·å–å¾…å¤„ç†çš„æ–‡æ¡£ç‰ˆæœ¬

    ç­–ç•¥:
    1. é€‰æ‹©æœ‰ chunks ä¸”æœªå¤„ç†å±‚æ¬¡åŒ–èŠ‚ç‚¹çš„ç‰ˆæœ¬
    2. ä» chunks è¡¨é‡å»º content_list

    Returns:
        [{
            'version_id': ç‰ˆæœ¬ID,
            'doc_id': æ–‡æ¡£ID,
            'document_title': æ–‡æ¡£æ ‡é¢˜,
            'content_list': content_list åˆ—è¡¨,
        }, ...]
    """
    with conn.cursor() as cur:
        # æŸ¥æ‰¾æœ‰å¾…å¤„ç† chunks çš„ç‰ˆæœ¬
        query = """
            SELECT DISTINCT
                dv.version_id,
                dv.doc_id,
                d.title as document_title
            FROM document_versions dv
            JOIN documents d ON dv.doc_id = d.doc_id
            JOIN chunks c ON dv.version_id = c.version_id
            LEFT JOIN hierarchical_nodes hn ON dv.version_id = hn.version_id
            WHERE hn.version_id IS NULL
        """

        params = []
        if version_id:
            query += " AND dv.version_id = %s"
            params.append(version_id)

        query += " ORDER BY dv.version_id LIMIT %s"
        params.append(batch_size)

        cur.execute(query, params)
        rows = cur.fetchall()

        result = []
        for row in rows:
            version_id_str = str(row[0])

            # ä» chunks è¡¨è·å–è¯¥ç‰ˆæœ¬çš„æ‰€æœ‰ chunks
            cur.execute(
                """
                SELECT 
                    chunk_id,
                    chunk_index,
                    page_idx,
                    bbox,
                    element_type,
                    text_raw,
                    text_level,
                    img_path,
                    image_caption,
                    image_footnote,
                    table_body,
                    table_caption,
                    table_footnote,
                    list_items,
                    sub_type
                FROM chunks 
                WHERE version_id = %s 
                ORDER BY chunk_index
                """,
                (version_id_str,),
            )

            chunks_rows = cur.fetchall()
            content_list = []
            for cr in chunks_rows:
                item = {
                    "chunk_id": str(cr[0]),
                    "chunk_index": cr[1],
                    "page_idx": cr[2] or 0,
                    "bbox": cr[3],
                    "type": cr[4] or "text",
                    "text": cr[5] or "",
                    "text_level": cr[6] or 0,
                    "img_path": cr[7],
                    "image_caption": cr[8],
                    "image_footnote": cr[9],
                    "table_body": cr[10],
                    "table_caption": cr[11],
                    "table_footnote": cr[12],
                    "list_items": cr[13],
                    "sub_type": cr[14],
                }
                content_list.append(item)

            result.append(
                {
                    "version_id": version_id_str,
                    "doc_id": str(row[1]),
                    "document_title": row[2] or "untitled",
                    "content_list": content_list,
                }
            )

        return result


def get_chunk_mapping(conn, version_id: str) -> dict[int, str]:
    """è·å–ç‰ˆæœ¬ä¸‹çš„ chunk æ˜ å°„

    é€šè¿‡ chunk_index æ˜ å°„åˆ° chunk_idï¼Œç”¨äºå…³è”å¶å­èŠ‚ç‚¹

    Returns:
        {chunk_index: chunk_id, ...}
    """
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT chunk_id, chunk_index 
            FROM chunks 
            WHERE version_id = %s 
            ORDER BY chunk_index
            """,
            (version_id,),
        )
        rows = cur.fetchall()

        return {row[1]: str(row[0]) for row in rows}


def insert_hierarchical_nodes(
    conn,
    version_id: str,
    nodes: list,
    chunk_mapping: dict[int, str],
    show_detail: bool = False,
) -> tuple[int, int]:
    """æ’å…¥å±‚æ¬¡åŒ–èŠ‚ç‚¹åˆ°æ•°æ®åº“

    æŒ‰å±‚çº§ä»é«˜åˆ°ä½æ’å…¥ï¼ˆdocument -> section -> paragraph -> sentenceï¼‰ï¼Œ
    ç¡®ä¿çˆ¶èŠ‚ç‚¹å…ˆäºå­èŠ‚ç‚¹æ’å…¥ï¼Œé¿å…å¤–é”®çº¦æŸé”™è¯¯ã€‚

    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_id: ç‰ˆæœ¬ ID
        nodes: HiChunkNode åˆ—è¡¨
        chunk_mapping: chunk_index åˆ° chunk_id çš„æ˜ å°„
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    if not nodes:
        return 0, 0

    success_count = 0
    fail_count = 0

    # æŒ‰å±‚çº§åˆ†ç»„èŠ‚ç‚¹ï¼ˆä»é«˜åˆ°ä½ï¼š3=document, 2=section, 1=paragraph, 0=sentenceï¼‰
    nodes_by_level = {3: [], 2: [], 1: [], 0: []}
    leaf_nodes = []

    for node in nodes:
        nodes_by_level[node.level].append(node)
        if node.level == 0:
            leaf_nodes.append(node)

    nodes_by_id = {n.node_id: n for n in nodes if n.node_id}
    leaf_by_id = {n.node_id: n for n in leaf_nodes if n.node_id}

    @lru_cache(maxsize=None)
    def _covered_unit_range(node_id: str) -> tuple[int, int] | None:
        n = nodes_by_id.get(node_id)
        if not n:
            return None
        if n.level == 0:
            source_idx = n.metadata.get("source_index")
            if source_idx is None:
                return None
            i = int(source_idx)
            return (i, i)

        ranges: list[tuple[int, int]] = []
        for child_id in n.children_ids or []:
            r = _covered_unit_range(child_id)
            if r is not None:
                ranges.append(r)
        if not ranges:
            return None
        return (min(r[0] for r in ranges), max(r[1] for r in ranges))

    # å‡†å¤‡æ‰€æœ‰å±‚çº§çš„æ’å…¥æ•°æ®
    def prepare_insert_data(node_list):
        data = []
        for node in node_list:
            cov = _covered_unit_range(node.node_id)
            if cov is not None:
                node.metadata["covered_unit_range"] = {"start": cov[0], "end": cov[1]}

            # å¯¹äºå¶å­èŠ‚ç‚¹ï¼Œå°è¯•å…³è” chunks
            start_chunk_id = None
            end_chunk_id = None

            if node.level == 0:
                source_idx = node.metadata.get("source_index")
                if source_idx is not None and source_idx in chunk_mapping:
                    start_chunk_id = chunk_mapping[source_idx]
                    end_chunk_id = chunk_mapping[source_idx]
            elif node.level == 1:  # paragraph
                # å¯¹äºæ®µè½ï¼Œå…³è”å…¶åŒ…å«çš„å¶å­èŠ‚ç‚¹çš„ chunks
                child_source_indices = []
                for child_id in node.children_ids:
                    child_node = leaf_by_id.get(child_id)
                    if child_node:
                        source_idx = child_node.metadata.get("source_index")
                        if source_idx is not None:
                            child_source_indices.append(source_idx)

                if child_source_indices:
                    min_idx = min(child_source_indices)
                    max_idx = max(child_source_indices)
                    if min_idx in chunk_mapping:
                        start_chunk_id = chunk_mapping[min_idx]
                    if max_idx in chunk_mapping:
                        end_chunk_id = chunk_mapping[max_idx]

            data.append(
                (
                    node.node_id,
                    version_id,
                    node.parent_id,
                    node.level,
                    node.node_type,
                    node.content,
                    node.children_ids,
                    start_chunk_id,
                    end_chunk_id,
                    Jsonb(node.metadata),
                )
            )
        return data

    # æŒ‰å±‚çº§é¡ºåºæ’å…¥ï¼š3 -> 2 -> 1 -> 0
    try:
        with conn.cursor() as cur:
            for level in [3, 2, 1, 0]:
                level_nodes = nodes_by_level[level]
                if not level_nodes:
                    continue

                insert_data = prepare_insert_data(level_nodes)
                cur.executemany(
                    """
                    INSERT INTO hierarchical_nodes (
                        node_id, version_id, parent_id, level, node_type,
                        content, children_ids, start_chunk_id, end_chunk_id, metadata
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (node_id) DO NOTHING
                    """,
                    insert_data,
                )
                success_count += len(level_nodes)

        conn.commit()

        if show_detail:
            print(f"  âœ… å·²æ’å…¥ {success_count} ä¸ªèŠ‚ç‚¹")
            # æ˜¾ç¤ºå±‚çº§ç»Ÿè®¡
            for level in range(4):
                count = len(nodes_by_level[level])
                level_name = ["sentence", "paragraph", "section", "document"][level]
                print(f"    - Level {level} ({level_name}): {count}")

    except Exception as e:
        conn.rollback()
        fail_count = len(nodes) - success_count
        print(f"  âŒ æ’å…¥å¤±è´¥: {e}")

    return success_count, fail_count


def process_version(
    conn,
    version_data: dict[str, Any],
    builder: HiChunkBuilder,
    show_detail: bool = False,
) -> tuple[int, int]:
    """å¤„ç†å•ä¸ªæ–‡æ¡£ç‰ˆæœ¬

    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_data: ç‰ˆæœ¬æ•°æ®
        builder: HiChunkBuilder å®ä¾‹
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    version_id = version_data["version_id"]
    document_title = version_data["document_title"]
    content_list = version_data["content_list"]

    if show_detail:
        print(f"\nğŸ“„ å¤„ç†ç‰ˆæœ¬: {version_id[:8]}...")
        print(f"   æ–‡æ¡£æ ‡é¢˜: {document_title}")
        print(f"   content_list é•¿åº¦: {len(content_list)}")

    try:
        # æ­¥éª¤ 1: æ„å»ºå±‚æ¬¡ç»“æ„
        if show_detail:
            print("  ğŸ—ï¸  æ„å»ºå±‚æ¬¡ç»“æ„...", end=" ", flush=True)

        nodes = builder.build_hierarchy(content_list, document_title)

        if show_detail:
            print(f"âœ… ({len(nodes)} ä¸ªèŠ‚ç‚¹)")

        # æ£€æŸ¥èŠ‚ç‚¹æ•°é‡é™åˆ¶
        if len(nodes) > DEFAULT_MAX_NODES_PER_DOC:
            print(
                f"  âš ï¸  èŠ‚ç‚¹æ•°é‡ ({len(nodes)}) è¶…è¿‡é™åˆ¶ ({DEFAULT_MAX_NODES_PER_DOC})ï¼Œè·³è¿‡"
            )
            return 0, len(nodes)

        # æ­¥éª¤ 2: è·å– chunk æ˜ å°„
        if show_detail:
            print("  ğŸ”— è·å– chunk æ˜ å°„...", end=" ", flush=True)

        chunk_mapping = get_chunk_mapping(conn, version_id)

        if show_detail:
            print(f"âœ… ({len(chunk_mapping)} ä¸ª chunks)")

        # æ­¥éª¤ 3: æ’å…¥èŠ‚ç‚¹
        if show_detail:
            print("  ğŸ’¾ æ’å…¥èŠ‚ç‚¹...", end=" ", flush=True)

        success, fail = insert_hierarchical_nodes(
            conn, version_id, nodes, chunk_mapping, show_detail
        )

        return success, fail

    except Exception as e:
        conn.rollback()
        print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
        return 0, 0


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        return f"{seconds / 60:.1f}åˆ†é’Ÿ"
    else:
        return f"{seconds / 3600:.1f}å°æ—¶"


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()

    # åŠ è½½é…ç½®
    settings = load_settings()
    dsn = settings["DATABASE_URL"]

    # è·å–å‘½ä»¤è¡Œå‚æ•°
    import argparse

    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk)")
    parser.add_argument("--version-id", help="æŒ‡å®šç‰ˆæœ¬ ID")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ {DEFAULT_BATCH_SIZE}ï¼‰",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=DEFAULT_LIMIT,
        help=f"æœ€å¤§å¤„ç†æ•°é‡ï¼ˆé»˜è®¤ {DEFAULT_LIMIT}ï¼‰",
    )
    parser.add_argument(
        "--max-nodes",
        type=int,
        default=DEFAULT_MAX_NODES_PER_DOC,
        help=f"å•ä¸ªæ–‡æ¡£æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤ {DEFAULT_MAX_NODES_PER_DOC}ï¼‰",
    )
    parser.add_argument("--show-detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¿›åº¦")
    parser.add_argument(
        "--dry-run", action="store_true", help="å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ•°æ®åº“ï¼‰"
    )
    parser.add_argument(
        "--reset",
        "-r",
        action="store_true",
        help="é‡ç½®/æ¸…ç©º hierarchical_nodes è¡¨åé‡æ–°ç”Ÿæˆ",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡ç½®ï¼Œè·³è¿‡ç¡®è®¤æç¤ºï¼ˆé…åˆ --reset ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--with-embeddings",
        action="store_true",
        help="ä¸ºéå¶å­èŠ‚ç‚¹ç”Ÿæˆ embeddingsï¼ˆå¯é€‰ï¼Œè¾ƒæ…¢ï¼‰",
    )
    parser.add_argument("--embedding-model", default=None, help="embedding æ¨¡å‹åç§°")
    args = parser.parse_args()

    # è¿æ¥æ•°æ®åº“
    with psycopg.connect(dsn) as conn:
        register_vector(conn)

        # å¤„ç†é‡ç½®è¯·æ±‚
        if args.reset:
            if not reset_hierarchical_nodes(conn, args.version_id, args.force):
                sys.exit(0)  # ç”¨æˆ·å–æ¶ˆï¼Œæ­£å¸¸é€€å‡º
            print()

        # è·å–åˆå§‹ç»Ÿè®¡
        stats = get_stats(conn, args.version_id)

        print("=" * 80)
        print("ğŸš€ å¼€å§‹ç”Ÿæˆå±‚æ¬¡åŒ–æ–‡æ¡£èŠ‚ç‚¹ (HiChunk)")
        print("=" * 80)
        print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        print(f"æœ€å¤§èŠ‚ç‚¹æ•°/æ–‡æ¡£: {args.max_nodes:,}")
        if args.with_embeddings:
            config = get_embedding_config()
            print(f"Embedding æ¨¡å‹: {args.embedding_model or config['model']}")
            print(f"Embedding ç»´åº¦: {config['dim']}")
        if args.version_id:
            print(f"ç‰ˆæœ¬è¿‡æ»¤: {args.version_id}")
        if args.dry_run:
            print("âš ï¸ å¹²è¿è¡Œæ¨¡å¼: ä¸ä¼šå†™å…¥æ•°æ®åº“")
        print()

        print("=" * 80)
        print("ğŸ“Š åˆå§‹ç»Ÿè®¡")
        print("=" * 80)
        print(f"  æ€»ç‰ˆæœ¬æ•°:     {stats['total_versions']:,}")
        print(f"  å·²å¤„ç†:       {stats['processed']:,}")
        print(f"  å¾…å¤„ç†:       {stats['to_process']:,}")
        print(f"  ç°æœ‰èŠ‚ç‚¹æ•°:   {stats['total_nodes']:,}")
        print()

        if stats["to_process"] == 0:
            print("âœ… æ‰€æœ‰ç‰ˆæœ¬éƒ½å·²å¤„ç†ï¼Œæ— éœ€å¤„ç†")
            return

        # ç¡®è®¤å¤„ç†
        to_process = min(stats["to_process"], args.limit)
        print(f"å°†å¤„ç† {to_process} ä¸ªç‰ˆæœ¬ï¼ˆé™åˆ¶: {args.limit}ï¼‰")
        print()

        # åˆå§‹åŒ– builder
        builder = HiChunkBuilder()

        # åˆå§‹åŒ– embedding å®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.with_embeddings:
            _embedding_client = get_embedding_client()

        # ä¸»å¾ªç¯
        total_success = 0
        total_fail = 0
        batch_num = 0
        processed = 0

        print("=" * 80)
        print("ğŸ”„ å¼€å§‹å¤„ç†")
        print("=" * 80)

        while processed < to_process:
            # è®¡ç®—å‰©ä½™éœ€è¦å¤„ç†çš„æ•°é‡
            remaining = to_process - processed
            batch_size = min(args.batch_size, remaining)

            # è·å–ä¸€æ‰¹å¾…å¤„ç†ç‰ˆæœ¬
            versions = fetch_pending_versions(
                conn,
                batch_size=batch_size,
                version_id=args.version_id,
            )

            if not versions:
                print("æ²¡æœ‰æ›´å¤šæ•°æ®éœ€è¦å¤„ç†")
                break

            batch_num += 1
            batch_len = len(versions)

            print(f"\næ‰¹æ¬¡ {batch_num:>3}: {batch_len:>3} ä¸ªç‰ˆæœ¬...")

            if args.dry_run:
                # å¹²è¿è¡Œæ¨¡å¼ï¼šåªæ‰“å°ï¼Œä¸å®é™…å¤„ç†
                for v in versions:
                    print(
                        f"  â­ï¸  {v['version_id'][:8]}... ({len(v['content_list'])} items)"
                    )
                processed += batch_len
                continue

            # å¤„ç†æ¯ä¸ªç‰ˆæœ¬
            batch_success = 0
            batch_fail = 0

            for version_data in versions:
                success, fail = process_version(
                    conn,
                    version_data,
                    builder,
                    show_detail=args.show_detail,
                )
                batch_success += success
                batch_fail += fail

            total_success += batch_success
            total_fail += batch_fail
            processed += batch_len

            print(f"  âœ… å®Œæˆ: {batch_success} ä¸ªèŠ‚ç‚¹")
            if batch_fail > 0:
                print(f"  âŒ å¤±è´¥: {batch_fail} ä¸ªèŠ‚ç‚¹")

            # æ¯ 10 æ‰¹æ¬¡æ˜¾ç¤ºè¿›åº¦
            if batch_num % 10 == 0:
                progress = 100 * processed / to_process
                elapsed = time.time() - start_time
                eta = (
                    (elapsed / processed) * (to_process - processed)
                    if processed > 0
                    else 0
                )
                print(
                    f"\n  ğŸ“ˆ è¿›åº¦: {processed}/{to_process} ({progress:.1f}%) | å·²ç”¨: {format_duration(elapsed)} | é¢„è®¡å‰©ä½™: {format_duration(eta)}"
                )

        # æœ€ç»ˆç»Ÿè®¡
        elapsed = time.time() - start_time

        print()
        print("=" * 80)
        print("âœ… å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"æˆåŠŸèŠ‚ç‚¹:     {total_success:,}")
        print(f"å¤±è´¥èŠ‚ç‚¹:     {total_fail:,}")
        print(f"å¤„ç†ç‰ˆæœ¬æ•°:   {processed:,}")
        print(f"ç”¨æ—¶:         {format_duration(elapsed)}")
        if processed > 0:
            print(f"å¹³å‡é€Ÿåº¦:     {processed / elapsed:.2f} ç‰ˆæœ¬/ç§’")

        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = get_stats(conn, args.version_id)
        print()
        print("ğŸ“Š æœ€ç»ˆçŠ¶æ€")
        print(
            f"  å·²å¤„ç†ç‰ˆæœ¬: {final_stats['processed']:,} / {final_stats['total_versions']:,} ({100 * final_stats['processed'] / final_stats['total_versions']:.1f}%)"
        )
        print(f"  æ€»èŠ‚ç‚¹æ•°:   {final_stats['total_nodes']:,}")
        print(f"  å¾…å¤„ç†:     {final_stats['to_process']:,}")


if __name__ == "__main__":
    main()
