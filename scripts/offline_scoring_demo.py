#!/usr/bin/env python3
"""
å›æ ‡åˆ†æè¯„åˆ† Schema - ç¦»çº¿æ•°æ®é›†æˆæµ‹è¯•

ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢å·²å¤„ç†çš„æ–‡æœ¬å†…å®¹ï¼Œæ— éœ€è°ƒç”¨ OpenAI APIã€‚

Usage:
    cd /Users/wangxq/Documents/æŠ•æ ‡åˆ†æ_kimi
    source .venv/bin/activate
    python scripts/offline_scoring_demo.py
"""

import os
import sys
import re
from datetime import datetime, timezone

sys.path.insert(0, '/Users/wangxq/Documents/æŠ•æ ‡åˆ†æ_kimi')

import psycopg
from dotenv import load_dotenv
load_dotenv(override=True)

from bid_scoring.scoring_schema import (
    BoundingBox, EvidenceItem, DurationEvidence, ResponseTimeEvidence, WarrantyEvidence,
    EvidenceField, ConflictResolutionStrategy,
    TrainingPlan, AfterSalesService, ScoringResult, DimensionScore,
    CompletenessLevel, ServiceLevel,
)
from bid_scoring.config import load_settings


# æ–‡æ¡£ç‰ˆæœ¬ ID
VERSION_ID = "9a5a0214-3b98-4a64-9194-a01648479f7a"


def extract_from_chunks(
    conn,
    version_id: str,
    keywords: list[str],
    field_name: str,
    evidence_type: str = "base",
) -> list[EvidenceItem]:
    """
    ä»æ•°æ®åº“ chunks ä¸­æå–è¯æ®
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_id: æ–‡æ¡£ç‰ˆæœ¬ ID
        keywords: å…³é”®è¯åˆ—è¡¨
        field_name: å­—æ®µå
        evidence_type: è¯æ®ç±»å‹
    
    Returns:
        æå–çš„è¯æ®åˆ—è¡¨
    """
    evidences = []
    
    with conn.cursor() as cur:
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = []
        params = [version_id]
        
        for keyword in keywords:
            conditions.append("content_for_embedding ILIKE %s")
            params.append(f'%{keyword}%')
        
        query = f"""
            SELECT node_id, heading, page_range, content_for_embedding
            FROM hierarchical_nodes
            WHERE version_id = %s 
              AND level = 2
              AND ({' OR '.join(conditions)})
            ORDER BY 
                CASE 
                    WHEN content_for_embedding ILIKE %s THEN 1
                    ELSE 2
                END,
                char_count DESC
            LIMIT 5
        """
        # æ·»åŠ ä¼˜å…ˆçº§æ’åºå‚æ•°
        params.append(f'%{keywords[0]}%')
        
        cur.execute(query, params)
        
        for row in cur.fetchall():
            node_id, heading, page_range, content = row
            
            # æ„å»ºè¯æ®
            evidence_data = {
                "field_name": field_name,
                "field_value": extract_value_from_content(content, field_name),
                "source_text": content[:200],
                "page_idx": page_range[0] if page_range else 0,
                "bbox": BoundingBox(x1=0, y1=0, x2=100, y2=100),  # ç®€åŒ–è¾¹ç•Œæ¡†
                "chunk_id": str(node_id),
                "confidence": calculate_confidence(content, keywords),
            }
            
            # æ ¹æ®ç±»å‹åˆ›å»ºè¯æ®
            if evidence_type == "duration":
                parsed = parse_duration(content)
                if parsed.get("days") or parsed.get("hours"):
                    evidence = DurationEvidence(
                        **evidence_data,
                        raw_value=content[:100],
                        **parsed,
                    )
                else:
                    # å›é€€åˆ°åŸºç¡€è¯æ®
                    evidence = EvidenceItem(**evidence_data)
            elif evidence_type == "response_time":
                parsed = parse_response_time(content)
                evidence = ResponseTimeEvidence(
                    **evidence_data,
                    raw_value=content[:100],
                    **parsed,
                )
            elif evidence_type == "warranty":
                parsed = parse_warranty(content)
                evidence = WarrantyEvidence(
                    **evidence_data,
                    raw_value=content[:100],
                    **parsed,
                )
            else:
                evidence = EvidenceItem(**evidence_data)
            
            evidences.append(evidence)
    
    return evidences


def extract_value_from_content(content: str, field_name: str) -> str:
    """ä»å†…å®¹ä¸­æå–å€¼"""
    # ç®€åŒ–å¤„ç†ï¼šå–å‰50ä¸ªå­—ç¬¦ä½œä¸ºå€¼
    value = content.strip()[:50]
    return value


def calculate_confidence(content: str, keywords: list[str]) -> float:
    """è®¡ç®—ç½®ä¿¡åº¦"""
    base_confidence = 0.75
    
    # æ ¹æ®å…³é”®è¯åŒ¹é…ç¨‹åº¦è°ƒæ•´
    content_lower = content.lower()
    matches = sum(1 for k in keywords if k.lower() in content_lower)
    
    if matches >= 2:
        base_confidence += 0.1
    if matches >= 3:
        base_confidence += 0.05
    
    # æ ¹æ®å†…å®¹é•¿åº¦è°ƒæ•´ï¼ˆè¾ƒé•¿å†…å®¹é€šå¸¸ä¿¡æ¯æ›´ä¸°å¯Œï¼‰
    if len(content) > 200:
        base_confidence += 0.05
    
    return min(base_confidence, 0.95)


def parse_duration(text: str) -> dict:
    """è§£ææ—¶é•¿ä¿¡æ¯"""
    result = {"days": None, "hours": None}
    
    # åŒ¹é…å¤©æ•°
    match = re.search(r'(\d+)\s*[å¤©æ—¥]', text)
    if match:
        result["days"] = float(match.group(1))
    
    # åŒ¹é…å°æ—¶æ•°
    match = re.search(r'(\d+)\s*å°æ—¶', text)
    if match:
        result["hours"] = float(match.group(1))
    
    return result


def parse_response_time(text: str) -> dict:
    """è§£æå“åº”æ—¶é—´"""
    result = {"response_hours": None, "on_site_hours": None}
    
    # åŒ¹é…å“åº”æ—¶é—´
    match = re.search(r'(\d+)\s*å°æ—¶[å†…]?', text)
    if match and match.group(1):
        result["response_hours"] = float(match.group(1))
    
    # åŒ¹é…åˆ°åœºæ—¶é—´ (ä½¿ç”¨ç‹¬ç«‹çš„å˜é‡)
    match2 = re.search(r'(\d+)\s*å°æ—¶.*åˆ°åœº|ç°åœº', text)
    if match2 and match2.group(1):
        result["on_site_hours"] = float(match2.group(1))
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å°æ—¶æ•°ï¼Œæ ¹æ®å…³é”®è¯åˆ¤æ–­
    if result["response_hours"] is None and ("å³æ—¶" in text or "ç«‹å³" in text or "é©¬ä¸Š" in text):
        result["response_hours"] = 1.0
    
    return result


def parse_warranty(text: str) -> dict:
    """è§£æè´¨ä¿æœŸé™"""
    result = {"years": None, "months": None}
    
    # åŒ¹é…å¹´æ•°
    match = re.search(r'(\d+)\s*å¹´', text)
    if match:
        result["years"] = float(match.group(1))
    
    # åŒ¹é…æœˆæ•°
    match = re.search(r'(\d+)\s*ä¸ªæœˆ', text)
    if match:
        result["months"] = int(match.group(1))
    
    # åŒ¹é… "60ä¸ªæœˆ" è¿™æ ·çš„æ ¼å¼
    match = re.search(r'(\d+)\s*ä¸ª?æœˆ', text)
    if match and not result["months"]:
        months = int(match.group(1))
        if months > 12:
            result["years"] = months / 12
        else:
            result["months"] = months
    
    return result


def score_training_plan(conn, version_id: str) -> TrainingPlan:
    """è¯„åˆ†ï¼šåŸ¹è®­æ–¹æ¡ˆ"""
    print("\n" + "="*70)
    print("ğŸ“š è¯„åˆ†ç»´åº¦: åŸ¹è®­æ–¹æ¡ˆ")
    print("="*70)
    
    plan = TrainingPlan(
        dimension_id="training",
        dimension_name="åŸ¹è®­æ–¹æ¡ˆ",
        weight=5.0,
        sequence=1,
    )
    
    # å®šä¹‰æŸ¥è¯¢é…ç½®
    training_configs = {
        "training_duration": {
            "keywords": ["åŸ¹è®­", "å¤©æ•°", "å°æ—¶", "æ—¶é•¿"],
            "field_name": "åŸ¹è®­æ—¶é•¿",
            "type": "duration",
        },
        "training_schedule": {
            "keywords": ["åŸ¹è®­å†…å®¹", "åŸ¹è®­è¯¾ç¨‹", "åŸ¹è®­æ–¹å¼", "ç°åœºæˆè¯¾"],
            "field_name": "åŸ¹è®­è®¡åˆ’",
            "type": "base",
        },
        "training_personnel": {
            "keywords": ["åŸ¹è®­äººå‘˜", "åŸ¹è®­å¯¹è±¡", "ä½¿ç”¨äººå‘˜", "ç®¡ç†äººå‘˜"],
            "field_name": "åŸ¹è®­äººå‘˜",
            "type": "base",
        },
        "instructor_qualifications": {
            "keywords": ["è®²å¸ˆ", "æˆè¯¾è€å¸ˆ", "åŸ¹è®­å¸ˆèµ„", "å·¥ç¨‹å¸ˆ"],
            "field_name": "æˆè¯¾è€å¸ˆèµ„è´¨",
            "type": "base",
        },
    }
    
    # æå–æ¯ä¸ªå­—æ®µçš„è¯æ®
    for attr, config in training_configs.items():
        print(f"\nğŸ” æŸ¥æ‰¾: {config['field_name']}")
        
        evidences = extract_from_chunks(
            conn=conn,
            version_id=version_id,
            keywords=config["keywords"],
            field_name=config["field_name"],
            evidence_type=config["type"],
        )
        
        if evidences:
            print(f"   âœ“ æ‰¾åˆ° {len(evidences)} ä¸ªç›¸å…³æ®µè½")
            
            field = EvidenceField(field_name=config["field_name"])
            
            for ev in evidences:
                field.add_candidate(ev)
                print(f"     - {ev.field_value[:40]}... (ç½®ä¿¡åº¦: {ev.confidence:.2f})")
            
            # è§£å†³å†²çª
            field.resolve_conflict(strategy=ConflictResolutionStrategy.HIGHEST_CONFIDENCE)
            setattr(plan, attr, field)
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
    
    # è®¡ç®—è¯„åˆ†
    completeness = plan.evaluate_completeness()
    score = plan.calculate_score()
    
    print(f"\nğŸ“Š åŸ¹è®­æ–¹æ¡ˆè¯„åˆ†:")
    print(f"   å®Œæ•´æ€§: {completeness.value}")
    print(f"   å¾—åˆ†: {score}/{plan.weight}")
    print(f"   å¾—åˆ†ç‡: {plan.get_score_ratio():.1%}")
    
    return plan


def score_after_sales_service(conn, version_id: str) -> AfterSalesService:
    """è¯„åˆ†ï¼šå”®åæœåŠ¡æ–¹æ¡ˆ"""
    print("\n" + "="*70)
    print("ğŸ”§ è¯„åˆ†ç»´åº¦: å”®åæœåŠ¡æ–¹æ¡ˆ")
    print("="*70)
    
    service = AfterSalesService(
        dimension_id="after_sales",
        dimension_name="å”®åæœåŠ¡æ–¹æ¡ˆ",
        weight=10.0,
        sequence=2,
    )
    
    # å®šä¹‰æŸ¥è¯¢é…ç½®
    service_configs = {
        "response_time": {
            "keywords": ["å“åº”æ—¶é—´", "å“åº”", "åˆ°è¾¾ç°åœº", "ä¸Šé—¨"],
            "field_name": "å“åº”æ—¶é—´",
            "type": "response_time",
        },
        "warranty_period": {
            "keywords": ["è´¨ä¿", "ä¿ä¿®", "ä¿ä¿®æœŸ", "è´¨é‡ä¿è¯"],
            "field_name": "è´¨ä¿æœŸé™",
            "type": "warranty",
        },
        "parts_supply_period": {
            "keywords": ["é…ä»¶", "è€—æ", "ä¾›åº”", "å¤‡ä»¶"],
            "field_name": "é…ä»¶ä¾›åº”æœŸé™",
            "type": "base",
        },
        "post_warranty_service_fee": {
            "keywords": ["è¿‡ä¿", "è´¨ä¿æœŸå", "ä¿ä¿®æœŸå", "æœåŠ¡è´¹ç”¨"],
            "field_name": "è´¨ä¿æœŸåæœåŠ¡è´¹",
            "type": "base",
        },
    }
    
    # æå–æ¯ä¸ªå­—æ®µçš„è¯æ®
    for attr, config in service_configs.items():
        print(f"\nğŸ” æŸ¥æ‰¾: {config['field_name']}")
        
        evidences = extract_from_chunks(
            conn=conn,
            version_id=version_id,
            keywords=config["keywords"],
            field_name=config["field_name"],
            evidence_type=config["type"],
        )
        
        if evidences:
            print(f"   âœ“ æ‰¾åˆ° {len(evidences)} ä¸ªç›¸å…³æ®µè½")
            
            field = EvidenceField(field_name=config["field_name"])
            
            for ev in evidences:
                field.add_candidate(ev)
                print(f"     - {ev.field_value[:40]}... (ç½®ä¿¡åº¦: {ev.confidence:.2f})")
                
                # æ‰“å°ç»“æ„åŒ–è§£æç»“æœ
                if isinstance(ev, (DurationEvidence, ResponseTimeEvidence, WarrantyEvidence)):
                    if hasattr(ev, 'total_hours') and ev.total_hours:
                        print(f"       è§£æ: {ev.total_hours}å°æ—¶")
                    if hasattr(ev, 'response_hours') and ev.response_hours:
                        print(f"       è§£æ: å“åº”{ev.response_hours}å°æ—¶")
                    if hasattr(ev, 'total_months') and ev.total_months:
                        print(f"       è§£æ: {ev.total_months}ä¸ªæœˆ ({ev.total_months/12:.1f}å¹´)")
            
            field.resolve_conflict(strategy=ConflictResolutionStrategy.HIGHEST_CONFIDENCE)
            setattr(service, attr, field)
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
    
    # è®¡ç®—è¯„åˆ†
    completeness = service.evaluate_completeness()
    service_level = service.evaluate_service_level()
    score = service.calculate_score()
    
    print(f"\nğŸ“Š å”®åæœåŠ¡è¯„åˆ†:")
    print(f"   å®Œæ•´æ€§: {completeness.value}")
    print(f"   æœåŠ¡ç­‰çº§: {service_level.value}")
    print(f"   å¾—åˆ†: {score}/{service.weight}")
    print(f"   å¾—åˆ†ç‡: {service.get_score_ratio():.1%}")
    
    return service


def generate_final_report(
    dimensions: list,
    version_id: str,
) -> ScoringResult:
    """ç”Ÿæˆæœ€ç»ˆè¯„åˆ†æŠ¥å‘Š"""
    
    dimension_scores = []
    total_score = 0.0
    max_possible = 0.0
    
    for dim in dimensions:
        score = dim.calculate_score()
        completeness = dim.evaluate_completeness()
        
        dim_score = DimensionScore(
            dimension_id=dim.dimension_id,
            dimension_name=dim.dimension_name,
            weight=dim.weight,
            score=score,
            completeness=completeness,
            evidence_count=len(dim.extracted_evidence),
        )
        
        dimension_scores.append(dim_score)
        total_score += score
        max_possible += dim.weight
    
    result = ScoringResult(
        bid_id="bid-253135-å¦™ç”Ÿ",
        document_version_id=version_id,
        dimension_scores=dimension_scores,
        total_score=total_score,
        max_possible_score=max_possible,
    )
    
    return result


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸ¯ å›æ ‡åˆ†æè¯„åˆ†ç³»ç»Ÿ - ç¦»çº¿æ•°æ®é›†æˆæµ‹è¯•")
    print("="*70)
    print(f"\næ–‡æ¡£ç‰ˆæœ¬: {VERSION_ID}")
    print(f"æŠ•æ ‡æ–¹: ä¸Šæµ·å¦™ç”Ÿç§‘è´¸æœ‰é™å…¬å¸")
    print(f"é¡¹ç›®: å…±èšç„¦æ˜¾å¾®é•œ")
    print(f"æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åŠ è½½é…ç½®
    settings = load_settings()
    
    try:
        # è¿æ¥æ•°æ®åº“
        with psycopg.connect(settings["DATABASE_URL"]) as conn:
            print("\nâœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # æ‰§è¡Œè¯„åˆ†
            training = score_training_plan(conn, VERSION_ID)
            after_sales = score_after_sales_service(conn, VERSION_ID)
            
            # ç”ŸæˆæŠ¥å‘Š
            result = generate_final_report([training, after_sales], VERSION_ID)
            
            # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            print("\n" + "="*70)
            print("ğŸ“‹ æœ€ç»ˆè¯„åˆ†æŠ¥å‘Š")
            print("="*70)
            
            for ds in result.dimension_scores:
                print(f"\n{ds.dimension_name}")
                print(f"  æƒé‡: {ds.weight}åˆ†")
                print(f"  å¾—åˆ†: {ds.score:.1f}åˆ†")
                print(f"  å¾—åˆ†ç‡: {ds.score/ds.weight:.1%}")
                print(f"  å®Œæ•´æ€§: {ds.completeness.value}")
                print(f"  è¯æ®æ•°: {ds.evidence_count}")
            
            print("\n" + "-"*70)
            print(f"æ€»åˆ†: {result.total_score:.1f}/{result.max_possible_score:.1f}")
            print(f"å¾—åˆ†ç‡: {result.score_percentage:.1f}%")
            print(f"è¯„å®¡ç»“æœ: {'âœ… é€šè¿‡' if result.is_passing else 'âŒ æœªé€šè¿‡'}")
            print("-"*70)
            
            # ä¿å­˜ç»“æœ
            output_file = "/tmp/scoring_result_offline.json"
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(result.model_dump_json(indent=2))
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
    except Exception as e:
        print(f"\nâŒ è¯„åˆ†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
