#!/usr/bin/env python3
"""æ‰¹é‡ç”Ÿæˆä¸Šä¸‹æ–‡å¢å¼º chunks - Contextual Retrieval å®ç°

Features:
- æ™ºèƒ½åˆ†æ‰¹: æŒ‰ chunk æ•°é‡åˆ†æ‰¹ï¼Œå¹³è¡¡ LLM è°ƒç”¨æ•ˆç‡
- æ‰¹é‡å¤„ç†: 5-10æ¡/æ‰¹ï¼ˆLLM ç”Ÿæˆä¸Šä¸‹æ–‡ï¼‰
- è¿›åº¦æ˜¾ç¤º: å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
- é”™è¯¯å¤„ç†: æ‰¹æ¬¡å¤±è´¥è‡ªåŠ¨å›æ»šï¼Œæ”¯æŒä¸­æ–­æ¢å¤
- ç©ºæ–‡æœ¬è¿‡æ»¤: è‡ªåŠ¨è·³è¿‡æ— æ–‡æœ¬çš„ chunks
- æ¢å¤æœºåˆ¶: è‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„ chunks
- ç»Ÿè®¡æŠ¥å‘Š: å¤„ç†å®Œæˆåæ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡

æœ€ä½³å®è·µå‚è€ƒ:
- Anthropic Contextual Retrieval: https://www.anthropic.com/news/contextual-retrieval
- OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings
"""

import sys
import time
from datetime import datetime
from typing import Any

import psycopg
from pgvector.psycopg import register_vector

from bid_scoring.config import load_settings
from bid_scoring.embeddings import embed_texts, estimate_tokens, get_embedding_client, get_embedding_config
from bid_scoring.contextual_retrieval import ContextualRetrievalGenerator


# é…ç½®å‚æ•°ï¼ˆå¯æ ¹æ®ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
DEFAULT_BATCH_SIZE = 5       # æ¯æ‰¹å¤„ç†æ•°é‡ï¼ˆLLM è°ƒç”¨ï¼Œæ¨è 5-10ï¼‰
DEFAULT_LIMIT = 500          # æ¯æ¬¡è¿è¡Œæœ€å¤§å¤„ç†æ•°é‡
DEFAULT_MAX_TOKENS = 50000   # æ¯æ‰¹æœ€å¤§ token æ•°ï¼ˆä¸Šä¸‹æ–‡ç”Ÿæˆï¼‰


def reset_contextual_chunks(conn, version_id: str | None = None, force: bool = False) -> bool:
    """é‡ç½®/æ¸…ç©º contextual_chunks è¡¨
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        version_id: ç‰ˆæœ¬ IDï¼Œå¦‚æœæŒ‡å®šåˆ™åªåˆ é™¤è¯¥ç‰ˆæœ¬çš„è®°å½•ï¼Œå¦åˆ™åˆ é™¤æ‰€æœ‰
        force: æ˜¯å¦è·³è¿‡ç¡®è®¤æç¤º
    
    Returns:
        True è¡¨ç¤ºå·²é‡ç½®ï¼ŒFalse è¡¨ç¤ºç”¨æˆ·å–æ¶ˆ
    """
    with conn.cursor() as cur:
        # å…ˆæŸ¥è¯¢å°†è¦åˆ é™¤çš„è®°å½•æ•°
        if version_id:
            cur.execute(
                "SELECT COUNT(*) FROM contextual_chunks WHERE version_id = %s",
                (version_id,)
            )
        else:
            cur.execute("SELECT COUNT(*) FROM contextual_chunks")
        
        count = cur.fetchone()[0]
        
        if count == 0:
            print("â„¹ï¸  contextual_chunks è¡¨ä¸ºç©ºï¼Œæ— éœ€é‡ç½®")
            return True
        
        # ç¡®è®¤æç¤º
        if not force:
            scope = f"ç‰ˆæœ¬ '{version_id}'" if version_id else "æ‰€æœ‰ç‰ˆæœ¬"
            print(f"\nâš ï¸  è­¦å‘Š: è¿™å°†åˆ é™¤ {scope} çš„ {count} æ¡ contextual_chunks è®°å½•ï¼")
            response = input("ç¡®è®¤é‡ç½®? è¾“å…¥ 'yes' ç»§ç»­: ")
            if response.lower() != 'yes':
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
                return False
        
        # æ‰§è¡Œåˆ é™¤
        if version_id:
            cur.execute(
                "DELETE FROM contextual_chunks WHERE version_id = %s",
                (version_id,)
            )
        else:
            cur.execute("DELETE FROM contextual_chunks")
        
        conn.commit()
        
        scope = f"ç‰ˆæœ¬ '{version_id}'" if version_id else "æ‰€æœ‰ç‰ˆæœ¬"
        print(f"âœ… å·²é‡ç½® {scope} çš„ {count} æ¡è®°å½•")
        return True


def get_stats(conn, version_id: str | None = None) -> dict[str, Any]:
    with conn.cursor() as cur:
        # åŸºç¡€æŸ¥è¯¢ï¼šç»Ÿè®¡ chunks è¡¨
        base_query = """
            SELECT 
                COUNT(*) FILTER (WHERE c.text_raw IS NOT NULL AND c.text_raw != '') as total_chunks,
                COUNT(*) FILTER (WHERE cc.chunk_id IS NOT NULL) as processed_chunks,
                COUNT(*) FILTER (WHERE c.text_raw IS NOT NULL AND c.text_raw != '' AND cc.chunk_id IS NULL) as to_process,
                COUNT(*) FILTER (WHERE c.text_raw IS NULL OR c.text_raw = '') as empty_text
            FROM chunks c
            LEFT JOIN contextual_chunks cc ON c.chunk_id = cc.chunk_id
        """
        
        params = []
        if version_id:
            base_query += " WHERE c.version_id = %s"
            params.append(version_id)
        
        cur.execute(base_query, params)
        row = cur.fetchone()
        
        return {
            "total_chunks": row[0],
            "processed": row[1],
            "to_process": row[2],
            "empty_text": row[3],
        }


def fetch_batch(
    conn, 
    batch_size: int = DEFAULT_BATCH_SIZE,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    version_id: str | None = None,
) -> list[dict[str, Any]]:
    """è·å–ä¸€æ‰¹éœ€è¦å¤„ç†çš„ chunks
    
    ç­–ç•¥:
    1. åªé€‰æ‹©æœ‰æ–‡æœ¬ä¸”æ²¡æœ‰ contextual_chunks çš„è®°å½•
    2. è·å–æ–‡æ¡£æ ‡é¢˜ç”¨äºä¸Šä¸‹æ–‡ç”Ÿæˆ
    3. è·å–å‰å chunks ç”¨äºå‘¨å›´ä¸Šä¸‹æ–‡
    4. æŒ‰ token æ•°é‡åˆ†æ‰¹ï¼Œé¿å…è¶…å‡ºé™åˆ¶
    
    Returns:
        [{chunk_id, version_id, text_raw, document_title, section_title, surrounding_chunks}, ...]
    """
    with conn.cursor() as cur:
        # è·å–å€™é€‰æ•°æ®ï¼ˆå¤šå–ä¸€äº›ä»¥ä¾¿æŒ‰ token ç­›é€‰ï¼‰
        query = """
            SELECT 
                c.chunk_id,
                c.version_id,
                c.text_raw,
                c.chunk_index,
                d.title as document_title,
                LENGTH(c.text_raw) as text_len
            FROM chunks c
            JOIN document_versions dv ON c.version_id = dv.version_id
            JOIN documents d ON dv.doc_id = d.doc_id
            LEFT JOIN contextual_chunks cc ON c.chunk_id = cc.chunk_id
            WHERE c.text_raw IS NOT NULL 
              AND c.text_raw != ''
              AND cc.chunk_id IS NULL
        """
        
        params = []
        if version_id:
            query += " AND c.version_id = %s"
            params.append(version_id)
        
        query += " ORDER BY c.version_id, c.chunk_index LIMIT %s"
        params.append(batch_size * 3)
        
        cur.execute(query, params)
        rows = cur.fetchall()
        
        if not rows:
            return []
        
        # æŒ‰ token æ•°é‡åˆ†æ‰¹
        result = []
        total_tokens = 0
        
        for row in rows:
            chunk_id, ver_id, text_raw, chunk_index, document_title, text_len = row
            tokens = estimate_tokens(text_raw)
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
            if total_tokens + tokens > max_tokens or len(result) >= batch_size:
                break
            
            # è·å–å‰å chunks ä½œä¸ºå‘¨å›´ä¸Šä¸‹æ–‡
            surrounding_chunks = _get_surrounding_chunks(cur, ver_id, chunk_index)
            
            result.append({
                "chunk_id": str(chunk_id),
                "version_id": str(ver_id),
                "text_raw": text_raw,
                "document_title": document_title or "æœªå‘½åæ–‡æ¡£",
                "section_title": None,  # TODO: ä» chunk å…ƒæ•°æ®ä¸­æå–
                "surrounding_chunks": surrounding_chunks,
            })
            total_tokens += tokens
        
        return result


def _get_surrounding_chunks(
    cur, 
    version_id: str, 
    chunk_index: int,
    window: int = 1
) -> list[str]:
    """è·å–æŒ‡å®š chunk å‰åçš„ chunks æ–‡æœ¬
    
    Args:
        cur: æ•°æ®åº“ cursor
        version_id: ç‰ˆæœ¬ ID
        chunk_index: å½“å‰ chunk ç´¢å¼•
        window: å‰åå„å–å¤šå°‘ä¸ª chunks
    
    Returns:
        å‘¨å›´ chunks çš„æ–‡æœ¬åˆ—è¡¨
    """
    query = """
        SELECT text_raw
        FROM chunks
        WHERE version_id = %s
          AND chunk_index BETWEEN %s AND %s
          AND text_raw IS NOT NULL
          AND text_raw != ''
        ORDER BY chunk_index
    """
    cur.execute(query, (version_id, chunk_index - window, chunk_index + window))
    rows = cur.fetchall()
    
    # æ’é™¤å½“å‰ chunk æœ¬èº«
    surrounding = [r[0] for r in rows if r[0]]
    return surrounding


def process_batch(
    conn, 
    chunks: list[dict[str, Any]], 
    context_generator: ContextualRetrievalGenerator,
    embedding_client = None,
    embedding_model: str | None = None,
    show_detail: bool = False,
) -> tuple[int, int]:
    """å¤„ç†ä¸€æ‰¹æ•°æ®
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        chunks: chunk æ•°æ®åˆ—è¡¨
        context_generator: ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨
        embedding_client: OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äº embeddingï¼‰
        embedding_model: embedding æ¨¡å‹åç§°
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    if not chunks:
        return 0, 0
    
    try:
        # æ­¥éª¤ 1: ç”Ÿæˆä¸Šä¸‹æ–‡å‰ç¼€
        if show_detail:
            print("  ğŸ“ ç”Ÿæˆä¸Šä¸‹æ–‡...", end=" ", flush=True)
        
        context_chunks = [
            {
                "chunk_text": c["text_raw"],
                "document_title": c["document_title"],
                "section_title": c.get("section_title"),
                "surrounding_chunks": c.get("surrounding_chunks"),
            }
            for c in chunks
        ]
        
        context_prefixes = context_generator.generate_context_batch(context_chunks)
        
        if show_detail:
            print("âœ…")
        
        # æ­¥éª¤ 2: å‡†å¤‡ contextualized_text
        contextualized_texts = []
        for i, chunk in enumerate(chunks):
            prefix = context_prefixes[i]
            contextualized = f"{prefix}\n\n{chunk['text_raw']}"
            contextualized_texts.append(contextualized)
        
        # æ­¥éª¤ 3: ç”Ÿæˆ embeddings
        if show_detail:
            print("  ğŸ”¢ ç”Ÿæˆå‘é‡...", end=" ", flush=True)
        
        embeddings = embed_texts(
            contextualized_texts,
            client=embedding_client,
            model=embedding_model,
            batch_size=10,  # å†…éƒ¨å†åˆ†æ‰¹
            show_progress=False,
        )
        
        if show_detail:
            print("âœ…")
        
        # æ­¥éª¤ 4: æ‰¹é‡æ’å…¥æ•°æ®åº“
        if show_detail:
            print("  ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...", end=" ", flush=True)
        
        with conn.cursor() as cur:
            insert_data = []
            for i, chunk in enumerate(chunks):
                insert_data.append((
                    chunk["chunk_id"],
                    chunk["version_id"],
                    chunk["text_raw"],
                    context_prefixes[i],
                    contextualized_texts[i],
                    embeddings[i],
                    context_generator.model,
                    embedding_model or get_embedding_config()["model"],
                ))
            
            cur.executemany(
                """
                INSERT INTO contextual_chunks (
                    chunk_id, version_id, original_text, context_prefix,
                    contextualized_text, embedding, model_name, embedding_model
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (chunk_id) DO NOTHING
                """,
                insert_data
            )
        
        conn.commit()
        
        if show_detail:
            print("âœ…")
        
        return len(chunks), 0
        
    except Exception as e:
        conn.rollback()
        print(f"\n  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        return 0, len(chunks)


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        return f"{seconds/60:.1f}åˆ†é’Ÿ"
    else:
        return f"{seconds/3600:.1f}å°æ—¶"


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # åŠ è½½é…ç½®
    settings = load_settings()
    dsn = settings["DATABASE_URL"]
    
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿæˆä¸Šä¸‹æ–‡å¢å¼º chunks")
    parser.add_argument("--version-id", help="æŒ‡å®šç‰ˆæœ¬ ID")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help=f"æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ {DEFAULT_BATCH_SIZE}ï¼‰")
    parser.add_argument("--limit", type=int, default=DEFAULT_LIMIT, help=f"æœ€å¤§å¤„ç†æ•°é‡ï¼ˆé»˜è®¤ {DEFAULT_LIMIT}ï¼‰")
    parser.add_argument("--max-tokens", type=int, default=DEFAULT_MAX_TOKENS, help=f"æ¯æ‰¹æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ {DEFAULT_MAX_TOKENS}ï¼‰")
    parser.add_argument("--llm-model", default="gpt-4", help="LLM æ¨¡å‹ï¼ˆé»˜è®¤ gpt-4ï¼‰")
    parser.add_argument("--show-detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¿›åº¦")
    parser.add_argument("--dry-run", action="store_true", help="å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ•°æ®åº“ï¼‰")
    parser.add_argument("--reset", "-r", action="store_true", help="é‡ç½®/æ¸…ç©º contextual_chunks è¡¨åé‡æ–°ç”Ÿæˆ")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡ç½®ï¼Œè·³è¿‡ç¡®è®¤æç¤ºï¼ˆé…åˆ --reset ä½¿ç”¨ï¼‰")
    args = parser.parse_args()
    
    # æ£€æŸ¥ API Key
    if not settings.get("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: OPENAI_API_KEY æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY=sk-xxx")
        sys.exit(1)
    
    # è·å– embedding é…ç½®
    config = get_embedding_config()
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹ç”Ÿæˆä¸Šä¸‹æ–‡å¢å¼º chunks")
    print("=" * 80)
    print(f"LLM æ¨¡å‹: {args.llm_model}")
    print(f"Embedding æ¨¡å‹: {config['model']}")
    print(f"Embedding ç»´åº¦: {config['dim']}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æœ€å¤§ Token: {args.max_tokens:,}")
    if args.version_id:
        print(f"ç‰ˆæœ¬è¿‡æ»¤: {args.version_id}")
    if args.dry_run:
        print("âš ï¸ å¹²è¿è¡Œæ¨¡å¼: ä¸ä¼šå†™å…¥æ•°æ®åº“")
    print()
    
    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    llm_client = ContextualRetrievalGenerator.get_openai_client() if hasattr(ContextualRetrievalGenerator, 'get_openai_client') else None
    if llm_client is None:
        from openai import OpenAI
        llm_client = OpenAI(
            api_key=settings["OPENAI_API_KEY"],
            base_url=settings.get("OPENAI_BASE_URL"),
        )
    
    embedding_client = get_embedding_client()
    
    # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨
    context_generator = ContextualRetrievalGenerator(
        client=llm_client,
        model=args.llm_model,
        temperature=0.0,
        max_tokens=200,
    )
    
    # è¿æ¥æ•°æ®åº“
    with psycopg.connect(dsn) as conn:
        register_vector(conn)
        
        # å¤„ç†é‡ç½®è¯·æ±‚
        if args.reset:
            if not reset_contextual_chunks(conn, args.version_id, args.force):
                sys.exit(0)  # ç”¨æˆ·å–æ¶ˆï¼Œæ­£å¸¸é€€å‡º
            print()
        
        # è·å–åˆå§‹ç»Ÿè®¡
        stats = get_stats(conn, args.version_id)
        
        print("=" * 80)
        print("ğŸ“Š åˆå§‹ç»Ÿè®¡")
        print("=" * 80)
        print(f"  æ€» chunks:    {stats['total_chunks']:,}")
        print(f"  å·²å¤„ç†:       {stats['processed']:,}")
        print(f"  å¾…å¤„ç†:       {stats['to_process']:,}")
        print(f"  æ— æ³•å¤„ç†:     {stats['empty_text']:,} (ç©ºæ–‡æœ¬)")
        print()
        
        if stats['to_process'] == 0:
            print("âœ… æ‰€æœ‰ chunks éƒ½å·²å¤„ç†ï¼Œæ— éœ€å¤„ç†")
            return
        
        # ç¡®è®¤å¤„ç†
        to_process = min(stats['to_process'], args.limit)
        print(f"å°†å¤„ç† {to_process} æ¡è®°å½•ï¼ˆé™åˆ¶: {args.limit}ï¼‰")
        print()
        
        # ä¸»å¾ªç¯
        total_success = 0
        total_fail = 0
        batch_num = 0
        processed = 0
        
        print("=" * 80)
        print("ğŸ”„ å¼€å§‹å¤„ç†")
        print("=" * 80)
        
        while processed < to_process:
            # è®¡ç®—å‰©ä½™éœ€è¦å¤„ç†çš„æ•°é‡
            remaining = to_process - processed
            batch_size = min(args.batch_size, remaining)
            
            # è·å–ä¸€æ‰¹æ•°æ®
            chunks = fetch_batch(
                conn, 
                batch_size=batch_size,
                max_tokens=args.max_tokens,
                version_id=args.version_id,
            )
            
            if not chunks:
                print("æ²¡æœ‰æ›´å¤šæ•°æ®éœ€è¦å¤„ç†")
                break
            
            batch_num += 1
            batch_len = len(chunks)
            
            # ä¼°ç®— token æ•°
            batch_tokens = sum(estimate_tokens(c["text_raw"]) for c in chunks)
            
            print(f"æ‰¹æ¬¡ {batch_num:>3}: {batch_len:>3} æ¡ ({batch_tokens:,} tokens)...", end=" ", flush=True)
            
            if args.dry_run:
                # å¹²è¿è¡Œæ¨¡å¼ï¼šåªæ‰“å°ï¼Œä¸å®é™…å¤„ç†
                print("â­ï¸  è·³è¿‡ï¼ˆå¹²è¿è¡Œï¼‰")
                processed += batch_len
                continue
            
            # å¤„ç†æ‰¹æ¬¡
            success, fail = process_batch(
                conn, chunks, 
                context_generator=context_generator,
                embedding_client=embedding_client,
                embedding_model=config['model'],
                show_detail=args.show_detail,
            )
            
            total_success += success
            total_fail += fail
            processed += batch_len
            
            if success == batch_len:
                elapsed = time.time() - start_time
                speed = total_success / elapsed if elapsed > 0 else 0
                print(f"âœ… ({speed:.2f} æ¡/ç§’)")
            else:
                print(f"âš ï¸ æˆåŠŸ {success}/{batch_len}")
            
            # æ¯ 10 æ‰¹æ¬¡æ˜¾ç¤ºè¿›åº¦
            if batch_num % 10 == 0:
                progress = 100 * processed / to_process
                elapsed = time.time() - start_time
                eta = (elapsed / processed) * (to_process - processed) if processed > 0 else 0
                print(f"  ğŸ“ˆ è¿›åº¦: {processed}/{to_process} ({progress:.1f}%) | å·²ç”¨: {format_duration(elapsed)} | é¢„è®¡å‰©ä½™: {format_duration(eta)}")
        
        # æœ€ç»ˆç»Ÿè®¡
        elapsed = time.time() - start_time
        
        print()
        print("=" * 80)
        print("âœ… å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"æˆåŠŸ:     {total_success:,}")
        print(f"å¤±è´¥:     {total_fail:,}")
        print(f"æ€»è®¡:     {total_success + total_fail:,}")
        print(f"ç”¨æ—¶:     {format_duration(elapsed)}")
        if total_success > 0:
            print(f"å¹³å‡é€Ÿåº¦: {total_success/elapsed:.2f} æ¡/ç§’")
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = get_stats(conn, args.version_id)
        print()
        print("ğŸ“Š æœ€ç»ˆçŠ¶æ€")
        print(f"  å·²å¤„ç†: {final_stats['processed']:,} / {final_stats['total_chunks']:,} ({100*final_stats['processed']/final_stats['total_chunks']:.1f}%)")
        print(f"  å¾…å¤„ç†: {final_stats['to_process']:,}")
        print(f"  æ— æ³•å¤„ç†: {final_stats['empty_text']:,}")


if __name__ == "__main__":
    main()
