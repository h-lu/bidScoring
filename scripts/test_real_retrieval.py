#!/usr/bin/env python3
"""
çœŸå®æ•ˆæœæµ‹è¯• - ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…å†…å®¹éªŒè¯ Hybrid Retrieval ä¼˜åŒ–

æµ‹è¯•å†…å®¹ï¼š
1. å…¨æ–‡æœç´¢ vs ILIKE æ€§èƒ½å¯¹æ¯”
2. å‘é‡æœç´¢å¬å›ç‡æµ‹è¯•
3. æ··åˆæœç´¢æ•ˆæœéªŒè¯
4. ç¼“å­˜æ•ˆæœæµ‹è¯•
5. å¼‚æ­¥æ¥å£æµ‹è¯•
"""

import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

from bid_scoring.config import load_settings
from bid_scoring.hybrid_retrieval import HybridRetriever

# æµ‹è¯•æŸ¥è¯¢ï¼ˆæ¥è‡ªå®é™…ä¸šåŠ¡åœºæ™¯ï¼‰
TEST_QUERIES = [
    "åŸ¹è®­æ—¶é•¿",
    "å”®åæœåŠ¡",
    "CTè®¾å¤‡å‚æ•°",
    "MRIç£å…±æŒ¯",
    "ä¿ä¿®æœŸé™",
    "å“åº”æ—¶é—´",
    "å·¥ç¨‹å¸ˆèµ„è´¨",
    "é…ä»¶ä¾›åº”",
    "è®¾å¤‡å®‰è£…",
    "éªŒæ”¶æ ‡å‡†",
]

SETTINGS = load_settings()
VERSION_ID = "83420a7c-b27b-480f-9427-565c47d2b53c"  # ä½¿ç”¨å®é™…ç‰ˆæœ¬


def test_fulltext_vs_ilike():
    """æµ‹è¯• 1: å…¨æ–‡æœç´¢ vs ILIKE æ€§èƒ½å¯¹æ¯” (AND vs OR è¯­ä¹‰)"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: å…¨æ–‡æœç´¢ AND vs OR è¯­ä¹‰å¯¹æ¯”")
    print("=" * 60)
    
    retriever = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=10,
    )
    
    keywords = ["åŸ¹è®­", "æ—¶é•¿"]
    
    # æµ‹è¯• OR è¯­ä¹‰ï¼ˆé»˜è®¤ï¼Œæé«˜å¬å›ç‡ï¼‰
    or_times = []
    for _ in range(5):
        start = time.perf_counter()
        results = retriever._keyword_search_fulltext(keywords, use_or_semantic=True)
        or_times.append(time.perf_counter() - start)
    
    or_avg = statistics.mean(or_times) * 1000
    or_results = len(results)
    
    # æµ‹è¯• AND è¯­ä¹‰ï¼ˆæé«˜ç²¾ç¡®ç‡ï¼‰
    and_times = []
    for _ in range(5):
        start = time.perf_counter()
        results = retriever._keyword_search_fulltext(keywords, use_or_semantic=False)
        and_times.append(time.perf_counter() - start)
    
    and_avg = statistics.mean(and_times) * 1000
    and_results = len(results)
    
    print(f"\næŸ¥è¯¢å…³é”®è¯: {keywords}")
    print(f"  OR è¯­ä¹‰ (é»˜è®¤ï¼Œæé«˜å¬å›ç‡):")
    print(f"    - å¹³å‡è€—æ—¶: {or_avg:.2f} ms")
    print(f"    - è¿”å›ç»“æœ: {or_results} æ¡")
    print(f"  AND è¯­ä¹‰ (æé«˜ç²¾ç¡®ç‡):")
    print(f"    - å¹³å‡è€—æ—¶: {and_avg:.2f} ms")
    print(f"    - è¿”å›ç»“æœ: {and_results} æ¡")
    
    if and_results > 0:
        recall_boost = or_results / and_results
        print(f"\n  ğŸ“ˆ OR è¯­ä¹‰å¬å›æå‡: {recall_boost:.1f}x")
    elif or_results > 0:
        print(f"\n  ğŸ“ˆ OR è¯­ä¹‰å¬å›æå‡: æ— é™ (AND æ— ç»“æœï¼ŒOR æœ‰ç»“æœ)")
    
    # æµ‹è¯• ILIKEï¼ˆé—ç•™æ–¹æ³•ï¼‰
    ilike_times = []
    for _ in range(5):
        start = time.perf_counter()
        results = retriever._keyword_search_legacy(keywords)
        ilike_times.append(time.perf_counter() - start)
    
    ilike_avg = statistics.mean(ilike_times) * 1000
    ilike_results = len(results)
    
    print(f"\n  ILIKE (æ—§æ–¹æ³•):")
    print(f"    - å¹³å‡è€—æ—¶: {ilike_avg:.2f} ms")
    print(f"    - è¿”å›ç»“æœ: {ilike_results} æ¡")
    
    if or_avg > 0:
        speedup = ilike_avg / or_avg
        print(f"\n  âš¡ å…¨æ–‡æœç´¢æ€§èƒ½æå‡: {speedup:.1f}x")
    
    retriever.close()


def test_vector_recall():
    """æµ‹è¯• 2: å‘é‡æœç´¢å¬å›ç‡æµ‹è¯•ï¼ˆä¸åŒ ef_searchï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: HNSW ef_search å‚æ•°å¯¹å¬å›ç‡çš„å½±å“")
    print("=" * 60)
    
    query = "CTè®¾å¤‡æŠ€æœ¯å‚æ•°è¦æ±‚"
    
    for ef in [40, 100, 200]:
        retriever = HybridRetriever(
            version_id=VERSION_ID,
            settings=SETTINGS,
            top_k=10,
            hnsw_ef_search=ef,
        )
        
        times = []
        for _ in range(3):
            start = time.perf_counter()
            results = retriever._vector_search(query)
            times.append(time.perf_counter() - start)
        
        avg_time = statistics.mean(times) * 1000
        print(f"\n  ef_search={ef}:")
        print(f"    - å¹³å‡è€—æ—¶: {avg_time:.2f} ms")
        print(f"    - è¿”å›ç»“æœ: {len(results)} æ¡")
        if results:
            print(f"    - æœ€é«˜ç›¸ä¼¼åº¦: {results[0][1]:.4f}")
        
        retriever.close()
    
    print("\n  ğŸ’¡ è¯´æ˜: ef_search=100 æ˜¯æ¨èé»˜è®¤å€¼ï¼ˆå¹³è¡¡æ€§èƒ½å’Œå¬å›ç‡ï¼‰")


def test_hybrid_search():
    """æµ‹è¯• 3: æ··åˆæœç´¢æ•ˆæœéªŒè¯"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: æ··åˆæœç´¢æ•ˆæœéªŒè¯")
    print("=" * 60)
    
    queries = [
        "åŸ¹è®­æ—¶é•¿æ˜¯å¤šå°‘",
        "CTè®¾å¤‡å”®åæœåŠ¡",
        "MRIæ ¸ç£å…±æŒ¯å‚æ•°",
    ]
    
    retriever = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=5,
        hnsw_ef_search=100,
        vector_weight=1.0,
        keyword_weight=1.0,
    )
    
    for query in queries:
        print(f"\n  æŸ¥è¯¢: '{query}'")
        
        # æå–å…³é”®è¯
        keywords = retriever.extract_keywords_from_query(query)
        print(f"    æ‰©å±•å…³é”®è¯: {keywords}")
        
        # æ‰§è¡Œæ··åˆæ£€ç´¢
        start = time.perf_counter()
        results = retriever.retrieve(query)
        elapsed = (time.perf_counter() - start) * 1000
        
        print(f"    æ€»è€—æ—¶: {elapsed:.2f} ms")
        print(f"    è¿”å›ç»“æœ: {len(results)} æ¡")
        
        if results:
            print(f"     top-1 ç»“æœ:")
            print(f"      - æ¥æº: {results[0].source}")
            print(f"      - RRFåˆ†æ•°: {results[0].score:.4f}")
            print(f"      - å‘é‡åˆ†æ•°: {results[0].vector_score}")
            print(f"      - å…³é”®è¯åˆ†æ•°: {results[0].keyword_score}")
            print(f"      - æ–‡æœ¬ç‰‡æ®µ: {results[0].text[:50]}...")
    
    retriever.close()


def test_rrf_weights():
    """æµ‹è¯• 4: RRF æƒé‡æ•ˆæœå¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: RRF æƒé‡æ•ˆæœå¯¹æ¯”")
    print("=" * 60)
    
    query = "åŸ¹è®­æ—¶é•¿"
    
    weight_configs = [
        ("å¹³è¡¡", 1.0, 1.0),
        ("å‘é‡ä¼˜å…ˆ", 2.0, 1.0),
        ("å…³é”®è¯ä¼˜å…ˆ", 1.0, 2.0),
    ]
    
    for name, vec_w, key_w in weight_configs:
        retriever = HybridRetriever(
            version_id=VERSION_ID,
            settings=SETTINGS,
            top_k=5,
            vector_weight=vec_w,
            keyword_weight=key_w,
        )
        
        results = retriever.retrieve(query)
        
        print(f"\n  é…ç½®: {name} (å‘é‡={vec_w}, å…³é”®è¯={key_w})")
        if results:
            print(f"    top-1 æ¥æº: {results[0].source}")
            print(f"    top-1 åˆ†æ•°: {results[0].score:.4f}")
        else:
            print(f"    ç»“æœ: æ— åŒ¹é…")
        
        retriever.close()


def test_cache_performance():
    """æµ‹è¯• 5: ç¼“å­˜æ•ˆæœæµ‹è¯•"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: æŸ¥è¯¢ç¼“å­˜æ•ˆæœæµ‹è¯•")
    print("=" * 60)
    
    query = "å”®åæœåŠ¡å“åº”æ—¶é—´"
    
    # æ— ç¼“å­˜
    retriever_no_cache = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=10,
        enable_cache=False,
    )
    
    times_no_cache = []
    for _ in range(3):
        start = time.perf_counter()
        retriever_no_cache.retrieve(query)
        times_no_cache.append(time.perf_counter() - start)
    
    avg_no_cache = statistics.mean(times_no_cache) * 1000
    retriever_no_cache.close()
    
    # æœ‰ç¼“å­˜
    retriever_with_cache = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=10,
        enable_cache=True,
        cache_size=100,
    )
    
    # ç¬¬ä¸€æ¬¡ï¼ˆå†·ç¼“å­˜ï¼‰
    start = time.perf_counter()
    retriever_with_cache.retrieve(query)
    cold_time = (time.perf_counter() - start) * 1000
    
    # ç¬¬äºŒæ¬¡ï¼ˆçƒ­ç¼“å­˜ï¼‰
    start = time.perf_counter()
    retriever_with_cache.retrieve(query)
    hot_time = (time.perf_counter() - start) * 1000
    
    stats = retriever_with_cache.get_cache_stats()
    retriever_with_cache.close()
    
    print(f"\n  æŸ¥è¯¢: '{query}'")
    print(f"  æ— ç¼“å­˜æ¨¡å¼:")
    print(f"    - å¹³å‡è€—æ—¶: {avg_no_cache:.2f} ms")
    print(f"  æœ‰ç¼“å­˜æ¨¡å¼:")
    print(f"    - å†·ç¼“å­˜: {cold_time:.2f} ms")
    print(f"    - çƒ­ç¼“å­˜: {hot_time:.2f} ms")
    print(f"    - ç¼“å­˜çŠ¶æ€: {stats}")
    
    if hot_time > 0:
        speedup = avg_no_cache / hot_time
        print(f"\n  âš¡ ç¼“å­˜åŠ é€Ÿ: {speedup:.1f}x")


@pytest.mark.asyncio
async def test_async_performance():
    """æµ‹è¯• 6: å¼‚æ­¥æ¥å£æ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: å¼‚æ­¥æ¥å£æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    retriever = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=10,
    )
    
    queries = TEST_QUERIES[:5]
    
    # åŒæ­¥é¡ºåºæ‰§è¡Œ
    print("\n  åŒæ­¥é¡ºåºæ‰§è¡Œ (5 ä¸ªæŸ¥è¯¢):")
    start = time.perf_counter()
    for query in queries:
        retriever.retrieve(query)
    sync_time = (time.perf_counter() - start) * 1000
    print(f"    - æ€»è€—æ—¶: {sync_time:.2f} ms")
    print(f"    - å¹³å‡: {sync_time/len(queries):.2f} ms/æŸ¥è¯¢")
    
    # å¼‚æ­¥å¹¶å‘æ‰§è¡Œ
    print("\n  å¼‚æ­¥å¹¶å‘æ‰§è¡Œ (5 ä¸ªæŸ¥è¯¢):")
    start = time.perf_counter()
    await asyncio.gather(*[
        retriever.retrieve_async(query)
        for query in queries
    ])
    async_time = (time.perf_counter() - start) * 1000
    print(f"    - æ€»è€—æ—¶: {async_time:.2f} ms")
    print(f"    - å¹³å‡: {async_time/len(queries):.2f} ms/æŸ¥è¯¢")
    
    if async_time > 0:
        speedup = sync_time / async_time
        print(f"\n  âš¡ å¹¶å‘åŠ é€Ÿ: {speedup:.1f}x")
    
    await retriever.close_async()


def test_connection_pool():
    """æµ‹è¯• 7: è¿æ¥æ± æ•ˆæœæµ‹è¯•"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 7: è¿æ¥æ± æ•ˆæœæµ‹è¯•")
    print("=" * 60)
    
    query = "åŸ¹è®­æ—¶é•¿"
    
    # æ— è¿æ¥æ± 
    retriever_no_pool = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=10,
        use_connection_pool=False,
    )
    
    times_no_pool = []
    for _ in range(5):
        start = time.perf_counter()
        retriever_no_pool.retrieve(query)
        times_no_pool.append(time.perf_counter() - start)
    
    avg_no_pool = statistics.mean(times_no_pool) * 1000
    retriever_no_pool.close()
    
    # æœ‰è¿æ¥æ± 
    retriever_with_pool = HybridRetriever(
        version_id=VERSION_ID,
        settings=SETTINGS,
        top_k=10,
        use_connection_pool=True,
        pool_min_size=2,
        pool_max_size=5,
    )
    
    times_with_pool = []
    for _ in range(5):
        start = time.perf_counter()
        retriever_with_pool.retrieve(query)
        times_with_pool.append(time.perf_counter() - start)
    
    avg_with_pool = statistics.mean(times_with_pool) * 1000
    retriever_with_pool.close()
    
    print(f"\n  æŸ¥è¯¢: '{query}'")
    print(f"  æ— è¿æ¥æ± :")
    print(f"    - å¹³å‡è€—æ—¶: {avg_no_pool:.2f} ms")
    print(f"  æœ‰è¿æ¥æ± :")
    print(f"    - å¹³å‡è€—æ—¶: {avg_with_pool:.2f} ms")
    
    if avg_with_pool > 0:
        speedup = avg_no_pool / avg_with_pool
        print(f"\n  âš¡ è¿æ¥æ± åŠ é€Ÿ: {speedup:.1f}x")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("Hybrid Retrieval çœŸå®æ•ˆæœæµ‹è¯•")
    print("=" * 60)
    print(f"\næ•°æ®åº“: {SETTINGS.get('DATABASE_URL', 'N/A').split('@')[-1]}")
    print(f"æµ‹è¯•ç‰ˆæœ¬: {VERSION_ID}")
    print(f"æµ‹è¯•æŸ¥è¯¢æ•°: {len(TEST_QUERIES)}")
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_fulltext_vs_ilike()
    test_vector_recall()
    test_hybrid_search()
    test_rrf_weights()
    test_cache_performance()
    await test_async_performance()
    test_connection_pool()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
