from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Compare two scoring run outputs generated by bid-pipeline."
    )
    parser.add_argument("--baseline", type=Path, required=True)
    parser.add_argument("--candidate", type=Path, required=True)
    parser.add_argument("--output", type=Path)
    return parser


def main() -> int:
    args = build_parser().parse_args()
    baseline = _load_json(args.baseline)
    candidate = _load_json(args.candidate)
    report = {
        "baseline_path": str(args.baseline),
        "candidate_path": str(args.candidate),
        "baseline": summarize_run_output(baseline),
        "candidate": summarize_run_output(candidate),
        **compare_run_outputs(baseline=baseline, candidate=candidate),
    }
    rendered = json.dumps(report, ensure_ascii=False, indent=2)
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        args.output.write_text(rendered, encoding="utf-8")
        print(
            json.dumps(
                {"status": "written", "output": str(args.output)}, ensure_ascii=False
            )
        )
        return 0
    print(rendered)
    return 0


def summarize_run_output(payload: dict[str, Any]) -> dict[str, Any]:
    scoring = payload.get("scoring", {})
    traceability = payload.get("traceability", {})
    observability = payload.get("observability", {})
    dimensions = scoring.get("dimensions", {})

    dimension_scores: dict[str, float] = {}
    if isinstance(dimensions, dict):
        for name, details in dimensions.items():
            if not isinstance(details, dict):
                continue
            score = details.get("score")
            if isinstance(score, (int, float)):
                dimension_scores[name] = float(score)

    warnings = payload.get("warnings", [])
    evidence_warnings = scoring.get("evidence_warnings", [])
    return {
        "status": payload.get("status"),
        "scoring_backend": observability.get("scoring_backend"),
        "overall_score": scoring.get("overall_score"),
        "risk_level": scoring.get("risk_level"),
        "total_risks": scoring.get("total_risks"),
        "total_benefits": scoring.get("total_benefits"),
        "chunks_analyzed": scoring.get("chunks_analyzed"),
        "traceability_status": traceability.get("status"),
        "citation_count_total": traceability.get("citation_count_total"),
        "citation_count_traceable": traceability.get("citation_count_traceable"),
        "coverage_ratio": traceability.get("citation_coverage_ratio"),
        "warnings": sorted(_string_list(warnings)),
        "evidence_warning_codes": sorted(_string_list(evidence_warnings)),
        "dimension_scores": dimension_scores,
    }


def compare_run_outputs(
    *, baseline: dict[str, Any], candidate: dict[str, Any]
) -> dict[str, Any]:
    baseline_summary = summarize_run_output(baseline)
    candidate_summary = summarize_run_output(candidate)

    return {
        "delta": {
            "overall_score": _round_delta(
                candidate_summary.get("overall_score"),
                baseline_summary.get("overall_score"),
            ),
            "coverage_ratio": _round_delta(
                candidate_summary.get("coverage_ratio"),
                baseline_summary.get("coverage_ratio"),
            ),
            "citation_count_total": _int_delta(
                candidate_summary.get("citation_count_total"),
                baseline_summary.get("citation_count_total"),
            ),
            "citation_count_traceable": _int_delta(
                candidate_summary.get("citation_count_traceable"),
                baseline_summary.get("citation_count_traceable"),
            ),
            "chunks_analyzed": _int_delta(
                candidate_summary.get("chunks_analyzed"),
                baseline_summary.get("chunks_analyzed"),
            ),
            "dimension_scores": _dimension_score_deltas(
                baseline_summary.get("dimension_scores", {}),
                candidate_summary.get("dimension_scores", {}),
            ),
        },
        "warnings_added": sorted(
            list(
                set(candidate_summary.get("warnings", []))
                - set(baseline_summary.get("warnings", []))
            )
        ),
        "warnings_removed": sorted(
            list(
                set(baseline_summary.get("warnings", []))
                - set(candidate_summary.get("warnings", []))
            )
        ),
    }


def _load_json(path: Path) -> dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def _string_list(value: Any) -> list[str]:
    if not isinstance(value, list):
        return []
    return [item for item in value if isinstance(item, str)]


def _round_delta(candidate: Any, baseline: Any) -> float | None:
    if not isinstance(candidate, (int, float)) or not isinstance(
        baseline, (int, float)
    ):
        return None
    return round(float(candidate) - float(baseline), 4)


def _int_delta(candidate: Any, baseline: Any) -> int | None:
    if not isinstance(candidate, int) or not isinstance(baseline, int):
        return None
    return candidate - baseline


def _dimension_score_deltas(
    baseline_scores: dict[str, float], candidate_scores: dict[str, float]
) -> dict[str, float]:
    deltas: dict[str, float] = {}
    all_dims = sorted(set(baseline_scores.keys()) | set(candidate_scores.keys()))
    for dimension in all_dims:
        base_score = baseline_scores.get(dimension)
        cand_score = candidate_scores.get(dimension)
        if not isinstance(base_score, (int, float)) or not isinstance(
            cand_score, (int, float)
        ):
            continue
        deltas[dimension] = round(float(cand_score) - float(base_score), 4)
    return deltas


if __name__ == "__main__":
    raise SystemExit(main())
