#!/usr/bin/env python3
"""æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡ - æœ€ä½³å®è·µå®ç°

Features:
- æ™ºèƒ½åˆ†æ‰¹: æŒ‰ token æ•°é‡å’Œæ‰¹æ¬¡å¤§å°åŒé‡é™åˆ¶
- æ‰¹é‡å¤„ç†: 50-100æ¡/æ‰¹ï¼ˆæ¨èï¼‰
- è¿›åº¦æ˜¾ç¤º: å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
- é”™è¯¯å¤„ç†: æ‰¹æ¬¡å¤±è´¥è‡ªåŠ¨å›æ»šï¼Œæ”¯æŒä¸­æ–­æ¢å¤
- ç©ºæ–‡æœ¬è¿‡æ»¤: è‡ªåŠ¨è·³è¿‡æ— æ–‡æœ¬çš„ chunks
- Token ä¼°ç®—: é¿å…è¶…å‡º OpenAI é™åˆ¶
- ç»Ÿè®¡æŠ¥å‘Š: å¤„ç†å®Œæˆåæ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡

æœ€ä½³å®è·µå‚è€ƒ:
- OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings
- Chunking Strategies: https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025
- AWS pgvector: https://aws.amazon.com/blogs/database/optimize-generative-ai-applications-with-pgvector-indexing/
"""

import sys
import time
from datetime import datetime
from typing import Any

import psycopg
from pgvector.psycopg import register_vector

from bid_scoring.config import load_settings
from bid_scoring.embeddings import embed_texts, estimate_tokens, get_embedding_client, get_embedding_config


# é…ç½®å‚æ•°ï¼ˆå¯æ ¹æ®ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
DEFAULT_BATCH_SIZE = 100      # æ¯æ‰¹å¤„ç†æ•°é‡ï¼ˆæ¨è 50-100ï¼‰
DEFAULT_LIMIT = 1000          # æ¯æ¬¡è¿è¡Œæœ€å¤§å¤„ç†æ•°é‡
DEFAULT_MAX_TOKENS = 100000   # æ¯æ‰¹æœ€å¤§ token æ•°ï¼ˆä¿å®ˆè®¾ç½®ï¼‰


def get_stats(conn, version_id: str | None = None) -> dict[str, Any]:
    """è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯"""
    with conn.cursor() as cur:
        base_query = """
            SELECT 
                COUNT(*) FILTER (WHERE embedding IS NULL) as null_count,
                COUNT(*) FILTER (WHERE embedding IS NOT NULL) as has_count,
                COUNT(*) FILTER (WHERE embedding IS NULL AND text_raw IS NOT NULL AND text_raw != '') as to_process,
                COUNT(*) FILTER (WHERE embedding IS NULL AND (text_raw IS NULL OR text_raw = '')) as empty_text
            FROM chunks
        """
        
        if version_id:
            base_query += " WHERE version_id = %s"
            cur.execute(base_query, (version_id,))
        else:
            cur.execute(base_query)
        
        row = cur.fetchone()
        return {
            "null_count": row[0],
            "has_count": row[1],
            "to_process": row[2],
            "empty_text": row[3],
            "total": row[0] + row[1],
        }


def fetch_batch(
    conn, 
    batch_size: int = DEFAULT_BATCH_SIZE,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    version_id: str | None = None,
) -> list[tuple[str, str]]:
    """è·å–ä¸€æ‰¹éœ€è¦å¤„ç†çš„ chunks
    
    ç­–ç•¥:
    1. åªé€‰æ‹©æœ‰æ–‡æœ¬ä¸”æ²¡æœ‰å‘é‡çš„è®°å½•
    2. æŒ‰ token æ•°é‡åˆ†æ‰¹ï¼Œé¿å…è¶…å‡ºé™åˆ¶
    3. æ”¯æŒæŒ‰ version_id è¿‡æ»¤
    
    Returns:
        [(chunk_id, text_raw), ...]
    """
    with conn.cursor() as cur:
        # è·å–å€™é€‰æ•°æ®ï¼ˆå¤šå–ä¸€äº›ä»¥ä¾¿æŒ‰ token ç­›é€‰ï¼‰
        query = """
            SELECT chunk_id, text_raw, LENGTH(text_raw) as text_len
            FROM chunks 
            WHERE embedding IS NULL 
              AND text_raw IS NOT NULL 
              AND text_raw != ''
        """
        
        params = []
        if version_id:
            query += " AND version_id = %s"
            params.append(version_id)
        
        query += " ORDER BY chunk_id LIMIT %s"
        params.append(batch_size * 3)
        
        cur.execute(query, params)
        rows = cur.fetchall()
        
        if not rows:
            return []
        
        # æŒ‰ token æ•°é‡åˆ†æ‰¹
        result = []
        total_tokens = 0
        
        for chunk_id, text_raw, text_len in rows:
            tokens = estimate_tokens(text_raw)
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
            if total_tokens + tokens > max_tokens or len(result) >= batch_size:
                break
            
            result.append((str(chunk_id), text_raw))
            total_tokens += tokens
        
        return result


def process_batch(
    conn, 
    rows: list[tuple[str, str]], 
    client = None,
    model: str | None = None,
    show_detail: bool = False,
) -> tuple[int, int]:
    """å¤„ç†ä¸€æ‰¹æ•°æ®
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        rows: [(chunk_id, text_raw), ...]
        client: OpenAI å®¢æˆ·ç«¯
        model: æ¨¡å‹åç§°
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    if not rows:
        return 0, 0
    
    ids = [r[0] for r in rows]
    texts = [r[1] for r in rows]
    
    try:
        # è°ƒç”¨ embedding APIï¼ˆä½¿ç”¨ embed_texts çš„æ‰¹é‡é€»è¾‘ï¼‰
        vecs = embed_texts(
            texts,
            client=client,
            model=model,
            batch_size=50,  # å†…éƒ¨å†åˆ†æ‰¹
            show_progress=show_detail,
        )
        
        # æ‰¹é‡æ›´æ–°æ•°æ®åº“
        with conn.cursor() as cur:
            # ä½¿ç”¨ executemany æé«˜æ•ˆç‡
            update_data = [(vecs[i], ids[i]) for i in range(len(ids))]
            cur.executemany(
                "UPDATE chunks SET embedding = %s WHERE chunk_id = %s",
                update_data
            )
        
        conn.commit()
        return len(rows), 0
        
    except Exception as e:
        conn.rollback()
        print(f"\n  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        return 0, len(rows)


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        return f"{seconds/60:.1f}åˆ†é’Ÿ"
    else:
        return f"{seconds/3600:.1f}å°æ—¶"


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # åŠ è½½é…ç½®
    settings = load_settings()
    dsn = settings["DATABASE_URL"]
    
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡")
    parser.add_argument("--version-id", help="æŒ‡å®šç‰ˆæœ¬ ID")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help=f"æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ {DEFAULT_BATCH_SIZE}ï¼‰")
    parser.add_argument("--limit", type=int, default=DEFAULT_LIMIT, help=f"æœ€å¤§å¤„ç†æ•°é‡ï¼ˆé»˜è®¤ {DEFAULT_LIMIT}ï¼‰")
    parser.add_argument("--max-tokens", type=int, default=DEFAULT_MAX_TOKENS, help=f"æ¯æ‰¹æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ {DEFAULT_MAX_TOKENS}ï¼‰")
    parser.add_argument("--show-detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¿›åº¦")
    args = parser.parse_args()
    
    # æ£€æŸ¥ API Key
    if not settings.get("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: OPENAI_API_KEY æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY=sk-xxx")
        sys.exit(1)
    
    # è·å– embedding é…ç½®
    config = get_embedding_config()
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹ç”Ÿæˆå‘é‡")
    print("=" * 80)
    print(f"æ¨¡å‹: {config['model']}")
    print(f"ç»´åº¦: {config['dim']}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æœ€å¤§ Token: {args.max_tokens:,}")
    if args.version_id:
        print(f"ç‰ˆæœ¬è¿‡æ»¤: {args.version_id}")
    print()
    
    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    client = get_embedding_client()
    
    # è¿æ¥æ•°æ®åº“
    with psycopg.connect(dsn) as conn:
        register_vector(conn)
        
        # è·å–åˆå§‹ç»Ÿè®¡
        stats = get_stats(conn, args.version_id)
        
        print("=" * 80)
        print("ğŸ“Š åˆå§‹ç»Ÿè®¡")
        print("=" * 80)
        print(f"  æ€» chunks:    {stats['total']:,}")
        print(f"  å·²æœ‰å‘é‡:     {stats['has_count']:,}")
        print(f"  å¾…å¤„ç†:       {stats['to_process']:,}")
        print(f"  æ— æ³•å¤„ç†:     {stats['empty_text']:,} (ç©ºæ–‡æœ¬)")
        print()
        
        if stats['to_process'] == 0:
            print("âœ… æ‰€æœ‰ chunks éƒ½å·²æœ‰å‘é‡ï¼Œæ— éœ€å¤„ç†")
            return
        
        # ç¡®è®¤å¤„ç†
        to_process = min(stats['to_process'], args.limit)
        print(f"å°†å¤„ç† {to_process} æ¡è®°å½•ï¼ˆé™åˆ¶: {args.limit}ï¼‰")
        print()
        
        # ä¸»å¾ªç¯
        total_success = 0
        total_fail = 0
        batch_num = 0
        processed = 0
        
        print("=" * 80)
        print("ğŸ”„ å¼€å§‹å¤„ç†")
        print("=" * 80)
        
        while processed < to_process:
            # è®¡ç®—å‰©ä½™éœ€è¦å¤„ç†çš„æ•°é‡
            remaining = to_process - processed
            batch_size = min(args.batch_size, remaining)
            
            # è·å–ä¸€æ‰¹æ•°æ®
            rows = fetch_batch(
                conn, 
                batch_size=batch_size,
                max_tokens=args.max_tokens,
                version_id=args.version_id,
            )
            
            if not rows:
                print("æ²¡æœ‰æ›´å¤šæ•°æ®éœ€è¦å¤„ç†")
                break
            
            batch_num += 1
            batch_len = len(rows)
            
            # ä¼°ç®— token æ•°
            batch_tokens = sum(estimate_tokens(r[1]) for r in rows)
            
            print(f"æ‰¹æ¬¡ {batch_num:>3}: {batch_len:>3} æ¡ ({batch_tokens:,} tokens)...", end=" ", flush=True)
            
            # å¤„ç†æ‰¹æ¬¡
            success, fail = process_batch(
                conn, rows, 
                client=client, 
                model=config['model'],
                show_detail=args.show_detail,
            )
            
            total_success += success
            total_fail += fail
            processed += batch_len
            
            if success == batch_len:
                elapsed = time.time() - start_time
                speed = total_success / elapsed if elapsed > 0 else 0
                print(f"âœ… ({speed:.1f} æ¡/ç§’)")
            else:
                print(f"âš ï¸ æˆåŠŸ {success}/{batch_len}")
            
            # æ¯ 10 æ‰¹æ¬¡æ˜¾ç¤ºè¿›åº¦
            if batch_num % 10 == 0:
                progress = 100 * processed / to_process
                elapsed = time.time() - start_time
                eta = (elapsed / processed) * (to_process - processed) if processed > 0 else 0
                print(f"  ğŸ“ˆ è¿›åº¦: {processed}/{to_process} ({progress:.1f}%) | å·²ç”¨: {format_duration(elapsed)} | é¢„è®¡å‰©ä½™: {format_duration(eta)}")
        
        # æœ€ç»ˆç»Ÿè®¡
        elapsed = time.time() - start_time
        
        print()
        print("=" * 80)
        print("âœ… å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"æˆåŠŸ:     {total_success:,}")
        print(f"å¤±è´¥:     {total_fail:,}")
        print(f"æ€»è®¡:     {total_success + total_fail:,}")
        print(f"ç”¨æ—¶:     {format_duration(elapsed)}")
        if total_success > 0:
            print(f"å¹³å‡é€Ÿåº¦: {total_success/elapsed:.1f} æ¡/ç§’")
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = get_stats(conn, args.version_id)
        print()
        print("ğŸ“Š æœ€ç»ˆçŠ¶æ€")
        print(f"  å·²æœ‰å‘é‡: {final_stats['has_count']:,} / {final_stats['total']:,} ({100*final_stats['has_count']/final_stats['total']:.1f}%)")
        print(f"  å¾…å¤„ç†:   {final_stats['to_process']:,}")
        print(f"  æ— æ³•å¤„ç†: {final_stats['empty_text']:,}")


if __name__ == "__main__":
    main()
