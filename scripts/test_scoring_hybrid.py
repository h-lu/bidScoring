#!/usr/bin/env python3
"""
æ··åˆæ£€ç´¢è¯„åˆ†æµ‹è¯•è„šæœ¬

ç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢çš„ä¼˜åŠ¿:
1. å‘é‡æ£€ç´¢: æ‰¾è¯­ä¹‰ç›¸å…³å†…å®¹
2. å…³é”®è¯æ£€ç´¢: æ‰¾ç²¾ç¡®åŒ¹é…å†…å®¹
3. RRFèåˆ: åˆå¹¶æ’åºç»“æœ

Usage:
    python scripts/test_scoring_hybrid.py
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import json
import traceback
from datetime import datetime, timezone
from typing import Any

import psycopg
from bid_scoring.config import load_settings
from bid_scoring.llm import LLMClient
from bid_scoring.scoring_schema import (
    AfterSalesService, BoundingBox, EvidenceField, EvidenceItem, TrainingPlan,
)
from bid_scoring.scoring_config import load_scoring_config
from bid_scoring.hybrid_retrieval import HybridRetriever


VERSION_ID = "83420a7c-b27b-480f-9427-565c47d2b53c"


class HybridScoringTester:
    """ä½¿ç”¨æ··åˆæ£€ç´¢çš„è¯„åˆ†æµ‹è¯•å™¨"""
    
    def __init__(self, version_id: str, top_k: int = 5):
        self.version_id = version_id
        self.top_k = top_k
        self.settings = load_settings()
        self.dsn = self.settings["DATABASE_URL"]
        self.llm = LLMClient(self.settings)
        self.retriever = HybridRetriever(
            version_id=version_id,
            settings=self.settings,
            top_k=top_k
        )
        self.results: dict[str, Any] = {}
        
    def extract_keywords(self, field_name: str) -> list[str]:
        """ä»å­—æ®µåæå–å…³é”®è¯"""
        # å­—æ®µç‰¹å®šçš„å…³é”®è¯æ˜ å°„
        keyword_map = {
            "åŸ¹è®­æ—¶é•¿": ["åŸ¹è®­", "æ—¶é•¿", "å¤©æ•°", "å°æ—¶", "å·¥ä½œæ—¥"],
            "åŸ¹è®­è®¡åˆ’": ["åŸ¹è®­", "è®¡åˆ’", "å†…å®¹", "è¯¾ç¨‹", "å®‰æ’"],
            "åŸ¹è®­å¯¹è±¡": ["åŸ¹è®­", "å¯¹è±¡", "äººå‘˜", "å—è®­", "ç”¨æˆ·"],
            "æˆè¯¾è€å¸ˆèµ„è´¨": ["æˆè¯¾", "è€å¸ˆ", "è®²å¸ˆ", "èµ„è´¨", "èµ„æ ¼", "è®¤è¯"],
            "å“åº”æ—¶é—´": ["å“åº”", "æ—¶é—´", "å°æ—¶", "åˆ°è¾¾", "ç°åœº"],
            "è´¨ä¿æœŸé™": ["è´¨ä¿", "ä¿ä¿®", "æœŸé™", "å¹´", "æœˆ"],
            "é…ä»¶ä¾›åº”æœŸé™": ["é…ä»¶", "ä¾›åº”", "å¤‡ä»¶", "è€—æ", "æœŸé™"],
            "è´¨ä¿æœŸåæœåŠ¡è´¹": ["è´¨ä¿", "æœåŠ¡", "è´¹ç”¨", "æ”¶è´¹", "è¿‡ä¿", "ä»·æ ¼"],
        }
        return keyword_map.get(field_name, [field_name])
    
    def extract_evidence(
        self, 
        field_name: str, 
        query: str
    ) -> EvidenceItem | None:
        """ä½¿ç”¨æ··åˆæ£€ç´¢æå–è¯æ®"""
        
        # 1. æå–å…³é”®è¯
        keywords = self.extract_keywords(field_name)
        print(f"   å…³é”®è¯: {', '.join(keywords)}")
        
        # 2. æ··åˆæ£€ç´¢
        results = self.retriever.retrieve(query, keywords=keywords)
        
        if not results:
            print(f"   âš ï¸  æ··åˆæ£€ç´¢æœªè¿”å›ç»“æœ")
            return None
        
        print(f"   âœ“ å‘é‡æ£€ç´¢+å…³é”®è¯æ£€ç´¢å…±æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        
        # 3. ä½¿ç”¨LLMä»å¤šä¸ªç»“æœä¸­æå–æœ€ä½³ç­”æ¡ˆ
        contexts = []
        for i, r in enumerate(results[:3]):
            text_clean = r.text.replace('\n', ' ').strip()[:400]
            contexts.append(f"[ç¬¬{r.page_idx}é¡µ] {text_clean}...")
        
        context_text = "\n\n".join(contexts)
        
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸“ä¸šçš„æŠ•æ ‡æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚"
                    "åŸºäºæä¾›çš„å¤šä¸ªæ£€ç´¢ç»“æœï¼Œæå–æœ€å‡†ç¡®çš„ä¿¡æ¯ã€‚"
                    "å¦‚æœä¿¡æ¯ä¸å­˜åœ¨æˆ–çŸ›ç›¾ï¼Œæ˜ç¡®å›ç­”'æœªæ‰¾åˆ°'ã€‚"
                    "åªå›ç­”æå–çš„ä¿¡æ¯ï¼Œä¸è¦è§£é‡Šã€‚"
                )
            },
            {
                "role": "user",
                "content": (
                    f"ã€éœ€è¦æå–çš„ä¿¡æ¯ã€‘\n{field_name}\n\n"
                    f"ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ã€‘\n{context_text}\n\n"
                    f"è¯·ä»ä¸Šè¿°å†…å®¹ä¸­æå–'{field_name}'çš„å…·ä½“ä¿¡æ¯ã€‚"
                    f"å¦‚æœæ²¡æœ‰æ˜ç¡®ä¿¡æ¯ï¼Œè¯·å›ç­”'æœªæ‰¾åˆ°'ã€‚"
                )
            }
        ]
        
        try:
            llm_response = self.llm.complete(messages, temperature=0.1)
            field_value = llm_response.strip()
            
            if not field_value or "æœªæ‰¾åˆ°" in field_value or len(field_value) < 3:
                print(f"   âš ï¸  LLMè¿”å›æ— æ•ˆç»“æœ")
                return None
                
        except Exception as e:
            print(f"   âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return None
        
        # 4. åˆ›å»ºè¯æ®å¯¹è±¡
        best_result = results[0]
        return EvidenceItem(
            field_name=field_name,
            field_value=field_value,
            source_text=best_result.text[:500],
            page_idx=best_result.page_idx,
            bbox=BoundingBox(x1=0, y1=0, x2=0, y2=0),
            chunk_id=best_result.chunk_id,
            confidence=0.85  # Hybrid confidence
        )
    
    def test_training_plan(self) -> TrainingPlan:
        """æµ‹è¯•åŸ¹è®­æ–¹æ¡ˆè¯„åˆ†"""
        print("\n" + "="*70)
        print("ğŸ“š åŸ¹è®­æ–¹æ¡ˆè¯„åˆ†æµ‹è¯• (æ··åˆæ£€ç´¢)")
        print("="*70)
        
        training = TrainingPlan(
            dimension_id="training",
            dimension_name="åŸ¹è®­æ–¹æ¡ˆ",
            weight=5.0,
            sequence=1
        )
        
        fields_config = [
            ("training_duration", "åŸ¹è®­æ—¶é•¿", "åŸ¹è®­æ—¶é•¿æ˜¯å¤šå°‘å¤©æˆ–å°æ—¶"),
            ("training_schedule", "åŸ¹è®­è®¡åˆ’", "åŸ¹è®­è®¡åˆ’å’ŒåŸ¹è®­å†…å®¹åŒ…æ‹¬å“ªäº›"),
            ("training_personnel", "åŸ¹è®­å¯¹è±¡", "åŸ¹è®­å¯¹è±¡å’Œäººå‘˜è¦æ±‚"),
            ("instructor_qualifications", "æˆè¯¾è€å¸ˆèµ„è´¨", "æˆè¯¾è€å¸ˆå’Œè®²å¸ˆèµ„è´¨è¦æ±‚"),
        ]
        
        found_count = 0
        training_evidence = []
        
        for attr, field_name, query in fields_config:
            print(f"\nğŸ” æå–: {field_name}")
            evidence = self.extract_evidence(field_name, query)
            
            if evidence:
                print(f"   âœ… æˆåŠŸæå– (ç¬¬{evidence.page_idx}é¡µ)")
                print(f"   ğŸ“ å†…å®¹: {evidence.field_value[:80]}...")
                
                training_evidence.append({
                    "field_name": evidence.field_name,
                    "field_value": evidence.field_value,
                    "page_idx": evidence.page_idx,
                    "chunk_id": evidence.chunk_id,
                })
                
                field = EvidenceField(field_name=field_name)
                field.add_candidate(evidence)
                field.resolve_conflict()
                setattr(training, attr, field)
                found_count += 1
            else:
                print(f"   âŒ æœªèƒ½æå–")
        
        completeness = training.evaluate_completeness()
        score = training.calculate_score()
        
        print(f"\nğŸ“Š åŸ¹è®­æ–¹æ¡ˆ: {score:.1f}/{training.weight}åˆ† ({completeness.value})")
        
        self.results["training"] = {
            "dimension": "åŸ¹è®­æ–¹æ¡ˆ",
            "weight": training.weight,
            "score": score,
            "completeness": completeness.value,
            "found_fields": found_count,
            "total_fields": 4,
            "evidence": training_evidence
        }
        return training
    
    def test_after_sales_service(self) -> AfterSalesService:
        """æµ‹è¯•å”®åæœåŠ¡è¯„åˆ†"""
        print("\n" + "="*70)
        print("ğŸ”§ å”®åæœåŠ¡æ–¹æ¡ˆè¯„åˆ†æµ‹è¯• (æ··åˆæ£€ç´¢)")
        print("="*70)
        
        service = AfterSalesService(
            dimension_id="after_sales",
            dimension_name="å”®åæœåŠ¡æ–¹æ¡ˆ",
            weight=10.0,
            sequence=2
        )
        
        fields_config = [
            ("response_time", "å“åº”æ—¶é—´", "å”®åæœåŠ¡å“åº”æ—¶é—´å¤šä¹…åˆ°è¾¾ç°åœº"),
            ("warranty_period", "è´¨ä¿æœŸé™", "è´¨ä¿æœŸé™ä¿ä¿®æœŸå¤šé•¿æ—¶é—´"),
            ("parts_supply_period", "é…ä»¶ä¾›åº”æœŸé™", "é…ä»¶ä¾›åº”è€—æå¤‡ä»¶æœŸé™"),
            ("post_warranty_service_fee", "è´¨ä¿æœŸåæœåŠ¡è´¹", "è´¨ä¿æœŸåè¿‡ä¿æœåŠ¡è´¹ç”¨æ”¶è´¹æ ‡å‡†"),
        ]
        
        found_count = 0
        service_evidence = []
        
        for attr, field_name, query in fields_config:
            print(f"\nğŸ” æå–: {field_name}")
            evidence = self.extract_evidence(field_name, query)
            
            if evidence:
                print(f"   âœ… æˆåŠŸæå– (ç¬¬{evidence.page_idx}é¡µ)")
                print(f"   ğŸ“ å†…å®¹: {evidence.field_value[:80]}...")
                
                service_evidence.append({
                    "field_name": evidence.field_name,
                    "field_value": evidence.field_value,
                    "page_idx": evidence.page_idx,
                    "chunk_id": evidence.chunk_id,
                })
                
                field = EvidenceField(field_name=field_name)
                field.add_candidate(evidence)
                field.resolve_conflict()
                setattr(service, attr, field)
                found_count += 1
            else:
                print(f"   âŒ æœªèƒ½æå–")
        
        completeness = service.evaluate_completeness()
        service_level = service.evaluate_service_level()
        score = service.calculate_score()
        
        print(f"\nğŸ“Š å”®åæœåŠ¡: {score:.1f}/{service.weight}åˆ† ({service_level.value})")
        
        self.results["after_sales"] = {
            "dimension": "å”®åæœåŠ¡æ–¹æ¡ˆ",
            "weight": service.weight,
            "score": score,
            "completeness": completeness.value,
            "service_level": service_level.value,
            "found_fields": found_count,
            "total_fields": 4,
            "evidence": service_evidence
        }
        return service
    
    def generate_report(self) -> dict:
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“‹ æ··åˆæ£€ç´¢è¯„åˆ†æŠ¥å‘Š")
        print("="*70)
        
        training = self.results.get("training", {})
        after_sales = self.results.get("after_sales", {})
        
        total_score = training.get("score", 0) + after_sales.get("score", 0)
        total_weight = training.get("weight", 0) + after_sales.get("weight", 0)
        percentage = (total_score / total_weight * 100) if total_weight > 0 else 0
        
        print(f"\nğŸ“š åŸ¹è®­æ–¹æ¡ˆ: {training.get('score', 0):.1f}/{training.get('weight', 5)}åˆ†")
        print(f"   æ‰¾åˆ°: {training.get('found_fields', 0)}/{training.get('total_fields', 4)} å­—æ®µ")
        
        print(f"\nğŸ”§ å”®åæœåŠ¡: {after_sales.get('score', 0):.1f}/{after_sales.get('weight', 10)}åˆ†")
        print(f"   æ‰¾åˆ°: {after_sales.get('found_fields', 0)}/{after_sales.get('total_fields', 4)} å­—æ®µ")
        
        print(f"\n{'-'*70}")
        print(f"æ€»åˆ†: {total_score:.1f}/{total_weight}åˆ† ({percentage:.1f}%)")
        passing = "âœ… é€šè¿‡" if percentage >= 60 else "âŒ æœªé€šè¿‡"
        print(f"ç»“æœ: {passing}")
        print(f"{'-'*70}")
        
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "version_id": self.version_id,
            "retrieval_method": "hybrid",
            "total_score": total_score,
            "total_weight": total_weight,
            "percentage": percentage,
            "passed": percentage >= 60,
            "dimensions": self.results
        }


def main():
    print("="*70)
    print("ğŸ¯ æ··åˆæ£€ç´¢è¯„åˆ†æµ‹è¯•")
    print("="*70)
    print(f"æ–‡æ¡£ç‰ˆæœ¬: {VERSION_ID}")
    print(f"æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        tester = HybridScoringTester(version_id=VERSION_ID, top_k=5)
        tester.test_training_plan()
        tester.test_after_sales_service()
        report = tester.generate_report()
        
        output_file = Path("scoring_report_hybrid.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        return 0 if report["passed"] else 1
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        traceback.print_exc()
        return 2


if __name__ == "__main__":
    sys.exit(main())
