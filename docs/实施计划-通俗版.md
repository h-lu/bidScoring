# 投标评分系统实施计划（通俗版）

> **一句话说明**：搭建一个本地文档检索+AI评分系统，让AI帮你给投标文档打分，而且每个分数都能追溯到原文。

---

## 一、这是什么项目？

### 业务场景
招投标评审工作中，需要阅读大量文档，按照多个维度（如"培训方案"、"售后服务"等）进行打分。这个过程：
- 工作量大，容易疲劳漏看
- 主观性强，不同人评分标准不一致
- 难以追溯，不知道分数从哪来

### 我们的解决方案
构建一个本地AI辅助评分系统，实现：
1. **文档入库**：把投标文档解析后存入数据库
2. **智能检索**：需要时能快速找到相关内容
3. **AI评分**：让AI按规则打分
4. **引用溯源**：每个评分都必须注明原文出处
5. **结果验证**：自动检查引用是否正确

---

## 二、系统长什么样？

```
┌─────────────────────────────────────────────────────────────┐
│                        工作流程                              │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. 投标文档 ──→ MineRU解析 ──→ 结构化内容                    │
│                     ↓                                        │
│  2. 结构化内容 ──→ 入库脚本 ──→ 数据库                       │
│                     ↓                                        │
│  3. 评分查询 ──→ 检索工具 ──→ 相关证据片段                   │
│                     ↓                                        │
│  4. 证据+规则 ──→ AI评分 ──→ 带引用的评分结果                │
│                     ↓                                        │
│  5. 评分结果 ──→ 引用验证 ──→ 最终可信赖的分数               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 技术架构（通俗版）

| 层次 | 技术选型 | 作用 |
|------|----------|------|
| 数据存储 | PostgreSQL + pgvector | 存文档、存向量、做全文检索 |
| 向量计算 | OpenAI Embeddings | 把文字变成数字，算相似度 |
| AI评分 | OpenAI GPT | 按规则打分并生成引用 |
| 对外接口 | MCP Server | 让Claude能调用的工具接口 |
| 质量保证 | pytest + deepeval | 自动化测试与评测 |

---

## 三、实施步骤（共8步）

### 第1步：搭骨架（初始化项目）

**要做什么**：创建项目目录结构，配置好环境变量

**产出**：
```
bid_scoring/
├── bid_scoring/
│   └── config.py          # 配置管理
├── tests/
│   └── test_config.py     # 配置测试
├── requirements.txt        # 依赖列表
└── .env.example           # 环境变量模板
```

**验收标准**：运行 `pytest tests/test_config.py` 绿灯通过

---

### 第2步：建仓库（数据库设计）

**要做什么**：创建数据库表结构

#### 2.1 ER图（实体关系图）

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              数据库 ER 图                                       │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   ┌──────────────┐                                                             │
│   │  projects    │  项目表：一个招标项目可以包含多份投标文档                    │
│   ├──────────────┤                                                             │
│   │ project_id PK│                                                             │
│   │ name         │                                                             │
│   │ owner        │                                                             │
│   │ status       │                                                             │
│   │ created_at   │                                                             │
│   └──────┬───────┘                                                             │
│          │ 1                                                                    │
│          │                                                                      │
│          │ N                                                                    │
│          ↓                                                                      │
│   ┌──────────────┐     ┌──────────────────┐     ┌──────────────┐               │
│   │  documents   │     │document_versions │     │   chunks     │               │
│   ├──────────────┤     ├──────────────────┤     ├──────────────┤               │
│   │ doc_id PK    │────→│ version_id PK    │────→│ chunk_id PK  │               │
│   │ project_id FK│ 1  │ doc_id FK        │ 1 N │ version_id FK│               │
│   │ title        │     │ source_uri       │     │ project_id FK│               │
│   │ source_type  │     │ source_hash      │     │ source_id    │               │
│   │ created_at   │     │ parser_version   │     │ text_raw     │ ← 全文检索    │
│   └──────────────┘     │ status           │     │ text_tsv     │               │
│                        │ created_at       │     │ embedding    │ ← 向量检索    │
│                        └──────────────────┘     │ page_idx     │               │
│                                                 │ bbox         │               │
│                                                 └──────────────┘               │
│                                                      ↑                          │
│                      ┌───────────────────────────────┼──────────────────┐      │
│                      │                               │                  │      │
│                      │ N                             │ 1                │      │
│               ┌──────────────┐              ┌──────────────┐    ┌────────────┐│
│               │  citations   │              │scoring_runs  │    │scoring_    ││
│               ├──────────────┤              ├──────────────┤    │results     ││
│               │ citation_id PK│              │ run_id PK    │    ├────────────┤│
│               │ result_id FK │──────────────→│ project_id FK│    │ result_id PK│
│               │ chunk_id FK  │              │ version_id FK│───→│ run_id FK   │
│               │ cited_text   │              │ dimensions[] │    │ dimension   │
│               │ verified     │              │ model        │    │ score       │
│               │ match_type   │              │ status       │    │ reasoning   │
│               └──────────────┘              │ started_at   │    │ evidence_   │
│                                            └──────────────┘    │ found       │
│                                                                 └────────────┘
│                                                                                 │
│   ═══════════════════════════════════════════════════════════════════════════  │
│   图例说明：                                                                     │
│   PK = Primary Key (主键，唯一标识)                                              │
│   FK = Foreign Key (外键，关联其他表)                                            │
│   1:N = 一对多关系                                                               │
│   ──→ = 数据流向                                                                 │
│   ═══════════════════════════════════════════════════════════════════════════  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

#### 2.2 表结构详解

---

##### 表1：projects（项目表）

**作用**：管理招标项目，是整个系统的顶层容器

**为什么需要**：
- 一个招标项目通常有多份投标文档需要对比评分
- 需要统一管理项目状态和归属

| 字段 | 类型 | 说明 | 必要性 |
|------|------|------|--------|
| project_id | uuid | 主键，唯一标识 | ★★★ 必须 |
| name | text | 项目名称 | ★★★ 必须 |
| owner | text | 项目负责人/归属部门 | ★★ 可选 |
| status | text | 状态：draft/active/closed | ★★ 可选 |
| created_at | timestamptz | 创建时间 | ★★★ 必须 |

**使用场景**：
```sql
-- 创建新项目
INSERT INTO projects (project_id, name, owner) VALUES ('xxx', 'XX医院设备采购', '采购部');

-- 查询某个项目的所有文档
SELECT * FROM documents WHERE project_id = 'xxx';
```

---

##### 表2：documents（文档表）

**作用**：记录投标文档的基本信息

**为什么需要**：
- 同一项目下有多份投标文档（不同投标商）
- 需要区分文档来源类型（PDF、Word等）

| 字段 | 类型 | 说明 | 必要性 |
|------|------|------|--------|
| doc_id | uuid | 主键 | ★★★ 必须 |
| project_id | uuid | 所属项目（外键） | ★★★ 必须 |
| title | text | 文档标题 | ★★★ 必须 |
| source_type | text | 来源类型：mineru/pdf/docx | ★★★ 必须 |
| created_at | timestamptz | 入库时间 | ★★★ 必须 |

**使用场景**：
```sql
-- 查询某项目的所有投标文档
SELECT doc_id, title FROM documents WHERE project_id = 'xxx';

-- 获取文档元信息
SELECT * FROM documents WHERE doc_id = 'yyy';
```

---

##### 表3：document_versions（文档版本表）

**作用**：管理同一文档的多个解析版本

**为什么需要**：
- 同一份文档可能被多次解析（解析器升级、参数调整）
- 需要保留历史版本以便回溯和对比
- 支持增量更新而非覆盖

| 字段 | 类型 | 说明 | 必要性 |
|------|------|------|--------|
| version_id | uuid | 主键 | ★★★ 必须 |
| doc_id | uuid | 所属文档（外键） | ★★★ 必须 |
| source_uri | text | 原始文件路径/URL | ★★ 可选 |
| source_hash | text | 文件哈希（检测变化） | ★★ 可选 |
| parser_version | text | 解析器版本号 | ★★ 可选 |
| status | text | 状态：pending/ready/failed | ★★★ 必须 |
| created_at | timestamptz | 创建时间 | ★★★ 必须 |

**使用场景**：
```sql
-- 获取文档的最新版本
SELECT version_id FROM document_versions
WHERE doc_id = 'yyy'
ORDER BY created_at DESC
LIMIT 1;

-- 检测文件是否变化
SELECT * FROM document_versions
WHERE doc_id = 'yyy' AND source_hash = 'abc123';
```

---

##### 表4：chunks（文档片段表）⭐核心表

**作用**：存储文档的最小可检索单元

**为什么需要**：
- AI评分需要引用具体片段，不能是整份文档
- 需要支持全文检索（精确匹配）和向量检索（语义匹配）
- 需要记录片段的位置信息（页码、坐标）以便溯源

| 字段 | 类型 | 说明 | 必要性 | 索引 |
|------|------|------|--------|------|
| chunk_id | uuid | 主键 | ★★★ 必须 | - |
| project_id | uuid | 所属项目（外键） | ★★★ 必须 | 普通索引 |
| version_id | uuid | 所属版本（外键） | ★★★ 必须 | 普通索引 |
| source_id | text | 来源标识（如chunk_0001） | ★★ 可选 | - |
| chunk_index | int | 在文档中的序号 | ★★ 可选 | - |
| page_idx | int | 页码 | ★★★ 必须 | 复合索引 |
| bbox | jsonb | 边界坐标[x1,y1,x2,y2] | ★★ 可选 | - |
| element_type | text | 类型：text/table/image | ★★ 可选 | - |
| text_raw | text | 原始文本 | ★★★ 必须 | - |
| text_hash | text | 文本哈希（去重） | ★★ 可选 | - |
| text_tsv | tsvector | 全文检索索引 | ★★★ 必须 | GIN索引 |
| embedding | vector(1536) | 向量表示 | ★★★ 必须 | HNSW索引 |

**核心索引设计**：
```sql
-- 全文检索索引（GIN，适合关键词搜索）
CREATE INDEX idx_chunks_text_tsv ON chunks USING gin(text_tsv);

-- 向量检索索引（HNSW，适合语义搜索）
CREATE INDEX idx_chunks_embedding_hnsw ON chunks USING hnsw(embedding vector_cosine_ops);

-- 组合查询索引（按项目和版本过滤）
CREATE INDEX idx_chunks_project_version_page ON chunks(project_id, version_id, page_idx);
```

**使用场景**：
```sql
-- 全文检索：找包含"培训"的片段
SELECT source_id, text_raw FROM chunks
WHERE version_id = 'zzz' AND text_tsv @@ plainto_tsquery('simple', '培训');

-- 向量检索：找语义相似的片段（最相似的前10条）
SELECT source_id, text_raw FROM chunks
WHERE version_id = 'zzz'
ORDER BY embedding <-> '[0.1, 0.2, ...]'  -- 查询向量
LIMIT 10;

-- 获取指定页面的所有内容
SELECT text_raw, bbox FROM chunks
WHERE version_id = 'zzz' AND page_idx = 12;
```

---

##### 表5：scoring_runs（评分任务表）

**作用**：记录一次评分任务的上下文

**为什么需要**：
- 一次评分可能包含多个维度
- 需要记录使用的模型和规则版本，便于回溯
- 支持任务级别的状态管理

| 字段 | 类型 | 说明 | 必要性 |
|------|------|------|--------|
| run_id | uuid | 主键 | ★★★ 必须 |
| project_id | uuid | 所属项目（外键） | ★★★ 必须 |
| version_id | uuid | 评分的文档版本（外键） | ★★★ 必须 |
| dimensions | text[] | 评分维度列表 | ★★★ 必须 |
| model | text | 使用的LLM模型 | ★★★ 必须 |
| rules_version | text | 评分规则版本 | ★★ 可选 |
| params_hash | text | 参数哈希（检测变化） | ★★ 可选 |
| started_at | timestamptz | 开始时间 | ★★★ 必须 |
| status | text | 状态：running/completed/failed | ★★★ 必须 |

**使用场景**：
```sql
-- 创建新的评分任务
INSERT INTO scoring_runs (run_id, project_id, version_id, dimensions, model, status)
VALUES ('run1', 'xxx', 'zzz', ARRAY['培训方案','售后服务'], 'gpt-4o', 'running');

-- 查询某文档的所有评分历史
SELECT * FROM scoring_runs WHERE version_id = 'zzz' ORDER BY started_at DESC;
```

---

##### 表6：scoring_results（评分结果表）

**作用**：存储每个维度的具体评分结果

**为什么需要**：
- 多维度评分需要独立存储
- 需要记录AI的推理过程
- 便于统计分析和对比

| 字段 | 类型 | 说明 | 必要性 |
|------|------|------|--------|
| result_id | uuid | 主键 | ★★★ 必须 |
| run_id | uuid | 所属任务（外键） | ★★★ 必须 |
| dimension | text | 评分维度名称 | ★★★ 必须 |
| score | numeric | 得分 | ★★★ 必须 |
| max_score | numeric | 满分 | ★★★ 必须 |
| reasoning | text | AI推理过程 | ★★★ 必须 |
| evidence_found | boolean | 是否找到证据 | ★★★ 必须 |
| confidence | text | 置信度：high/medium/low | ★★ 可选 |

**使用场景**：
```sql
-- 查询某次评分的所有结果
SELECT dimension, score, max_score, reasoning
FROM scoring_results WHERE run_id = 'run1';

-- 计算总分
SELECT SUM(score) as total_score, SUM(max_score) as max_possible
FROM scoring_results WHERE run_id = 'run1';
```

---

##### 表7：citations（引用关系表）⭐核心表

**作用**：连接评分结果与原文片段，实现可追溯性

**为什么需要**：
- **这是系统的核心价值**：每个分数都能追溯到原文
- 需要验证AI引用的准确性
- 支持人工审核和争议处理

| 字段 | 类型 | 说明 | 必要性 |
|------|------|------|--------|
| citation_id | uuid | 主键 | ★★★ 必须 |
| result_id | uuid | 所属评分结果（外键） | ★★★ 必须 |
| source_id | text | 证据的source_id | ★★★ 必须 |
| chunk_id | uuid | 引用的片段（外键） | ★★ 可选 |
| cited_text | text | AI引用的具体文字 | ★★★ 必须 |
| verified | boolean | 是否通过验证 | ★★★ 必须 |
| match_type | text | 匹配类型：exact/fuzzy/partial | ★★★ 必须 |

**使用场景**：
```sql
-- 查询某个评分的所有引用
SELECT c.cited_text, ch.text_raw, ch.page_idx
FROM citations c
JOIN chunks ch ON c.chunk_id = ch.chunk_id
WHERE c.result_id = 'res1';

-- 验证引用是否在原文中
SELECT cited_text, verified, match_type
FROM citations WHERE result_id = 'res1';

-- 统计验证通过率
SELECT
  COUNT(*) FILTER (WHERE verified = true) * 100.0 / COUNT(*) as pass_rate
FROM citations WHERE result_id = 'res1';
```

---

#### 2.3 数据流转示意图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              数据流转                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  [入库阶段]                                                                 │
│                                                                             │
│  投标文档.pdf                                                               │
│       ↓                                                                     │
│  MineRU解析 → content_list.json                                            │
│       ↓                                                                     │
│  ┌─────────────┐   ┌──────────────────┐   ┌─────────────────────────────┐  │
│  │ documents   │→  │document_versions │→  │ chunks (文本+向量+全文索引) │  │
│  └─────────────┘   └──────────────────┘   └─────────────────────────────┘  │
│                                                                             │
│  [评分阶段]                                                                 │
│                                                                             │
│  查询："培训方案"                                                            │
│       ↓                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ chunks 检索 (BM25全文检索 + HNSW向量检索 + RRF融合)                   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│       ↓                                                                     │
│  返回证据片段列表                                                            │
│       ↓                                                                     │
│  AI评分 + 生成引用                                                          │
│       ↓                                                                     │
│  ┌──────────────┐        ┌──────────────────┐                              │
│  │scoring_runs  │ ←────→ │scoring_results   │                              │
│  └──────────────┘        └─────────┬────────┘                              │
│                                    ↓                                        │
│                           ┌──────────────────┐                               │
│                           │   citations      │ ←────── chunks (溯源验证)    │
│                           └──────────────────┘                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### 2.4 设计亮点

| 设计决策 | 理由 |
|----------|------|
| 三层文档结构 | documents → versions → chunks，支持版本管理和增量更新 |
| 双检索索引 | 全文检索（关键词）+ 向量检索（语义），互补提升召回率 |
| 引用验证表 | citations独立存储，支持自动验证和人工审核 |
| UUID主键 | 分布式友好，避免自增ID的并发问题 |
| 时间戳记录 | 所有表都有created_at，支持审计和回溯 |

**验收标准**：能成功创建所有表和索引

---

### 第3步：存内容（文档入库）

**要做什么**：把MineRU解析出的JSON数据存入数据库

#### 3.1 MineRU输出文件详解

> **重要**：MinerU有两种使用方式，输出文件命名不同！

##### 本地部署 vs 在线API

| 方式 | 输出文件 | 说明 |
|------|----------|------|
| **本地部署** | `model.json`<br>`middle.json`<br>`content_list.json` | 官方标准命名 |
| **在线API** | `layout.json` ≈ `model.json`<br>`???`<br>`content_list.json` | API服务有自己命名 |

**关键发现**：在线API的 `layout.json` 实际上就是官方的 `model.json`（内容结构相同，只是命名不同）。而 `content_list.json` 两者**完全一致**。

---

##### 本地部署文件结构（官方标准）

```
my_document.zip 解压后：
├── model.json           ← 模型原始输出（最原始）
├── middle.json          ← 中间处理结果（最完整）
├── content_list.json    ← 内容列表（最简洁）★ 我们主要用这个
├── layout.pdf           ← 可视化调试（布局分析结果）
├── full.md              ← 完整Markdown（人类可读）
└── images/              ← 提取的图片文件夹
```

##### 在线API文件结构

```
API返回ZIP 解压后：
├── layout.json          ← 模型原始输出（= 本地版的model.json）
├── model.json           ← 中间格式
├── content_list.json    ← 内容列表（与本地版完全一致）★ 我们主要用这个
├── full.md              ← 完整Markdown
└── images/              ← 提取的图片文件夹
```

---

#### 3.2 三个JSON文件对比

| 维度 | model.json / layout.json | middle.json | content_list.json |
|------|--------------------------|-------------|-------------------|
| **定位** | 模型原始输出 | 完整结构化数据 | 简化内容列表 |
| **结构** | 两层嵌套list | 复杂树状结构 | 扁平数组 |
| **包含内容** | 仅检测到的块 | 所有内容+层级关系 | 可读内容块 |
| **页眉页脚** | 保留在结果中 | 移到discarded_blocks | 已过滤 |
| **层级信息** | 无 | 完整块层级 | 简化为text_level |
| **适用场景** | 调试模型 | 二次开发 | **入库检索** ★ |
| **文件大小** | 中等（~500KB） | 最大（~2MB） | 最小（~400KB） |

> **注**：在线API的 `layout.json` 等同于本地版的 `model.json`。两者结构相同，只是命名不同。

---

#### 3.3 model.json / layout.json（模型原始输出）

**是什么**：VLM模型的直接输出，未经后处理

**文件命名**：
- 本地部署：`xxx_model.json`
- 在线API：`layout.json`

**结构**（两层嵌套数组，外层是页，内层是块）：
```json
[
  [  // 第一页的内容块
    {"type": "title", "bbox": [0.297, 0.166, 0.695, 0.2], "angle": 0, "content": "上海市第六人民医院"},
    {"type": "title", "bbox": [0.361, 0.223, 0.627, 0.257], "angle": 0, "content": "共聚焦显微镜"},
    {"type": "text", "bbox": [0.27, 0.313, 0.72, 0.339], "angle": 0, "content": "招标编号：0811-DSITC253135"}
  ],
  [  // 第二页的内容块
    {"type": "text", "bbox": [...], "content": "..."},
    {"type": "table", "bbox": [...], "content": "..."}
  ]
]
```

**字段说明**：
| 字段 | 说明 |
|------|------|
| type | 内容类型：title/text/table/image/equation等 |
| bbox | 相对坐标 [x0, y0, x1, y1]，范围0-1 |
| angle | 旋转角度（0/90/180/270） |
| content | 文本内容 |

**特点**：
- `bbox`是**相对坐标**（0-1范围，如0.297表示29.7%）
- 包含页眉、页脚、页码等噪音
- 没有阅读顺序信息
- **不建议直接用于入库**

**与content_list.json的坐标差异**：
| 文件 | bbox格式 | 示例 |
|------|----------|------|
| layout.json | 相对坐标（0-1） | `[0.297, 0.166, 0.695, 0.2]` |
| content_list.json | 绝对坐标（像素） | `[296, 165, 693, 200]` |

---

#### 3.4 middle.json（中间处理结果）⭐最完整

**是什么**：经过后处理的完整结构化数据，包含所有层级关系

**结构层次**：
```
pdf_info[] (页数组)
  └── 一级块 (table | image | equation)
       └── 二级块 (table_body | table_caption | image_body...)
            └── 行 (line)
                 └── 片段 (span)
```

**结构示例**：
```json
{
  "pdf_info": [
    {
      "page_idx": 0,
      "page_size": [612, 792],
      "discarded_blocks": [    // 被过滤的块（页眉页脚等）
        {"type": "header", "bbox": [...], "lines": [...]}
      ],
      "para_blocks": [          // 段落块（实际内容）
        {
          "type": "text",
          "bbox": [52, 61, 294, 82],
          "lines": [
            {
              "bbox": [52, 61, 294, 72],
              "spans": [
                {"type": "text", "content": "这是正文内容", "bbox": [...]}
              ]
            }
          ]
        }
      ],
      "tables": [...],           // 表格块
      "images": [...],           // 图片块
      "interline_equations": [...] // 公式块
    }
  ],
  "_backend": "pipeline",
  "_version_name": "0.6.1"
}
```

**支持的块类型**：
| 一级块 | 说明 |
|--------|------|
| table | 表格区域 |
| image | 图片区域 |
| interline_equations | 行间公式区域 |

| 二级块 | 说明 |
|--------|------|
| text | 普通文本 |
| title | 标题 |
| list | 列表 |
| index | 目录 |
| table_body | 表格本体 |
| table_caption | 表格标题 |
| table_footnote | 表格脚注 |
| image_body | 图片本体 |
| image_caption | 图片描述 |
| image_footnote | 图片脚注 |
| interline_equation | 行间公式 |

**使用场景**：
- 需要精确还原文档结构
- 需要处理复杂的表格层级
- 需要区分标题和正文
- **但结构复杂，入库处理成本高**

---

#### 3.5 content_list.json（内容列表）⭐推荐使用

**是什么**：middle.json的简化版，按阅读顺序平铺所有可读内容

**核心特点**：
- 已去除页眉、页脚、页码
- 按人类阅读顺序排列
- 扁平数组结构，易于遍历
- 包含层级标识（text_level）
- **本地部署与在线API输出完全一致**

**真实数据示例**（来自你的投标文件）：
```json
[
  {
    "type": "text",
    "text": "上海市第六人民医院",
    "text_level": 1,           // 1级标题
    "bbox": [296, 165, 693, 200],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "共聚焦显微镜",
    "text_level": 1,           // 1级标题
    "bbox": [360, 222, 626, 255],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "招标编号：0811-DSITC253135",
    "bbox": [269, 311, 718, 338],
    "page_idx": 0
  },
  {
    "type": "image",
    "img_path": "images/5b1602f5f95ae93a9d3c1e27b76997551e04aec7289fe0da0e6e0eacf63cd8d2.jpg",
    "image_caption": [],
    "image_footnote": [],
    "bbox": [533, 600, 624, 645],
    "page_idx": 0
  }
]
```

**字段说明**：
| 字段 | 类型 | 说明 |
|------|------|------|
| type | string | 内容类型：text/table/image/equation |
| text | string | 文本内容（text类型专用） |
| text_level | int | 标题层级：无/0=正文, 1=H1, 2=H2... |
| bbox | [int] | 边界坐标 [x0, y0, x1, y1]，**绝对像素坐标** |
| page_idx | int | 页码（从0开始） |
| img_path | string | 图片路径（table/image/equation） |
| table_body | string | 表格HTML（table类型） |
| table_caption | array | 表格标题 |
| table_footnote | array | 表格脚注 |
| image_caption | array | 图片描述 |
| image_footnote | array | 图片脚注 |

**为什么推荐用于入库**：
- ✅ 结构简单，扁平数组
- ✅ 已过滤噪音（页眉页脚）
- ✅ 阅读顺序正确
- ✅ 包含位置信息（bbox、page_idx）
- ✅ 支持层级区分（text_level）
- ✅ **本地部署与在线API完全一致**

---

#### 3.6 入库映射关系

```
content_list.json              →    chunks表
─────────────────────────────────────────────────────────
type = "text"                 →    element_type = 'text'
type = "table"                →    element_type = 'table'
type = "image"                →    element_type = 'image'
type = "equation"             →    element_type = 'equation'

text / table_body字段         →    text_raw
page_idx                      →    page_idx
bbox                          →    bbox
数组索引                       →    chunk_index

text_level = 0                →    正文
text_level > 0                →    标题（可额外标记）
```

**入库代码示例**：
```python
import json
from pathlib import Path

def ingest_content_list(conn, project_id, document_id, version_id, content_list_path):
    data = json.loads(Path(content_list_path).read_text(encoding='utf-8'))

    rows = []
    for i, item in enumerate(content_list):
        # 只处理文本类型，表格/图片可以后续处理
        if item.get("type") not in ["text", "table"]:
            continue

        # 提取文本内容
        text = ""
        if item.get("type") == "text":
            text = item.get("text", "")
        elif item.get("type") == "table":
            # 表格可以取caption或body
            captions = item.get("table_caption", [])
            text = " ".join(captions) if captions else ""

        if not text.strip():
            continue

        rows.append((
            project_id,
            version_id,
            f"chunk_{i:04d}",
            i,
            item.get("page_idx", 0),
            item.get("bbox", []),
            item.get("type"),
            text,
        ))

    # 批量插入
    with conn.cursor() as cur:
        cur.executemany(
            """INSERT INTO chunks (
                project_id, version_id, source_id, chunk_index,
                page_idx, bbox, element_type, text_raw, text_tsv
              ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, to_tsvector('simple', %s)
              )""",
            [(r[0], r[1], r[2], r[3], r[4], r[5], r[6], r[7], r[7]) for r in rows]
        )
    conn.commit()
```

---

#### 3.7 处理逻辑总结

1. **按项目、文档、版本组织数据**
   - 一个招标项目 = 一个 project
   - 一份投标文档 = 一个 document
   - 一次解析结果 = 一个 document_version

2. **每段文字存为一个chunk**
   - content_list.json中的每个元素 → 一条chunk记录
   - 保留原始位置信息（page_idx、bbox）
   - 自动生成全文检索索引（tsvector）

3. **后续生成向量**
   - 入库完成后，单独运行向量化脚本
   - 只处理 embedding 为 NULL 的记录
   - 批量调用OpenAI Embeddings API

**验收标准**：数据能正确入库，能查到记录数

---

#### 3.8 Chunk合并与语义完整性 ⭐重要议题

> **问题背景**：MineRU按物理布局切分文档，得到的chunk可能打断语义边界。比如：
> - 一句话被拆成两半："本产品提供三年" / "质保服务"
> - 一段完整说明被切成碎片
> - 表格与其caption被分开处理
>
> **影响**：
> - 向量检索时，语义不完整的chunk匹配度降低
> - AI评分时，引用的片段可能读起来不知所云
> - 需要人工额外拼接才能理解完整含义

---

##### 3.8.1 解决方案对比（2026最佳实践）

| 方案 | 原理 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **固定窗口合并** | 滑动窗口合并相邻chunk | 实现简单，可控 | 可能合并不相关内容 | chunk较小的场景 |
| **语义边界合并** | 基于句子/段落边界合并 | 保持语义完整 | 需要NLP分句处理 | 通用RAG场景 |
| **语义相似度合并** | 相邻chunk相似度高于阈值则合并 | 自适应，智能 | 计算成本高 | 高质量要求场景 |
| **Parent-Child** | 大块Parent存上下文，小块Child用于检索 | **上下文最完整**，定位精确 | 存储量大，需维护关系 | ⭐推荐：结构化长文档 |
| **Sentence Window** | 检索时扩展上下文，存储时不变 | 不改存储，灵活 | 检索结果需后处理 | ⭐推荐：流式/短文档 |

---

##### 3.8.2 方案一：固定窗口合并

**思路**：每N个chunk合并为一个，保持重叠避免边界丢失

```python
def merge_chunks_fixed_window(chunks: list[dict], window_size: int = 3, overlap: int = 1) -> list[dict]:
    """
    固定窗口合并chunk

    Args:
        chunks: 原始chunk列表
        window_size: 每组合并的chunk数量
        overlap: 窗口之间的重叠数量
    """
    merged = []
    for i in range(0, len(chunks), window_size - overlap):
        window = chunks[i : i + window_size]
        if not window:
            break

        # 合并文本
        merged_text = "\n".join(c["text"] for c in window)

        # 合并bbox（取第一个的起点，最后一个的终点）
        first_bbox = window[0].get("bbox", [0, 0, 0, 0])
        last_bbox = window[-1].get("bbox", [0, 0, 0, 0])
        merged_bbox = [first_bbox[0], first_bbox[1], last_bbox[2], last_bbox[3]]

        merged.append({
            "text": merged_text,
            "bbox": merged_bbox,
            "page_idx": window[0]["page_idx"],
            "source_ids": [c["source_id"] for c in window],
        })

    return merged
```

**优点**：实现简单，参数可控
**缺点**：可能把不相关的内容合并在一起
**参数建议**：
- `window_size = 3-5`：合并3-5个原始chunk
- `overlap = 1`：保留1个chunk重叠

---

##### 3.8.3 方案二：语义边界合并

**思路**：按句子/段落边界合并，避免打断语义

```python
import re

def merge_chunks_by_sentence(chunks: list[dict], max_tokens: int = 500) -> list[dict]:
    """
    按句子边界合并chunk

    Args:
        chunks: 原始chunk列表
        max_tokens: 合并后的最大token数（近似）
    """
    merged = []
    current_chunk = None

    sentence_endings = re.compile(r'[。！？；\n]+')

    for chunk in chunks:
        text = chunk["text"]

        # 第一个chunk直接作为起点
        if current_chunk is None:
            current_chunk = {
                "text": text,
                "bbox": chunk.get("bbox", []),
                "page_idx": chunk["page_idx"],
                "source_ids": [chunk["source_id"]],
            }
            continue

        # 检查是否应该合并
        combined_text = current_chunk["text"] + "\n" + text
        approx_tokens = len(combined_text) // 2  # 粗略估计token数

        # 如果当前chunk以句号等结尾，说明是完整句子，检查是否超限
        if sentence_endings.search(current_chunk["text"]):
            if approx_tokens > max_tokens:
                # 超限，先保存当前chunk
                merged.append(current_chunk)
                # 开始新的chunk
                current_chunk = {
                    "text": text,
                    "bbox": chunk.get("bbox", []),
                    "page_idx": chunk["page_idx"],
                    "source_ids": [chunk["source_id"]],
                }
            else:
                # 合并到当前chunk
                current_chunk["text"] = combined_text
                current_chunk["source_ids"].append(chunk["source_id"])
                # 更新bbox
                if chunk.get("bbox"):
                    current_chunk["bbox"][2] = chunk["bbox"][2]
                    current_chunk["bbox"][3] = chunk["bbox"][3]
        else:
            # 当前chunk不是完整句子，强制合并
            current_chunk["text"] = combined_text
            current_chunk["source_ids"].append(chunk["source_id"])

    # 添加最后一个chunk
    if current_chunk:
        merged.append(current_chunk)

    return merged
```

**优点**：保持语义完整性，避免句子被切断
**缺点**：需要处理边界情况，中文分句不如英文清晰
**参数建议**：
- `max_tokens = 500-800`：控制合并后的大小

---

##### 3.8.4 方案三：语义相似度合并

**思路**：相邻chunk之间计算向量相似度，高于阈值则合并

```python
import numpy as np
from openai import OpenAI

def merge_chunks_by_similarity(
    chunks: list[dict],
    client: OpenAI,
    threshold: float = 0.75,
    max_merge: int = 5
) -> list[dict]:
    """
    基于语义相似度合并相邻chunk

    Args:
        chunks: 原始chunk列表
        client: OpenAI客户端（用于获取embedding）
        threshold: 相似度阈值，高于此值则合并
        max_merge: 最多连续合并的chunk数
    """
    if not chunks:
        return []

    # 1. 获取所有chunk的向量
    texts = [c["text"] for c in chunks]
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    embeddings = np.array([e.embedding for e in response.data])

    # 2. 计算相邻chunk的余弦相似度
    similarities = []
    for i in range(len(embeddings) - 1):
        sim = np.dot(embeddings[i], embeddings[i+1]) / (
            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i+1])
        )
        similarities.append(sim)

    # 3. 根据相似度决定合并
    merged = []
    i = 0
    while i < len(chunks):
        current_chunk = {
            "text": chunks[i]["text"],
            "bbox": chunks[i].get("bbox", []),
            "page_idx": chunks[i]["page_idx"],
            "source_ids": [chunks[i]["source_id"]],
        }

        merge_count = 0
        # 检查后续chunk是否应该合并
        while (
            i + 1 < len(chunks) and
            merge_count < max_merge and
            similarities[i] >= threshold
        ):
            i += 1
            merge_count += 1
            current_chunk["text"] += "\n" + chunks[i]["text"]
            current_chunk["source_ids"].append(chunks[i]["source_id"])
            # 更新bbox
            if chunks[i].get("bbox"):
                current_chunk["bbox"][2] = chunks[i]["bbox"][2]
                current_chunk["bbox"][3] = chunks[i]["bbox"][3]

        merged.append(current_chunk)
        i += 1

    return merged
```

**优点**：自适应，语义相关的chunk会自动合并
**缺点**：需要额外的向量计算，成本较高
**参数建议**：
- `threshold = 0.75-0.85`：相似度阈值
- `max_merge = 3-5`：防止过度合并

---

##### 3.8.5 方案四：Parent-Child Chunking ⭐2025-2026热门方案

**思路**：小chunk用于检索，大chunk用于提供上下文

这是由LlamaIndex在2024-2025年大力推广的模式，在RAG社区广受认可。

```
┌─────────────────────────────────────────────────────────────────┐
│                    Parent-Child 结构示意                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Parent Chunk (大段文本，不生成向量)                              │
│  ┌────────────────────────────────────────────────────────┐     │
│  │ "本产品提供三年质保服务。质保期内，免费更换配件，         │     │
│  │  免费上门维修。响应时间：24小时内到达现场。               │     │
│  │  质保期外，提供终身有偿维护服务。"                        │     │
│  └────────────────────────────────────────────────────────┘     │
│     │                 │                 │                       │
│     ├─────────────────┼─────────────────┤                       │
│     ▼                 ▼                 ▼                       │
│  Child Chunk        Child Chunk       Child Chunk               │
│  (生成向量)          (生成向量)         (生成向量)                │
│  "本产品提供"        "三年质保服务"     "24小时内到达"            │
│                                                                  │
│  检索流程：                                                      │
│  1. 查询"保修" → 匹配到 Child Chunk "三年质保服务"               │
│  2. 返回对应的 Parent Chunk（完整上下文）                        │
│  3. 引用来源时仍指向 Child Chunk（精确定位）                     │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

**实现代码**：

```python
from typing import list, dict
from openai import OpenAI

class ParentChildChunker:
    """
    Parent-Child 分块器

    - Parent: 大块文本，用于提供完整上下文（不生成向量）
    - Child: 小块文本，用于精确匹配检索（生成向量）
    """

    def __init__(self, parent_size: int = 1000, child_size: int = 200):
        """
        Args:
            parent_size: parent chunk的字符数
            child_size: child chunk的字符数
        """
        self.parent_size = parent_size
        self.child_size = child_size

    def create_parent_child_chunks(self, text: str) -> list[dict]:
        """
        创建parent-child chunk结构
        """
        result = []

        # 1. 先按parent_size切分成大的parent chunks
        parent_chunks = self._split_by_size(text, self.parent_size)

        for parent_idx, parent_text in enumerate(parent_chunks):
            parent_id = f"parent_{parent_idx:04d}"

            # 2. 每个parent再切分成多个child chunks
            child_chunks = self._split_by_size(parent_text, self.child_size)

            children = []
            for child_idx, child_text in enumerate(child_chunks):
                child_id = f"{parent_id}_child_{child_idx:04d}"
                children.append({
                    "child_id": child_id,
                    "parent_id": parent_id,
                    "text": child_text,
                })

            result.append({
                "parent_id": parent_id,
                "text": parent_text,      # parent的完整文本
                "children": children,     # 属于这个parent的所有child
            })

        return result

    def _split_by_size(self, text: str, size: int) -> list[str]:
        """按字符数切分，尽量保持句子完整"""
        chunks = []
        current = ""
        for char in text:
            current += char
            if len(current) >= size and char in "。！？；\n":
                chunks.append(current.strip())
                current = ""
        if current:
            chunks.append(current.strip())
        return chunks


# 入库示例
def ingest_parent_child(conn, project_id, version_id, parent_child_chunks: list[dict]):
    """
    将parent-child结构存入数据库

    注意：
    - Parent存为chunk，但embedding为NULL（不生成向量）
    - Child也存为chunk，生成向量
    - Child通过parent_id字段关联Parent
    """
    with conn.cursor() as cur:
        for pc in parent_child_chunks:
            parent = pc["parent_id"]
            parent_text = pc["text"]

            # 插入parent（不生成向量）
            cur.execute("""
                INSERT INTO chunks (chunk_id, project_id, version_id, source_id, text_raw, text_tsv)
                VALUES (%s, %s, %s, %s, %s, to_tsvector('simple', %s))
            """, (parent, project_id, version_id, parent, parent_text, parent_text))

            # 插入children（后续生成向量）
            for child in pc["children"]:
                cur.execute("""
                    INSERT INTO chunks (chunk_id, project_id, version_id, source_id, text_raw, text_tsv)
                    VALUES (%s, %s, %s, %s, %s, to_tsvector('simple', %s))
                """, (
                    child["child_id"], project_id, version_id,
                    child["child_id"], child["text"], child["text"]
                ))
                # 可选：存储parent-child关系到单独的字段
                # cur.execute("UPDATE chunks SET parent_id = %s WHERE chunk_id = %s",
                #            (parent, child["child_id"]))

    conn.commit()


# 检索示例
def retrieve_with_parent(
    conn,
    query_embedding: list[float],
    top_k: int = 5
) -> list[dict]:
    """
    检索child，返回对应的parent
    """
    with conn.cursor() as cur:
        # 1. 向量检索匹配的child chunks
        cur.execute("""
            SELECT c.chunk_id, c.text_raw, c.source_id
            FROM chunks c
            WHERE c.embedding IS NOT NULL
            ORDER BY c.embedding <-> %s
            LIMIT %s
        """, (query_embedding, top_k))
        children = cur.fetchall()

        results = []
        for child in children:
            child_id, child_text, source_id = child

            # 2. 获取child对应的parent（通过source_id前缀匹配）
            parent_id = source_id.split("_child_")[0] if "_child_" in source_id else None

            if parent_id:
                cur.execute("""
                    SELECT text_raw FROM chunks WHERE source_id = %s
                """, (parent_id,))
                parent_row = cur.fetchone()
                parent_text = parent_row[0] if parent_row else child_text
            else:
                parent_text = child_text

            results.append({
                "child_id": child_id,
                "child_text": child_text,       # 精确匹配的文本（用于溯源）
                "parent_id": parent_id,
                "parent_text": parent_text,     # 完整上下文（用于AI理解）
            })

    return results
```

**优点**：
- ✅ **最佳上下文保留**：Parent完整保留段落/章节语义
- ✅ **精确检索定位**：Child小而精确，匹配度高
- ✅ **灵活可控**：Parent和Child大小可独立调整
- ✅ **溯源友好**：引用时可以精确到Child，展示时显示Parent

**缺点**：
- 存储量增加（Parent + Child）
- 需要维护父子关系
- 检索需要二次查询（先Child后Parent）

**参数建议**：
- `parent_size = 800-1200` 字符：相当于一个完整段落
- `child_size = 150-300` 字符：相当于1-2句话
- `parent_child_ratio = 3:1 到 5:1`：一个Parent包含3-5个Child

**与Sentence Window对比**：

| 维度 | Parent-Child | Sentence Window |
|------|-------------|-----------------|
| 上下文来源 | 大块Parent（预定义） | 相邻chunk（动态获取） |
| 语义完整性 | ★★★★★ 更好 | ★★★★☆ 较好 |
| 实现复杂度 | 中等 | 简单 |
| 存储开销 | 较高（需存Parent） | 较低（只存邻接关系） |
| 检索性能 | 需要二次查询 | 需要扩展查询 |
| **推荐场景** | 长文档、结构化内容 | 短文档、流式内容 |

---

##### 3.8.6 方案五：Sentence Window检索模式

**思路**：存储时保持原始chunk不变，检索时自动扩展上下文

这是LlamaIndex在2025年推广的模式，特别适合需要溯源的场景。

```python
# 存储阶段：保持原始chunk不变
# 每个chunk记录其相邻chunk的ID

class ChunkWithWindow:
    def __init__(self, chunk_id: str, text: str, window_size: int = 2):
        self.chunk_id = chunk_id
        self.text = text
        self.window_size = window_size
        self.prev_chunk_ids = []  # 前N个chunk的ID
        self.next_chunk_ids = []  # 后N个chunk的ID

# 检索阶段：返回带上下文的chunk
def retrieve_with_window(
    query_embedding: list[float],
    top_k: int = 5,
    window_size: int = 2
) -> list[dict]:
    """
    检索时返回带上下文的chunk
    """
    # 1. 向量检索获取原始chunk
    raw_chunks = vector_search(query_embedding, top_k=top_k)

    # 2. 每个chunk扩展其窗口上下文
    results = []
    for chunk in raw_chunks:
        # 获取前后chunk
        prev_chunks = get_prev_chunks(chunk["chunk_id"], window_size)
        next_chunks = get_next_chunks(chunk["chunk_id"], window_size)

        # 构建带上下文的文本
        window_text = "\n".join([c["text"] for c in prev_chunks])
        window_text += "\n[匹配内容]\n" + chunk["text"]
        window_text += "\n" + "\n".join([c["text"] for c in next_chunks])

        results.append({
            "chunk_id": chunk["chunk_id"],
            "matched_text": chunk["text"],  # 原始匹配文本（用于溯源）
            "window_text": window_text,     # 扩展文本（用于AI理解）
            "page_idx": chunk["page_idx"],
            "bbox": chunk["bbox"],
        })

    return results
```

**优点**：
- ✅ 存储不变，易于溯源
- ✅ 检索时可灵活调整窗口大小
- ✅ 原始chunk精确对应原文位置
- ✅ 特别适合投标评分场景（需要精确引用）

**缺点**：
- 检索结果需要后处理
- 需要额外存储chunk之间的邻接关系

**参数建议**：
- `window_size = 2-3`：前后各扩展2-3个chunk

---

##### 3.8.7 投标评分场景的特殊考虑

| 场景特征 | 推荐方案 | 理由 |
|----------|----------|------|
| **需要精确引用溯源** | **Parent-Child** 或 Sentence Window | Parent-Child：Child精确定位，Parent提供完整上下文<br>Sentence Window：原始chunk不变，易于溯源 |
| **表格内容较多** | 语义边界合并 或 Parent-Child | 表格往往被意外拆分，需按结构合并 |
| **评分规则详细** | **Parent-Child** 或 固定窗口合并 | Parent-Child提供最完整的上下文 |
| **混合场景** | 组合策略 | 根据chunk类型采用不同策略 |

**投标文档特点分析**：

```
投标文档通常具有以下特征：
├─ 结构化程度高（有明确章节）
├─ 表格多（参数对比、价格明细）
├─ 需要精确引用（评分需追溯原文）
└─ 上下文重要（单个条款需要前后文理解）

因此：⭐ Parent-Child 是首选方案
      - Parent：按章节/自然段切分，保留完整上下文
      - Child：按MineRU原始chunk或更小粒度切分，用于精确检索
```

**方案选择决策树**：

```
                        你的文档有清晰的章节结构吗？
                                    │
                    ┌───────────────┴───────────────┐
                   │是                              │否
                   ▼                                ▼
            ┌──────────────┐              ┌──────────────┐
            │ Parent-Child  │              │Sentence Window│
            │  ⭐首选      │              │  ⭐备选      │
            └──────────────┘              └──────────────┘
                   │                                │
            需要处理大量表格？                  数据量特别大？
                   │                                │
          ┌────────┴────────┐             ┌────────┴────────┐
         │是               │否           │是               │否
         ▼                 ▼             ▼                 ▼
   语义边界合并      保持Parent-Child    固定窗口合并    保持Sentence Window
   +表格特殊处理                                       +重叠策略
```

**组合策略示例**：

```python
def hybrid_merge_strategy(chunks: list[dict]) -> list[dict]:
    """
    根据chunk类型采用不同合并策略
    """
    result = []
    i = 0

    while i < len(chunks):
        chunk = chunks[i]

        # 表格类型：尝试与caption合并
        if chunk["type"] == "table":
            merged = merge_table_with_caption(chunks, i)
            result.append(merged)
            i += merged["original_count"]
            continue

        # 文本类型：按句子边界合并
        elif chunk["type"] == "text":
            # 检查是否是标题
            if chunk.get("text_level", 0) > 0:
                # 标题单独保留
                result.append(chunk)
                i += 1
            else:
                # 正文按句子合并
                merged_group = []
                for j in range(i, len(chunks)):
                    if chunks[j]["type"] != "text":
                        break
                    if chunks[j].get("text_level", 0) > 0:
                        break
                    merged_group.append(chunks[j])

                    # 达到目标大小后停止
                    if sum(len(c["text"]) for c in merged_group) > 500:
                        break

                # 合并这一组
                if len(merged_group) == 1:
                    result.append(merged_group[0])
                else:
                    result.append(merge_text_chunks(merged_group))
                i += len(merged_group)
            continue

        # 其他类型：保持原样
        result.append(chunk)
        i += 1

    return result
```

---

##### 3.8.8 实施建议（不预设方案）

> **重要**：以下建议基于2026年RAG系统的最佳实践，但最终方案应通过测试确定。

**分阶段验证思路**：

```
阶段1：基线（不做合并）
├─ 直接使用MineRU原始chunk
├─ 评估检索召回率和评分质量
└─ 建立性能基准

阶段2：Parent-Child（⭐推荐优先尝试）
├─ Parent按章节/段落切分，提供上下文
├─ Child按MineRU原始chunk，用于精确检索
├─ 对比阶段1的检索质量和上下文完整性
└─ 评估存储开销和检索性能

阶段3：Sentence Window（备选方案）
├─ 如果阶段2存储开销太大
├─ 改用Sentence Window模式
└─ 评估是否满足上下文需求

阶段4：智能合并（最后手段）
├─ 如果上述方案仍不满足需求
├─ 针对性地合并问题chunk类型
└─ 评估合并后的效果
```

**评估指标**：

| 指标 | 衡量方式 | 目标 |
|------|----------|------|
| 检索召回率 | 人工标注答案在Top-K中的比例 | >80% |
| 引用精确度 | AI引用能精确匹配原文的比例 | >90% |
| 评分一致性 | 同一文档多次评分的标准差 | <0.5分 |
| 上下文完整性 | 检索片段语义完整的比例 | >85% |

---

##### 3.8.9 实用工具推荐（2026）

| 工具 | 功能 | 链接 |
|------|------|------|
| **LlamaIndex** | Parent-Child, Sentence Window | docs.llamaindex.ai |
| **semchunk** | 语义边界分块 | github.com/aurelio-labs/semchunk |
| **chonkie** | 轻量级chunk处理库 | github.com/bstraehle/chonkie |
| **LangChain** | Parent Document Retriever | python.langchain.com |

**快速示例（LlamaIndex Parent-Child）**：

```python
# pip install llama-index-core llama-index-embeddings-openai
from llama_index.core.node_parser import ParentDocumentParser
from llama_index.embeddings.openai import OpenAIEmbedding

# 创建Parent-Child解析器
parser = ParentDocumentParser(
    child_splitter_kwargs={"chunk_size": 200},  # Child大小
    parent_splitter_kwargs={"chunk_size": 1000},  # Parent大小
)

# 解析文档
nodes = parser.get_nodes_from_documents(documents)

# 只给Child生成向量（Parent不生成）
embed_model = OpenAIEmbedding(model="text-embedding-3-small")
for node in nodes:
    if node.metadata["node_type"] == "child":
        node.embedding = embed_model.get_text_embedding(node.text)
```

**快速示例（semchunk）**：

```python
# pip install semchunk
import semchunk

# 自动按语义边界分块
chunker = semchunk.SemanticChunker(
    embedding_model="text-embedding-3-small",
    max_chunk_tokens=500,
)

chunks = chunker.chunk_text(long_text)
```

---

**验收标准**（针对chunk合并）：
- [ ] 原始MineRU数据能正确解析
- [ ] 合并后的chunk保持语义完整
- [ ] 每个合并chunk记录了来源chunk_id（可溯源）
- [ ] bbox范围正确（合并多个chunk时）
- [ ] 检索质量测试通过

---

### 第4步：算向量（向量化）

**要做什么**：把每段文字变成向量数字，支持相似度检索

---

#### 4.1 为什么需要向量

| 检索类型 | 擅长 | 不擅长 |
|----------|------|--------|
| 全文检索(BM25) | 精确关键词匹配 | 语义相似性 |
| 向量检索 | 语义理解、同义词 | 精确匹配 |
| **混合检索** | **两者互补** | - |

**举例说明**：
- 查询"产品保修"
- 全文检索：能找到包含"保修"的片段，但可能漏掉"质保"、"售后服务"
- 向量检索：能理解语义，找到相关内容
- **两者结合**：召回率更高

---

#### 4.2 2025-2026 向量化最佳实践

##### OpenAI Embeddings 选择

| 模型 | 维度 | 特点 | 适用场景 |
|------|------|------|----------|
| `text-embedding-3-small` | 1536 | 速度快、成本低 | **推荐默认选择** |
| `text-embedding-3-large` | 3072 | 精度更高 | 需要更高精度时 |

**最佳实践**（来源：[OpenAI官方文档](https://platform.openai.com/docs/guides/embeddings)）：
- ✅ 使用 `text-embedding-3-small` 作为默认选择
- ✅ 缓存已计算的向量，避免重复调用
- ✅ 使用批量请求提高效率

##### Chunking 最佳实践

**Chunk大小推荐**（来源：[Best Chunking Strategies for RAG in 2025](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025)）：

| 场景 | Token范围 | 建议值 |
|------|-----------|--------|
| 通用RAG | 256-1,024 tokens | **500 tokens** |
| 精确匹配 | 128-256 tokens | 200 tokens |
| 长文档理解 | 1,024-2,048 tokens | 1,500 tokens |

**Chunk重叠**：
- 使用 **10-20%** 重叠避免上下文丢失
- 对于500-token chunk，使用50-100 tokens重叠

##### 批量大小策略

| 批量大小 | 适用场景 | 说明 |
|----------|----------|------|
| 1-10 | 小批量、实时处理 | 低延迟 |
| 10-50 | **推荐默认** | 平衡速度与效率 |
| 50-100 | 大批量离线处理 | 最高吞吐量 |
| 100+ | 需要测试API限制 | 可能触限 |

**OpenAI API限制**（需查阅最新文档）：
- `text-embedding-3-small`: 约**3,000次/分钟**（批量计为1次）
- 单次最多支持**2048个文本**
- 建议每批不超过100-200个文本

---

#### 4.3 pgvector 索引最佳实践

> 参考资料：[AWS pgvector索引优化](https://aws.amazon.com/blogs/database/optimize-generative-ai-applications-with-pgvector-indexing-a-deep-dive-into-ivfflat-and-hnsw-techniques/) | [pgvector GitHub](https://github.com/pgvector/pgvector)

##### HNSW vs IVFFlat 选择

| 维度 | **HNSW** ⭐推荐 | **IVFFlat** |
|------|-------------------|-------------|
| 查询速度 | **最快** (~1.5ms) | 较快 (~2.4ms) |
| 构建时间 | 较慢 (~30秒) | **最快** (~15秒) |
| 内存占用 | 较高 | **较低** |
| 动态更新 | **支持良好** | 需要重建 |
| 召回率 | 可调参数优化 | 可调参数优化 |

**选择建议**：
- ✅ **HNSW**：大多数场景（数据会增长、需要实时更新）
- ✅ **IVFFlat**：静态数据、内存受限、构建速度优先

##### HNSW 参数调优

**构建参数**（建索引时设置）：
```sql
CREATE INDEX ON chunks USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

| 参数 | 默认值 | 说明 | 调优建议 |
|------|--------|------|----------|
| `m` | 16 | 每个节点的最大边数 | **通常保持默认** |
| `ef_construction` | 64 | 构建时候选队列大小 | **通常保持默认** |

**查询参数**（查询时设置）：
```sql
SET hnsw.ef_search = 100;  -- 默认40，提高可提升召回率
SELECT * FROM chunks
ORDER BY embedding <-> '[...]'
LIMIT 10;
```

| 参数 | 默认值 | 说明 | 调优建议 |
|------|--------|------|----------|
| `hnsw.ef_search` | 40 | 搜索时候选队列大小 | **建议设置为64-100** |

**最佳实践**：
- ✅ 数据量大时（>10万条）增加 `hnsw.ef_search` 到100-200
- ✅ 需要高召回率时增加 `hnsw.ef_search`
- ✅ 查询时有WHERE过滤条件，考虑使用 `iterative_scan`

##### IVFFlat 参数调优

**构建参数**（建索引时设置）：
```sql
CREATE INDEX ON chunks USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**lists参数选择**：
```sql
-- 规则（来源：AWS官方博客）
-- 小于100万向量：lists = 行数 / 1000
-- 大于100万向量：lists = √行数

-- 示例：1万条记录
lists = 10000 / 1000 = 10

-- 示例：100万条记录
lists = SQRT(1000000) = 1000
```

**查询参数**：
```sql
SET ivfflat.probes = 10;  -- 默认1，建议设为√lists
SELECT * FROM chunks
ORDER BY embedding <-> '[...]'
LIMIT 10;
```

---

#### 4.4 向量生成流程

```
┌─────────────────────────────────────────────────────────────────┐
│                        向量化流程                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. 查询待向量化记录                                            │
│     SELECT chunk_id, text_raw FROM chunks                       │
│     WHERE embedding IS NULL LIMIT 200;                         │
│                          ↓                                        │
│  2. 批量调用OpenAI API                                          │
│     POST /v1/embeddings                                         │
│     {                                                         │
│       "model": "text-embedding-3-small",                       │
│       "input": ["文本1", "文本2", ...]                         │
│     }                                                         │
│                          ↓                                        │
│  3. 批量更新数据库                                              │
│     UPDATE chunks SET embedding = $1                            │
│     WHERE chunk_id = $2                                       │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

#### 4.5 批量处理最佳实践代码

```python
from openai import OpenAI
from pgvector.psycopg import register_vector
import psycopg
from bid_scoring.config import load_settings

BATCH_SIZE = 100  # 每批处理100条
LIMIT = 200        # 每次运行最多处理200条

def build_embeddings():
    dsn = load_settings()["DATABASE_URL"]
    settings = load_settings()

    # 初始化OpenAI客户端
    client = OpenAI(
        api_key=settings["OPENAI_API_KEY"],
        base_url=settings["OPENAI_BASE_URL"],
        timeout=settings["OPENAI_TIMEOUT"] or 30,
        max_retries=settings["OPENAI_MAX_RETRIES"] or 2,
    )

    with psycopg.connect(dsn) as conn:
        register_vector(conn)

        # 查询待向量化记录
        with conn.cursor() as cur:
            cur.execute("""
                SELECT chunk_id, text_raw
                FROM chunks
                WHERE embedding IS NULL
                LIMIT %s
            """, (LIMIT,))
            rows = cur.fetchall()

        if not rows:
            print("没有需要向量化的记录")
            return

        chunk_ids = [r[0] for r in rows]
        texts = [r[1] for r in rows]

        # 分批处理
        all_embeddings = []
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i+BATCH_SIZE]
            response = client.embeddings.create(
                model=settings["OPENAI_EMBEDDING_MODEL"],
                input=batch_texts
            )
            all_embeddings.extend([item.embedding for item in response.data])

        # 批量更新
        with conn.cursor() as cur:
            cur.executemany(
                "UPDATE chunks SET embedding = %s WHERE chunk_id = %s",
                [(emb, cid) for emb, cid in zip(all_embeddings, chunk_ids)]
            )
        conn.commit()
        print(f"成功向量化 {len(chunk_ids)} 条记录")

if __name__ == "__main__":
    build_embeddings()
```

---

#### 4.6 性能优化建议

| 优化项 | 建议 | 说明 |
|--------|------|------|
| **批量大小** | 50-100 | 平衡API调用次数与单次请求大小 |
| **并发处理** | 谨慎使用 | OpenAI API有限流，并发需控制 |
| **增量更新** | 只处理embedding为NULL | 避免重复计算 |
| **索引时机** | 先入库后建索引 | 大量数据插入时先不建HNSW索引 |
| **存储优化** | `SET STORAGE PLAIN` | 小向量可内联存储避免TOAST |

**先入库后建索引**（大量数据推荐）：
```sql
-- 1. 插入数据时不建索引（或用CONCURRENTLY）
-- 2. 数据全部插入后再创建索引
CREATE INDEX CONCURRENTLY idx_chunks_embedding_hnsw
ON chunks USING hnsw(embedding vector_cosine_ops);
```

---

**验收标准**：chunks表的embedding字段有值

### 第5步：能检索（混合检索）

**要做什么**：实现全文+向量的混合检索，用RRF算法融合结果

**RRF是什么**：
- Reciprocal Rank Fusion（倒数排名融合）
- 简单理解：把两个排名列表按公式合并，平衡两种检索方式
- 公式：`score = weight / (K + rank)`

**验收标准**：输入查询能返回相关片段列表

---

### 第6步：对外接口（MCP Server）

**要做什么**：把检索功能封装成MCP工具，让Claude能调用

**提供的工具**：

| 工具名 | 功能 |
|--------|------|
| search_chunks | 搜索文档片段（混合检索） |
| get_document_info | 获取文档元信息 |
| get_page_content | 获取指定页面的所有内容 |

**验收标准**：MCP Server能启动，工具能被调用

---

### 第7步：AI评分（核心功能）

**要做什么**：实现基于LLM的评分逻辑

**关键点**：
1. **结构化输出**：用JSON Schema强制AI返回标准格式
2. **必须带引用**：每个评分必须注明原文出处
3. **自动验证**：检查引用是否真的存在于原文中

**输出格式**：
```json
{
  "dimension": "培训方案",
  "score": 8,
  "max_score": 10,
  "reasoning": "证据[1]表明包含培训时长与内容",
  "citations": [
    {
      "source_number": 1,
      "cited_text": "培训时间：2天，含安装培训",
      "supports_claim": "包含培训时长与内容"
    }
  ],
  "evidence_found": true
}
```

**验收标准**：AI能返回符合格式的评分，引用能通过验证

---

### 第8步：质量保证（评测回归）

**要做什么**：建立测试集，确保系统质量

**测试层级**：
- 单元测试：每个函数独立测试
- 集成测试：多个模块协同测试
- 评测测试：用deepeval评估AI输出质量

**验收标准**：所有测试绿灯通过

---

## 四、使用流程（最终效果）

```
┌──────────────────────────────────────────────────────────┐
│  用户：Claude                                             │
│  任务：给这份投标文档的"培训方案"打分                      │
├──────────────────────────────────────────────────────────┤
│  1. Claude调用 search_chunks("培训方案", document_id)     │
│  → 返回：["培训时间：2天...", "培训师资质...", ...]       │
│                                                          │
│  2. Claude构造评分请求                                    │
│  → 维度：培训方案，满分10分                               │
│  → 证据：上述检索结果                                     │
│  → 规则：方案详细程度评分标准                             │
│                                                          │
│  3. 调用AI评分                                            │
│  → 返回：score=8, 引用=[1], reasoning=...                │
│                                                          │
│  4. 验证引用                                              │
│  → 检查：cited_text是否真的在证据[1]中                   │
│  → 结果：verified ✓                                      │
│                                                          │
│  5. 最终输出                                              │
│  "培训方案得分8/10分。根据第12页'培训时间：2天，含       │
│   安装培训、操作培训'，方案包含明确的培训时长和内容       │
│   安排，培训方案较完整。"                                 │
└──────────────────────────────────────────────────────────┘
```

---

## 五、开始前的准备

### 环境要求
- Python 3.11+
- PostgreSQL 15+（已安装pgvector扩展）
- OpenAI API Key

### 配置文件（.env）
```bash
# 数据库连接
DATABASE_URL=postgresql://localhost:5432/bid_scoring

# OpenAI配置
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://api.openai.com/v1

# 模型配置
OPENAI_LLM_MODEL_DEFAULT=gpt-4o-mini      # 默认模型
OPENAI_LLM_MODEL_SCORING=gpt-4o           # 评分用更强的模型
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBEDDING_DIM=1536

# 超时与重试
OPENAI_TIMEOUT=30
OPENAI_MAX_RETRIES=2
```

### 初始化命令
```bash
# 1. 创建数据库
createdb bid_scoring
psql bid_scoring -c "CREATE EXTENSION vector;"

# 2. 安装依赖
pip install -r requirements.txt

# 3. 复制配置
cp .env.example .env
# 编辑.env填入真实配置

# 4. 初始化数据库
python scripts/apply_migrations.py

# 5. 运行测试
pytest -q
```

---

## 六、验收清单

全部完成后，你应该能：

- [ ] 所有单元测试通过（`pytest -q`）
- [ ] 文档能成功入库（`scripts/ingest_mineru.py`）
- [ ] 向量能成功生成（`scripts/build_embeddings.py`）
- [ ] 检索能返回结果（MCP工具调用）
- [ ] AI能返回带引用的评分
- [ ] 引用验证能识别真伪
- [ ] 评测测试能跑通

---

## 七、常见问题

**Q: 为什么不用生产级数据库配置？**
A: 本项目侧重最小可用闭环（MVP），生产级优化（高可用、分布式）不在本次范围。

**Q: 为什么每个评分都要带引用？**
A: 可追溯性是评分系统的核心要求，否则无法验证分数依据。

**Q: 向量检索和全文检索为什么要结合？**
A: 单一方式都有局限：全文检索懂关键词不懂语义，向量检索懂语义但可能飘。结合后更稳定。

**Q: 如果没有GPU怎么办？**
A: 向量计算在云端（OpenAI API），本地不需要GPU。

---

## 八、项目结构预览

```
bid_scoring/
├── bid_scoring/           # 核心代码
│   ├── config.py         # 配置加载
│   ├── ingest.py         # 文档入库
│   ├── embeddings.py     # 向量生成
│   ├── search.py         # 检索与RRF
│   ├── llm.py           # LLM客户端
│   ├── scoring.py       # 评分逻辑
│   └── verify.py        # 引用验证
│
├── mcp_servers/          # MCP服务
│   └── bid_documents/
│       └── server.py    # MCP工具定义
│
├── scripts/             # 命令行脚本
│   ├── apply_migrations.py
│   ├── ingest_mineru.py
│   ├── build_embeddings.py
│   ├── verify_citations.py
│   └── score_dimension.py
│
├── migrations/          # 数据库迁移
│   └── 001_init.sql
│
├── tests/              # 测试代码
│   ├── test_*.py
│   └── fixtures/       # 测试数据
│
├── references/         # 规则与Schema
│   ├── scoring_rules.yaml
│   └── output_schema.json
│
├── eval/               # 评测数据
│   └── dataset.jsonl
│
├── .env.example        # 环境变量模板
└── requirements.txt    # 依赖列表
```

---

**最后**：这个项目采用测试驱动开发（TDD），每个任务都是"先写测试→实现→测试通过"的节奏。这样做的好处是每一步都有质量保证，出问题能快速定位。
